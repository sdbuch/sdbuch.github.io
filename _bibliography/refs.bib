---
---
References
--
--
For LaTeX, right delimiters to use are \( and \)
  some things need to be escaped (braces?)
===========

@INPROCEEDINGS{Wang2021-rc,
  title     = "Deep Networks Provably Classify Data on Curves",
  booktitle = "Advances in Neural Information Processing Systems",
  author    = "Wang, Tingran and Buchanan, Sam and Gilboa, Dar and Wright, John",
  publisher = "Curran Associates, Inc.",
  volume    =  34,
  pages     = "28940--28953",
  year      =  2021,
  month     =  dec,
  abstract = {Data with low-dimensional nonlinear structure are ubiquitous in engineering and scientific problems. We study a model problem with such structure: a binary classification task that uses a deep fully-connected neural network to classify data drawn from two disjoint smooth curves on the unit sphere. Aside from mild regularity conditions, we place no restrictions on the configuration of the curves. We prove that when (i) the network depth is large relative to certain geometric properties that set the difficulty of the problem and (ii) the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To our knowledge, this is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. Our analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime, where the network depth plays the role of a fitting resource in solving the classification problem. In particular, via fine-grained control of the decay properties of the NTK, we demonstrate that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization.},
  html      = "https://papers.nips.cc/paper/2021/hash/f26df67e8110ee2b44923db775e3e47f-Abstract.html",
  talk = "https://www.youtube.com/watch?v=PEaYY2TLvYY",
  arxivid       = "2107.14324"
}

@INPROCEEDINGS{Buchanan2021-sj,
  title     = "Deep Networks and the Multiple Manifold Problem",
  booktitle = "International Conference on Learning Representations",
  author    = "Buchanan, Sam and Gilboa, Dar and Wright, John",
  year      =  2021,
  month     =  jan,
  html      = {https://openreview.net/forum?id=O-6Pm_d_Q-},
  talk      = "https://iclr.cc/virtual/2021/poster/2530",
  talk      = "https://www.youtube.com/watch?v=qloPDlYqnIk",
  abstract  = {We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth \( L \) is large relative to certain geometric and statistical properties of the data, the network width \(n\) grows as a sufficiently large polynomial in \(L\), and the number of i.i.d. samples from the manifolds is polynomial in \(L\), randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the "neural tangent kernel" of Jacot et al. and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected ReLU networks, requiring width \(n \geq L\,\mathrm\{poly\}(d_0)\) to achieve uniform concentration of the initial kernel over a \(d_0\)-dimensional submanifold of the unit sphere \(\mathbb\{S\}^\{n_0-1\}\), and a nonasymptotic framework for establishing generalization of networks trained in the "NTK regime" with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.},
}

@INPROCEEDINGS{Gilboa2019-px,
  title     = "Efficient Dictionary Learning with Gradient Descent",
  booktitle = "Proceedings of the 36th International Conference on Machine
               Learning",
  author    = "Gilboa, Dar and Buchanan, Sam and Wright, John",
  publisher = "PMLR",
  volume    =  97,
  pages     = "2252--2259",
  series    = "Proceedings of Machine Learning Research",
  year      =  2019,
  html      = {http://proceedings.mlr.press/v97/gilboa19a.html},
  abstract = {Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem – complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well.},
}

@INPROCEEDINGS{Buchanan2018-ip,
  title     = "Efficient Model-Free Learning to Overcome Hardware
               Nonidealities in Analog-to-Information Converters",
  booktitle = "2018 IEEE International Conference on Acoustics, Speech and
               Signal Processing (ICASSP)",
  author    = "Buchanan, S and Haque, T and Kinget, P and Wright, J",
  pages     = "3574--3578",
  month     =  apr,
  year      =  2018,
  html      = {http://dx.doi.org/10.1109/ICASSP.2018.8461811},
  abstract  = {This paper considers compressed sensing (CS) in the context of RF spectrum sensing and presents an efficient approach for learning hardware nonidealities in an analog-to-information converter (A2IC). The proposed methodology is based on the learned iterative shrinkage-thresholding algorithm (LISTA), which enables co-optimization of the hardware and the reconstruction algorithm and leads to a model-free recovery approach that is optimally tuned for the unique computational constraints and hardware nonidealities present in the RF frontend. To achieve this, we devise a training protocol that employs a dataset and neural network of minimal sizes. We demonstrate the effectiveness of our methodology on simulated data from a model of a well-established CS A2IC in the presence of linear impairments and noise. The recovery process extrapolates from training on 1-sparse signals to recovering the support of signals whose sparsity runs up to the theoretical optimum for \(\ell^1\) -based algorithms across a range of typical operating SNRs.},
  slides   = {sslista_deck.pdf}
}

@ARTICLE{Buchanan2022-qp,
  title         = "Resource-Efficient Invariant Networks: Exponential Gains
                   by Unrolled Optimization",
  author        = "Buchanan, Sam and Yan, Jingkai and Haber, Ellie and Wright,
                   John",
  month         =  mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.05006",
  archivePrefix = "arXiv",
  abstract = { Achieving invariance to nuisance transformations is a fundamental challenge in the
              construction of robust and reliable vision systems.  Existing approaches to
              invariance scale exponentially with the dimension of the family of transformations,
              making them unable to cope with natural variabilities in visual data such as changes
              in pose and perspective.  We identify a common limitation of these approaches---they
              rely on sampling to traverse the high-dimensional space of transformations---and
              propose a new computational primitive for building invariant networks based instead
              on optimization, which in many scenarios provides a provably more efficient method
              for high-dimensional exploration than sampling.  We provide empirical and
              theoretical corroboration of the efficiency gains and soundness of our proposed
              method, and demonstrate its utility in constructing an efficient invariant network
              for a simple hierarchical object detection task when combined with unrolled
              optimization.  Code for our networks and experiments is available at
              https://github.com/sdbuch/refine.},
  eprint        = "2203.05006",
  primaryClass  = "cs.CV",
  code          = "https://github.com/sdbuch/refine",
  arxivid       = "2203.05006"
}

@PHDTHESIS{Buchanan2022-ye,
  title  = "Deep Networks Through the Lens of Low-Dimensional Structure:
            Towards Mathematical and Computational Principles for Nonlinear
            Data",
  author = "Buchanan, Sam",
  year   =  2022,
  month  = oct,
  html    = "https://academiccommons.columbia.edu/doi/10.7916/p2n1-kp52",
  school = "Columbia University",
  doi    = "10.7916/p2n1-kp52"
}

@inproceedings{Yu2023-ig,
  title         = "White-Box Transformers via Sparse Rate Reduction",
  author        = "Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe
                   and Wu, Ziyang and Tong, Shengbang and Haeffele, Benjamin D
                   and Ma, Yi",
  booktitle     = "Advances in Neural Information Processing Systems (NeurIPS)",
  month         =  dec,
  year          =  2023,
  arxivid       = "2306.01129",
  website       = "https://ma-lab-berkeley.github.io/CRATE/",
  abstract      = {In this paper, we contend that the objective of
                   representation learning is to compress and transform the
                   distribution of the data, say sets of tokens, towards a
                   mixture of low-dimensional Gaussian distributions supported
                   on incoherent subspaces. The quality of the final
                   representation can be measured by a unified objective
                   function called sparse rate reduction. From this
                   perspective, popular deep networks such as transformers can
                   be naturally viewed as realizing iterative schemes to
                   optimize this objective incrementally. Particularly, we show
                   that the standard transformer block can be derived from
                   alternating optimization on complementary parts of this
                   objective: the multi-head self-attention operator can be
                   viewed as a gradient descent step to compress the token sets
                   by minimizing their lossy coding rate, and the subsequent
                   multi-layer perceptron can be viewed as attempting to
                   sparsify the representation of the tokens. This leads to a
                   family of white-box transformer-like deep network
                   architectures which are mathematically fully interpretable.
                   Despite their simplicity, experiments show that these
                   networks indeed learn to optimize the designed objective:
                   they compress and sparsify representations of large-scale
                   real-world vision datasets such as ImageNet, and achieve
                   performance very close to thoroughly engineered transformers
                   such as ViT.  }
}

@inproceedings{tilted2023,
    author = {Yi, Brent and Zeng, Weijia and Buchanan, Sam and Ma, Yi},
    title = {Canonical Factors for Hybrid Neural Fields},
    booktitle = {International Conference on Computer Vision (ICCV)},
    month = oct,
    year = {2023},
    abstract = { Factored feature volumes offer a simple way to build more
                compact, efficient, and interpretable neural fields, but also
                introduce biases that are not necessarily beneficial for
                real-world data. In this work, we (1) characterize the
                undesirable biases that these architectures have for
                axis-aligned signals -- they can lead to radiance field
                reconstruction differences of as high as 2 PSNR -- and (2)
                explore how learning a set of canonicalizing transformations
                can improve representations by removing these biases. We prove
                in a two-dimensional model problem that simultaneously learning
                these transformations together with scene appearance succeeds
                with drastically improved efficiency. We validate the resulting
                architectures, which we call TILTED, using image, signed
                distance, and radiance field reconstruction tasks, where we
                observe improvements across quality, robustness, compactness,
                and runtime. Results demonstrate that TILTED can enable
                capabilities comparable to baselines that are 2x larger, while
                highlighting weaknesses of neural field evaluation procedures.
                },
  arxivid       = "2308.15461",
  website       = "https://brentyi.github.io/tilted/",
  html          = "https://openaccess.thecvf.com/content/ICCV2023/html/Yi_Canonical_Factors_for_Hybrid_Neural_Fields_ICCV_2023_paper.html"
}

@inproceedings{Yu2023-fc,
  title         = "Emergence of Segmentation with Minimalistic White-Box
                   Transformers",
  author        = "Yu*, Yaodong and Chu*, Tianzhe and Tong, Shengbang and Wu,
                   Ziyang and Pai, Druv and Buchanan, Sam and Ma, Yi",
  month         =  jan,
  year          =  2024,
  booktitle     = "Conference on Parsimony and Learning (CPAL)",
  url           = "http://arxiv.org/abs/2308.16271",
  arxivid       = "2308.16271",
  abstract      = {Transformer-like models for vision tasks have recently proven
                   effective for a wide range of downstream applications such as
                   segmentation and detection. Previous works have shown that
                   segmentation properties emerge in vision transformers (ViTs)
                   trained using self-supervised methods such as DINO, but not in
                   those trained on supervised classification tasks. In this study,
                   we probe whether segmentation emerges in transformer-based models
                   solely as a result of intricate self-supervised learning
                   mechanisms, or if the same emergence can be achieved under much
                   broader conditions through proper design of the model
                   architecture. Through extensive experimental results, we
                   demonstrate that when employing a white-box transformer-like
                   architecture known as CRATE, whose design explicitly models and
                   pursues low-dimensional structures in the data distribution,
                   segmentation properties, at both the whole and parts levels,
                   already emerge with a minimalistic supervised training recipe.
                   Layer-wise finer-grained analysis reveals that the emergent
                   properties strongly corroborate the designed mathematical
                   functions of the white-box network. Our results suggest a path to
                   design white-box foundation models that are simultaneously highly
                   performant and mathematically fully interpretable.},
  website        = "https://ma-lab-berkeley.github.io/CRATE/",
  html           = "https://proceedings.mlr.press/v234/yu24a.html"
}

@article{JMLR:v25:23-1547,
  author  = "Yu*, Yaodong and Buchanan*, Sam and Pai*, Druv and Chu, Tianzhe
             and Wu, Ziyang and Tong, Shengbang and Bai, Hao and Zhai,
             Yuexiang and Haeffele, Benjamin D and Ma, Yi",
  title   = "White-Box Transformers via Sparse Rate Reduction:
             Compression Is All There Is?",
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  month   = sep,
  pages   = {1--129},
  html     = {http://jmlr.org/papers/v25/23-1547.html},
  abstract      = { In this paper, we contend that a natural objective of
    representation learning is to compress and transform the distribution of the
      data, say sets of tokens, towards a low-dimensional Gaussian mixture
      supported on incoherent subspaces. The goodness of such a representation
      can be evaluated by a principled measure, called sparse rate reduction,
    that simultaneously maximizes the intrinsic information gain and extrinsic
      sparsity of the learned representation. From this perspective, popular
      deep network architectures, including transformers, can be viewed as
      realizing iterative schemes to optimize this measure. Particularly, we
      derive a transformer block from alternating optimization on parts of this
      objective: the multi-head self-attention operator compresses the
      representation by implementing an approximate gradient descent step on the
      coding rate of the features, and the subsequent multi-layer perceptron
      sparsifies the features. This leads to a family of white-box
      transformer-like deep network architectures, named CRATE, which are
      mathematically fully interpretable. We show, by way of a novel connection
      between denoising and compression, that the inverse to the aforementioned
      compressive encoding can be realized by the same class of CRATE
      architectures. Thus, the so-derived white-box architectures are universal
      to both encoders and decoders. Experiments show that these networks,
    despite their simplicity, indeed learn to compress and sparsify
      representations of large-scale real-world image and text datasets, and
      achieve performance very close to highly engineered transformer-based
      models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed
      computational framework demonstrates great potential in bridging the gap
      between theory and practice of deep learning, from a unified perspective
      of data compression. },
  website        = "https://ma-lab-berkeley.github.io/CRATE/",
  arxivid       = "2311.13110",
}


@INPROCEEDINGS{Fang2024-ey,
  title     = "What's in a Prior? {L}earned Proximal Networks for Inverse
               Problems",
  booktitle = "International Conference on Learning Representations",
  author    = "Fang*, Zhenghan and Buchanan*, Sam and Sulam, Jeremias",
  year      =  2024,
  month     =  may,
  html       = "https://openreview.net/forum?id=kNPcOaqC5r",
  code       = "https://github.com/Sulam-Group/learned-proximal-networks",
  website    = "https://zhenghanfang.github.io/learned-proximal-networks/",
  abstract      = { Proximal operators are ubiquitous in inverse problems, commonly appearing as part of algorithmic strategies to regularize problems that are otherwise ill-posed. Modern deep learning models have been brought to bear for these tasks too, as in the framework of plug-and-play or deep unrolling, where they loosely resemble proximal operators. Yet, something essential is lost in employing these purely data-driven approaches: there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. This not only makes guaranteeing convergence of iterative schemes challenging but, more fundamentally, complicates the analysis of what has been learned by these networks about their training data. Herein we provide a framework to develop learned proximal networks (LPN), prove that they provide exact proximal operators for a data-driven nonconvex regularizer, and show how a new training strategy, dubbed proximal matching, provably promotes the recovery of the log-prior of the true data distribution. Such LPN provide general, unsupervised, expressive proximal operators that can be used for general inverse problems with convergence guarantees. We illustrate our results in a series of cases of increasing complexity, demonstrating that these models not only result in state-of-the-art performance, but provide a window into the resulting priors learned from data. }
}

@INPROCEEDINGS{Pai2024-jm,
  title     = "Masked Completion via Structured Diffusion with White-Box
               Transformers",
  booktitle = "International Conference on Learning Representations",
  author    = "Pai, Druv and Wu, Ziyang and Buchanan, Sam and Yu, Yaodong and Ma, Yi",
  year      =  2024,
  month     =  may,
  website        = "https://ma-lab-berkeley.github.io/CRATE/",
  html       = "https://openreview.net/forum?id=PvyOYleymy",
  abstract  = { Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving a deep transformer-like masked autoencoder architecture, called CRATE-MAE, in which the role of each layer is mathematically fully interpretable: they transform the data distribution to and from a structured representation. Extensive empirical evaluations confirm our analytical insights. CRATE-MAE demonstrates highly promising performance on large-scale imagery datasets while using only ~30\% of the parameters compared to the standard masked autoencoder with the same model configuration. The representations learned by CRATE-MAE have explicit structure and also contain semantic meaning.}
}

@INPROCEEDINGS{Buchanan2025-dw,
  title         = "On the Edge of Memorization in Diffusion Models",
  author        = "Buchanan*, Sam and Pai*, Druv and Ma, Yi and Bortoli, Valentin
                   De",
  month         =  dec,
  year          =  2025,
  booktitle     = "Advances in Neural Information Processing Systems (NeurIPS)",
  url           = "http://arxiv.org/abs/2508.17689",
  arxivid        = "2508.17689",
  code           = "https://github.com/DruvPai/diffusion_mem_gen"
}

@INPROCEEDINGS{Fang2025-kg,
  title         = "Beyond Scores: Proximal Diffusion Models",
  author        = "Fang, Zhenghan and Díaz, Mateo and Buchanan, Sam and Sulam,
                   Jeremias",
  month         =  dec,
  year          =  2025,
  booktitle     = "Advances in Neural Information Processing Systems (NeurIPS)",
  url           = "http://arxiv.org/abs/2507.08956",
  arxivid        = "2507.08956"
}


@book{ldrdd2025,
  title={Learning Deep Representations of Data Distributions},
  author={Buchanan*, Sam and Pai*, Druv and Wang, Peng and Ma, Yi},
  month=aug,
  year={2025},
  publisher={Online},
  note={\url{https://ma-lab-berkeley.github.io/deep-representation-learning-book/}.},
  html = "https://ma-lab-berkeley.github.io/deep-representation-learning-book/",
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%      Jax blog references
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{scaling-book,
    title = {How to Scale Your Model},
    author = {Austin, Jacob and Douglas, Sholto and Frostig, Roy and Levskaya, Anselm and Chen, Charlie and Vikram, Sharad
        and Lebron, Federico and Choy, Peter and Ramasesh, Vinay and Webson, Albert and Pope, Reiner},
    publisher = {Google DeepMind},
    howpublished = {Online},
    url = {https://jax-ml.github.io/scaling-book/},
    year = {2025},
    exclude = {true},
}

@online{jax_sharded_computation,
  title = {Introduction to parallel programming},
  author = {{JAX Team}},
  organization = {Google},
  url = {https://docs.jax.dev/en/latest/sharded-computation.html},
  urldate = {2025-08-20},
  year = {2025},
  note = {JAX Documentation},
  exclude = {true},
}

@online{jax-explicit-sharding,
  title = {Explicit sharding (a.k.a. ``sharding in types'')},
  author = {{JAX Team}},
  organization = {Google},
  url = {https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html},
  urldate = {2025-08-26},
  year = {2025},
  note = {JAX Documentation},
  exclude = {true},
}

@online{jax-training-cookbook,
  title = {The Training Cookbook},
  author = {{JAX Team}},
  organization = {Google},
  url = {https://docs.jax.dev/en/latest/the-training-cookbook.html},
  urldate = {2025-08-29},
  year = {2025},
  note = {JAX Documentation},
  exclude = {true},
}

@book{book-wright-ma,
author = {John Wright and Yi Ma},
title = {High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications},
publisher = {Cambridge University Press},
year = {2022},
exclude = {true},
}

@online{bernstein2025manifolds,
  title = {Modular Manifolds},
  author = {Jeremy Bernstein},
  organization = {Thinking Machines Lab},
  url = {https://thinkingmachines.ai/blog/modular-manifolds/},
  year = {2025},
  note = {Connectionism},
  doi = {10.64434/tml.20250926},
  exclude = {true},
}

@online{keigwin2025gramspace,
  title = {Gram-Space Manifold Muon},
  author = {Ben Keigwin and Dhruv Pai and Nathan Chen},
  organization = {Tilde Research},
  url = {https://www.tilderesearch.com/vignettes/gram-space},
  year = {2025},
  note = {Vignettes},
  exclude = {true},
}

@ARTICLE{Davis2024-ef,
  title     = "Stochastic algorithms with geometric step decay converge linearly
               on sharp functions",
  author    = "Davis, Damek and Drusvyatskiy, Dmitriy and Charisopoulos,
               Vasileios",
  journal   = "Mathematical programming",
  publisher = "Springer Science and Business Media LLC",
  volume    =  207,
  number    = "1-2",
  pages     = "145--190",
  month     =  sep,
  year      =  2024,
  url       = "http://dx.doi.org/10.1007/s10107-023-02003-w",
  doi       = "10.1007/s10107-023-02003-w",
  issn      = "0025-5610,1436-4646",
  language  = "en",
  exclude = {true},
}

@ARTICLE{Gao2020-cu,
  title     = "{ADMM} for multiaffine constrained optimization",
  author    = "Gao, Wenbo and Goldfarb, Donald and Curtis, Frank E",
  journal   = "Optimization methods \& software",
  publisher = "Informa UK Limited",
  volume    =  35,
  number    =  2,
  pages     = "257--303",
  month     =  mar,
  year      =  2020,
  url       = "http://dx.doi.org/10.1080/10556788.2019.1683553",
  doi       = "10.1080/10556788.2019.1683553",
  issn      = "1055-6788,1029-4937",
  language  = "en",
  exclude = {true},
}

@ARTICLE{Boyd2011-yb,
  title     = "Distributed Optimization and Statistical Learning via the
               Alternating Direction Method of Multipliers",
  author    = "Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja
               and Eckstein, Jonathan",
  journal   = "Foundations and Trends® in Machine Learning",
  publisher = "Now Publishers Inc.",
  address   = "Hanover, MA, USA",
  volume    =  3,
  number    =  1,
  pages     = "1--122",
  month     =  jan,
  year      =  2011,
  url       = "http://dx.doi.org/10.1561/2200000016",
  doi       = "10.1561/2200000016",
  issn      = "1935-8237",
  exclude = {true},
}

@ARTICLE{Amsel2025-qj,
  title         = "The Polar Express: Optimal matrix sign methods and their
                   application to the Muon algorithm",
  author        = "Amsel, Noah and Persson, David and Musco, Christopher and
                   Gower, Robert M",
  journal       = "arXiv [cs.LG]",
  month         =  sep,
  year          =  2025,
  url           = "http://arxiv.org/abs/2505.16932",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2505.16932",
  doi           = "10.48550/arXiv.2505.16932",
  exclude = {true},
}


@misc{buchanan2025mmuonadmm,
  author = {Buchanan, Sam},
  title = {Speeding Up Manifold Muon with {ADMM}},
  year = 2025,
  howpublished = {\url{https://sdbuchanan.com/blog/manifold-muon-admm/}},
  exclude = {true},
}
