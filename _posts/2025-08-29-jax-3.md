---
layout: post
title: "SPMD in JAX #3: NanoGPT (+ Mods) in JAX"
date: 2025-08-29
categories: jax
bibliography: refs.bib
published: false
---

In this third post in a series on programming with JAX for training
models like transformers, we write and train a transformer with a decent set of
bells and whistles, then benchmark and scale it as much as we can on our TPU v4
half-cube!
As in previous posts in the series, we make good use of the Google scaling
playbook {% cite scaling-book %}, as well as some useful JAX tutorials {% cite
jax-training-cookbook %}.

* TOC
{:toc}

### Setup

As usual, we'll test things out in Python 3.13 and JAX 0.7.1 on a single TPU v4
host. We'll scale things up later.

```python

import jax
import jax.numpy as jnp

jax.devices()
```
```
Output:
[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
```

# Design Choices

A project like this is for learning and pedagogy---it's inevitably behind the
research/infra frontiers. So the code we write is going to lean into this
somewhat. Notably,

> *We'll implement everything in **bare-metal JAX**, rather than using
neural network libraries like Flax/nnx/Equinox.*

This will let us expose and
understand all the implementation details, both for infra and architecture, that
we might normally overlook. On the flip side, we'll take some performance hits
for this (which we'll profile and understand), and it will lead to some slightly
unfortunate code repetition.[^1]

# Hacking

```python
from dataclasses import dataclass
from functools import partial
from typing import NamedTuple

from jax import Array
```


```python
# Architecture: config
@jax.tree_util.register_static
@dataclass(kw_only=True, frozen=True)
class Config:
    mesh_axis_names: tuple[str, ...] = ("dp",)
    mesh_shape: tuple[int, ...] = (4,)
    seed: int = 1337

    seq_length: int = 256
    global_batch_size: int = 128
    num_steps: int = 10**4
    lr = 1e-3

    num_vocab: int = 2**8
    d_model: int = 768
    mlp_factor: int = 4
    num_layers: int = 12
    param_std: float = 0.02
    param_dtype = jnp.bfloat16

    eps_ln: float = 1e-6
    use_bias_mlp: bool = True

    # TODO: look at bias impl, maybe shard differently
    sharding_mlp_up: jax.sharding.PartitionSpec = jax.P()
    sharding_mlp_down: jax.sharding.PartitionSpec = jax.P()
    sharding_hidden: jax.sharding.PartitionSpec = jax.P()
    sharding_res_stream: jax.sharding.PartitionSpec = jax.P()

    def __post_init__(self):
        mesh = jax.make_mesh(
            self.mesh_shape,
            self.mesh_axis_names,
            len(self.mesh_shape) * (jax.sharding.AxisType.Explicit,),
        )
        jax.sharding.set_mesh(mesh)
```

```python
# Architecture: building blocks


class Mlp(NamedTuple):
    w_up: Array
    bias_up: Array
    w_down: Array
    bias_down: Array


def _mlp(x: Array, params: Mlp, config: Config):
    preact = jnp.dot(x, params.w_up, out_sharding=config.sharding_hidden)
    if config.use_bias_mlp:
        # PERF: check that compiler fuses these
        preact += params.bias_up
    act = jax.nn.gelu(preact, approximate=True)
    out = jnp.dot(act, params.w_down, out_sharding=config.sharding_res_stream)
    if config.use_bias_mlp:
        out += params.bias_down
    return out


class Attn(NamedTuple):
    w_qkv: Array
    w_o: Array


def _attn(x_seq: Array, params: Attn, config: Config):
    return x_seq


class Embedding(NamedTuple):
    w: Array


def _embedding(token: jax.Array, params: Embedding, config: Config):
    emb = params.w.at[token].get(out_sharding=config.sharding_res_stream)
    return emb


class Unembedding(NamedTuple):
    w: Array


def _unembedding(x: Array, params: Unembedding, config: Config):
    logits = jnp.dot(x, params.w, out_sharding=config.sharding_res_stream)
    return logits


class LayerNorm(NamedTuple):
    gamma: Array
    beta: Array


def _layernorm(x: Array, params: LayerNorm, config: Config):
    x_std = jax.nn.standardize(x, epsilon=config.eps_ln)
    return params.gamma * x_std + params.beta
```


```python
# Architecture: derived components
class Block(NamedTuple):
    norm_attn: LayerNorm
    attn: Attn
    norm_mlp: LayerNorm
    mlp: Mlp


def _block(x_seq: Array, params: Block, config: Config):
    att_skip = x_seq
    out = jax.vmap(partial(_layernorm, params=params.norm_attn, config=config))(x_seq)
    out = _attn(out, params.attn, config)
    out += att_skip

    mlp_skip = out
    out = jax.vmap(partial(_layernorm, params=params.norm_mlp, config=config))(out)
    out = jax.vmap(partial(_mlp, params=params.mlp, config=config))(out)
    out += mlp_skip

    return out


class Transformer(NamedTuple):
    emb: Embedding
    blocks: Block  # vmapped at init
    unemb: Unembedding


def _transformer(tokens: Array, params: Transformer, config: Config):
    x_seq = jax.vmap(partial(_embedding, params=params.emb, config=config))(tokens)

    def _block_fun(x_seq: Array, params: Block):
        # TODO: output would be the KV cache (chunked operation), rn None
        return _block(x_seq, params, config), None

    out, _ = jax.lax.scan(_block_fun, x_seq, params.blocks)

    out = jax.vmap(partial(_unembedding, params=params.unemb, config=config))(out)
    return out
```

```python
# Basic architecture: init
def init_model_params(key, config: Config) -> Transformer:
    def init_embedding(key) -> Embedding:
        emb = config.param_std * jax.random.normal(
            key,
            (config.num_vocab, config.d_model),
            config.param_dtype,
            out_sharding=config.sharding_res_stream,
        )
        return Embedding(emb)

    def init_unembedding(key) -> Unembedding:
        unemb = config.param_std * jax.random.normal(
            key,
            (config.d_model, config.num_vocab),
            config.param_dtype,
            out_sharding=config.sharding_res_stream,
        )
        return Unembedding(unemb)

    def init_mlp(key) -> Mlp:
        k_w_up, k_w_down = jax.random.split(key, 2)
        w_up = config.param_std * jax.random.normal(
            k_w_up,
            (config.d_model, config.mlp_factor * config.d_model),
            config.param_dtype,
            out_sharding=config.sharding_mlp_up,
        )
        w_down = config.param_std * (
            jax.random.normal(
                k_w_down,
                (config.mlp_factor * config.d_model, config.d_model),
                config.param_dtype,
                out_sharding=config.sharding_mlp_down,
            )
        )
        bias_up = jnp.zeros(
            (config.mlp_factor * config.d_model,),
            config.param_dtype,
            out_sharding=config.sharding_hidden,
        )
        bias_down = jnp.zeros(
            (config.d_model,),
            config.param_dtype,
            out_sharding=config.sharding_res_stream,
        )
        return Mlp(w_up, bias_up, w_down, bias_down)

    def init_attn(key) -> Attn:
        return Attn(jnp.array(0, config.param_dtype), jnp.array(0, config.param_dtype))

    def init_layernorm() -> LayerNorm:
        gamma = jnp.ones(
            (config.d_model,),
            config.param_dtype,
            out_sharding=config.sharding_res_stream,
        )
        beta = jnp.zeros(
            (config.d_model,),
            config.param_dtype,
            out_sharding=config.sharding_res_stream,
        )
        return LayerNorm(gamma, beta)

    def init_block(key) -> Block:
        key_attn, key_mlp = jax.random.split(key)
        return Block(
            init_layernorm(),
            init_attn(key_attn),
            init_layernorm(),
            init_mlp(key_mlp),
        )

    # Make the full network
    key_emb, key_blocks, key_unemb = jax.random.split(key, 3)
    keys_blocks = jax.random.split(key_blocks, config.num_layers)
    return Transformer(
        init_embedding(key_emb),
        jax.vmap(init_block)(keys_blocks),
        init_unembedding(key_unemb),
    )
```

```python
# Test it out
config = Config(num_vocab=10, num_layers=1)

key = jax.random.key(config.seed)
key_params, key_data = jax.random.split(key, 2)

tf = init_model_params(key_params, config)
tokens = jnp.array((0, 1, 2, 3))


@jax.jit
def model(params, tokens):
    return _transformer(tokens, params, config)


out = model(tf, tokens)
print(jax.typeof(out))
```

```python
# Simple data sequence!
# Small, so mega replicate for ease

text = "012345678987654321" * 1024
seqs = [
    [int(c) for c in text[i : i + config.seq_length + 1]]
    for i in range(len(text) - config.seq_length - 1)
]
data = jnp.array(seqs, dtype=jnp.int32)


key_data, sk = jax.random.split(key_data)
data = jax.random.permutation(sk, data, axis=0)
num_data = len(data)
print(data[128])

Xtr = data[: int(0.8 * num_data)]
Xdev = data[int(0.8 * num_data) : int(0.9 * num_data)]
Xte = data[int(0.9 * num_data) :]
Xtr.shape
```


```python
# Simple dataloader
def get_batch(key, config: Config):
    offsets = jax.random.randint(key, (config.global_batch_size,), 0, num_data)
    return (Xtr.at[offsets, :-1].get(), Xtr.at[offsets, 1:].get())


key_data, sk = jax.random.split(key_data)
batch = get_batch(sk, config)
```

```python
# Simple training loop
# Start with SGD then adam (state)... then adamw (selective)... each challenges!
@jax.jit
def train_step(config: Config, batch, params: Transformer):
    def loss_fn(params: Transformer):
        inputs, targets = batch
        logits = jax.vmap(partial(_transformer, params=params, config=config))(inputs)
        logprobs = jax.nn.log_softmax(logits, axis=-1)
        return -jnp.take_along_axis(logprobs, targets[..., None], axis=-1).mean()

    loss, grad = jax.value_and_grad(loss_fn)(params)
    return loss, grad


params = tf
for step in range(config.num_steps):
    loss, grad = train_step(config, batch, params)
    params = jax.tree.map(lambda x, y: x - config.lr * y, params, grad)
    print(loss)
```

```python
jax.tree.map
```




```python
inputs, targets = batch
logits = jax.vmap(partial(_transformer, params=tf, config=config))(inputs)
logprobs = jax.nn.log_softmax(logits, axis=-1)
crossent = jnp.take_along_axis(logprobs, targets[..., None], axis=-1)
```









# Acknowledgments

Thanks to the [TRC program](https://sites.research.google/trc/about/) for
compute support.


[^1]: Namely, because we won't have a clean way to combine our model
    parameter definitions with the actual application of the model.