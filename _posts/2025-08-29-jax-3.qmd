---
layout: post
title: "SPMD in JAX #3: NanoGPT (+ Mods) in JAX"
date: 2025-08-29
categories: jax
bibliography: refs.bib
published: false
---

In this third post in a series on programming with JAX for training
models like transformers, we write and train a transformer with a decent set of
bells and whistles, then benchmark and scale it as much as we can on our TPU v4
half-cube!
As in previous posts in the series, we make good use of the Google scaling
playbook {% cite scaling-book %}, as well as some useful JAX tutorials {% cite
jax-training-cookbook %}.

* TOC
{:toc}

### Setup

As usual, we'll test things out in Python 3.13 and JAX 0.7.1 on a single TPU v4
host. We'll scale things up later.

```python
from functools import partial

import jax
import jax.numpy as jnp

mesh = jax.make_mesh((2, 2), ("x", "y"), (jax.sharding.AxisType.Explicit,) * 2)

A = jnp.zeros((1024, 1024), device=jax.sharding.NamedSharding(mesh, jax.P()))

print(jax.typeof(A))
print(jax.debug.visualize_array_sharding(A))
jax.devices()
```
```
Output:
[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
```

# Design Choices

A project like this is for learning and pedagogy---it's inevitably behind the
research/infra frontiers. So the code we write is going to lean into this
somewhat. Notably,

> *We'll implement everything in **bare-metal JAX**, rather than using
neural network libraries like Flax/nnx/Equinox.*

This will let us expose and
understand all the implementation details, both for infra and architecture, that
we might normally overlook. On the flip side, we'll take some performance hits
for this (which we'll profile and understand), and it will lead to some slightly
unfortunate code repetition.[^1]

# Hacking

```python
# Basic architecture
from collections import namedtuple

Mlp = namedtuple('Layer', ['W_up', 'bias_up', 'W_down', 'bias_down'])
```



# Acknowledgments

Thanks to the [TRC program](https://sites.research.google/trc/about/) for
compute support.


[^1]: Namely, because we won't have a clean way to combine our model
    parameter definitions with the actual application of the model.
