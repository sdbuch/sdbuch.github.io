<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Speeding Up Manifold Muon with ADMM | Sam D. Buchanan</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Speeding Up Manifold Muon with ADMM" />
<meta name="author" content="Sam Buchanan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In a recent blog post, Jeremy Bernstein (Thinking Machines) gave an algorithm for optimizing matrix-valued parameters—say, weights in a transformer—that ensures that both the update directions and the parameters themselves are well-conditioned (Bernstein, 2025). This algorithm, called manifold Muon, has an inner loop that is relatively slow to converge, and requires some hyperparameter tuning to get right." />
<meta property="og:description" content="In a recent blog post, Jeremy Bernstein (Thinking Machines) gave an algorithm for optimizing matrix-valued parameters—say, weights in a transformer—that ensures that both the update directions and the parameters themselves are well-conditioned (Bernstein, 2025). This algorithm, called manifold Muon, has an inner loop that is relatively slow to converge, and requires some hyperparameter tuning to get right." />
<link rel="canonical" href="http://sdbuchanan.com/blog/manifold-muon/" />
<meta property="og:url" content="http://sdbuchanan.com/blog/manifold-muon/" />
<meta property="og:site_name" content="Sam D. Buchanan" />
<meta property="og:image" content="http://sdbuchanan.com/assets/blog/manifold-muon-admm-og.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-20T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://sdbuchanan.com/assets/blog/manifold-muon-admm-og.png" />
<meta property="twitter:title" content="Speeding Up Manifold Muon with ADMM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Sam Buchanan"},"dateModified":"2025-10-20T00:00:00+00:00","datePublished":"2025-10-20T00:00:00+00:00","description":"In a recent blog post, Jeremy Bernstein (Thinking Machines) gave an algorithm for optimizing matrix-valued parameters—say, weights in a transformer—that ensures that both the update directions and the parameters themselves are well-conditioned (Bernstein, 2025). This algorithm, called manifold Muon, has an inner loop that is relatively slow to converge, and requires some hyperparameter tuning to get right.","headline":"Speeding Up Manifold Muon with ADMM","image":"http://sdbuchanan.com/assets/blog/manifold-muon-admm-og.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://sdbuchanan.com/blog/manifold-muon/"},"url":"http://sdbuchanan.com/blog/manifold-muon/"}</script>
<!-- End Jekyll SEO tag -->


  <link rel="stylesheet" href="/assets/main.css">
  
  
  <link rel="alternate" type="application/rss+xml" title="Sam D. Buchanan" href="http://sdbuchanan.com/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js"></script>

<!-- MathJax -->
<script type="text/javascript">
  const macros = {};

  // Generate BB letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`bb${letter}`] = `{\\mathbb{${letter}}}`;
  }

  // Generate script letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`c${letter}`] = `{\\mathscr{${letter}}}`;
  }

  // Generate calligraphic letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`s${letter}`] = `{\\mathcal{${letter}}}`;
  }

  // Generate vector uppercase letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Generate vector lowercase letters
  for (let i = 97; i <= 122; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Add Greek letters
  const greekLetters = [
    'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'varepsilon', 'zeta', 'eta',
    'theta', 'vartheta', 'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi',
    'pi', 'varpi', 'rho', 'varrho', 'sigma', 'varsigma', 'tau', 'upsilon',
    'phi', 'varphi', 'chi', 'psi', 'omega', 'Gamma', 'Delta', 'Theta',
    'Lambda', 'Xi', 'Pi', 'Sigma', 'varSigma', 'Upsilon', 'Phi', 'Psi',
    'Omega', 'ell'
  ];
  greekLetters.forEach(letter => {
    macros[`v${letter}`] = `{\\boldsymbol{\\${letter}}}`;
  });

  // Add these to your macros object
  const additionalMacros = {
    // Basic macros
    "Beta": "{\\mathrm{B}}",
    "eps": "{\\epsilon}",
    "Diff": "{\\mathop{}\\!\\mathrm{D}}",
    "diff": "{\\mathop{}\\!\\mathrm{d}}",
    "Partial": ["{\\frac{\\partial #1}{\\partial #2}}", 2],
    "PartialN": ["{\\frac{\\partial^{#3} #1}{\\partial {#2}^{#3}}}", 3],
    "dPartial": ["{\\dfrac{\\partial #1}{\\partial #2}}", 2],
    "PPartial": ["{\\tfrac{\\partial}{\\partial #1}}", 1],
    "dac": "{\\left.\\frac{\\partial}{\\partial t}\\right|_{t=0}}",
    "half": "{\\tfrac{1}{2}}",
    "third": "{\\tfrac{1}{3}}",
    "fourth": "{\\tfrac{1}{4}}",
    "sixth": "{\\tfrac{1}{6}}", // Fixed typo (was 1/4)
    "eighth": "{\\tfrac{1}{8}}",

    // Accents and decorations
    "overbar": ["{\\mkern 1.5mu\\overline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "underbar": ["{\\mkern 1.5mu\\underline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "conj": ["{\\overbar{#1}}", 1],
    "wh": "{\\widehat}",
    "wt": "{\\widetilde}",
    "ol": ["{\\overbar{#1}}", 1],
    "kron": "{\\otimes}",
    "elwise": "{\\odot}", // Using \odot as circleddot is not standard
    "dsum": "{\\oplus}",
    "spcdot": "{\\,\\cdot\\,}",

    // Special symbols
    "iu": "{\\mathfrak{i}}",
    "given": [' \\, \\vert \\, ', 0],
    "llangle": ['\\langle\\!\\langle', 0],
    "rrangle": ['\\rangle\\!\\rangle', 0],

      // Analysis operators
    "Lip": "{\\mathrm{Lip}}",
    "mem": "{\\mathrm{mem}}",
    "softmax": "{\\operatorname{\\mathrm{softmax}}}",
    "equid": "{\\overset{d}{=}}",
    "xor": "{\\oplus}",
    "bigxor": "{\\bigoplus}",
    "minimize": ["{\\underset{#1}{\\operatorname{minimize}}}", 1],
    "maximize": ["{\\underset{#1}{\\operatorname{maximize}}}", 1],
    "argmin": "{\\mathop{\\mathrm{arg\\,min}}}",
    "argmax": "{\\mathop{\\mathrm{arg\\,max}}}",
    "orth": "{\\operatorname{orth}}",
    "ow": "{\\mathrm{otherwise}}",
    "iid": "{\\mathrm{i.i.d.}}",
    "wass": "{\\mathrm{W}}",
    "TV": "{\\mathrm{TV}}",
    "as": "{\\mathrm{a.s.}}",
    "whp": "{\\mathrm{w.h.p.}}",
    "simiid": "{\\sim_{\\iid}}",
    "ltsim": "{\\lesssim}",
    "gtsim": "{\\gtrsim}",
    "ltsimwhp": "{\\underset{\\whp}{\\lesssim}}",
    "gtsimwhp": "{\\underset{\\whp}{\\gtrsim}}",
    "psdgeq": "{\\succcurlyeq}",
    "psdleq": "{\\preccurlyeq}",
    "defn": "{\\overset{\\text{def}}{=}}",
    "normsubdiff": ["{\\partial\\norm{}_{#1}}", 1],
    "normalize": ["{\\frac{#1}{\\norm*{#1}_2}}", 1],
    "normalizeabs": ["{\\frac{#1}{\\abs*{#1}}}", 1],
    "iter": ["{#1^{(#2)}}", 2],
    "prox": ["{\\operatorname{prox}}_{#1}", 1],

    // Vector operators
    "tr": "{\\operatorname{tr}}",
    "diag": "{\\operatorname{diag}}",
    "Diag": "{\\operatorname{Diag}}",
    "vect": "{\\operatorname{vec}}",
    "vec": "{\\operatorname{vec}}",
    "Sym": "{\\operatorname{sym}}",
    "Symm": "{\\operatorname{sym}}",
    "Skew": "{\\operatorname{skew}}",
    "rank": "{\\operatorname{rank}}",
    "krank": "{\\operatorname{krank}}",
    "sign": "{\\operatorname{sign}}",
    "sgn": "{\\operatorname{sgn}}",
    "supp": "{\\operatorname{supp}}",
    "esssup": "{\\mathop{\\operatorname{ess\\,sup}}}",
    "vol": "{\\operatorname{vol}}",
    "Vol": "{\\operatorname{Vol}}",
    "tp": "{^{\\mathrm{T}}}",
    "adj": "{^{\\ast}}",
    "inv": "{^{-1}}",
    "One": "{\\mathbf{1}}",
    "Zero": "{\\mathbf{0}}",
    "Id": "{\\operatorname{\\mathrm{Id}}}",
    "conv": "{\\mathbin{\\ast}}",
    "iconv": "{\\mathbin{\\square}}",
    "xcorr": "{\\mathbin{\\star}}",
    "cconv": "{\\mathbin{\\circledast}}",
    "frob": "{\\mathrm{F}}",
    "HS": "{\\mathrm{HS}}",

    // Trig stuff
    "acos": "{\\operatorname{\\cos\\inv}}",
    "asin": "{\\operatorname{\\sin\\inv}}",
    "atan": "{\\operatorname{\\tan\\inv}}",
    "sech": "{\\operatorname{sech}}",
    "csch": "{\\operatorname{csch}}",

    // Calculus/geometry operators
    "Hess": "{\\operatorname{Hess}}",
    "grad": "{\\operatorname{grad}}",
    "Div": "{\\operatorname{div}}",
    "curl": "{\\operatorname{curl}}",
    "downto": "{\\searrow}",
    "upto": "{\\nearrow}",

    // CS operators
    "polylog": "{\\operatorname{polylog}}",
    "poly": "{\\operatorname{poly}}",

    // Stats operators
    "Ind": ["{\\mathds{1}}_{#1}", 1],
    "stddev": "{\\operatorname{stddev}}",
    "Unif": ["{\\operatorname{Unif}(#1)}", 1],
    "Bern": ["{\\operatorname{Bern}(#1)}", 1],
    "Pois": ["{\\operatorname{Pois}(#1)}", 1],
    "Binom": ["{\\operatorname{Binom}(#1, #2)}", 2],
    "Exp": "{\\operatorname{Exp}}",
    "BG": "{\\operatorname{BG}}",
    "Law": "{\\mathrm{Law}}",
    "indep": "{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}",
    "independenT": ["{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}", 2],

    // Topology / set operators
    "compl": "{\\mathsf{c}}",
    "bd": "{\\operatorname{bd}}",
    "relbd": "{\\operatorname{relbd}}",
    "cl": "{\\operatorname{cl}}",
    "Conv": "{\\operatorname{conv}}",
    "dom": "{\\operatorname{dom}}",
    "epi": "{\\operatorname{epi}}",
    "aff": "{\\operatorname{aff}}",
    "cone": "{\\operatorname{cone}}",
    "ri": "{\\operatorname{ri}}",
    "im": "{\\operatorname{im}}",
    "Hom": "{\\operatorname{Hom}}",
    "End": "{\\operatorname{End}}",
    "Aut": "{\\operatorname{Aut}}",
    "Null": "{\\operatorname{null}}",
    "Span": "{\\operatorname{span}}",
    "row": "{\\operatorname{row}}",
    "col": "{\\operatorname{col}}",
    "range": "{\\operatorname{range}}",
    "Ran": "{\\operatorname{ran}}",
    "diam": "{\\operatorname{diam}}",
    "len": "{\\operatorname{len}}",
    "dist": "{\\operatorname{dist}}",
    "nnz": "{\\operatorname{nnz}}",
    "RE": "{\\operatorname{RE}}",
    "err": "{\\operatorname{err}}",
    "circulant": "{\\operatorname{circ}}",
    "tre": "{\\operatorname{tre}}",
    "etr": "{\\operatorname{etr}}",
    "proj": ["{\\operatorname{proj}_{#1}}", 1]
  };

  const delimitersToDefine = [
    // Simple paired delimiters
    "\\DeclarePairedDelimitersX{\\abs[1]}{\\lvert}{\\rvert}{ \ifblank{#1}{\:\cdot\:}{#1} }",
  ];

  const all_macros = { ...macros, ...additionalMacros };

  window.MathJax = {
    loader: {
      load: ['[tex]/boldsymbol', '[tex]/mathtools', '[tex]/ams', '[tex]/color']
    },
    tex: {
      packages: {'[+]': ['boldsymbol', 'mathtools', 'ams', 'color']},
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      tags: 'ams',
      macros: all_macros,
      mathtools: {
        pairedDelimiters: {
          // General pairing commands
          abs: ['\\lvert', '\\rvert', '{#1}', 1, '', ''],
          norm: ['\\lVert', '\\rVert', '{#1}', 1, '', ''],
          nnorm: ['\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '{#1}', 1, '', ''],
          ip: ['\\langle', '\\rangle', '{#1}, {#2}', 2, '', ''],
          iip: ['\\llangle', '\\rrangle', '{#1}, {#2}', 2, '', ''],
          ceil: ['\\lceil', '\\rceil', '{#1}', 1, '', ''],
          floor: ['\\lfloor', '\\rfloor', '{#1}', 1, '', ''],
          KL: ['(', ')', '{#1} \\:\\|\\: {#2}', 2, '\\mathop{\\mathrm{KL}}', ''],

          // Set type commands
          set: ['\\{', '\\}', '{#1}', 1, '', ''],
          Pr: ['[', ']', '{#1}', 1, '\\mathbb{P}', ''],
          Prsub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{P}}_{#1}', ''],
          E: ['[', ']', '{#1}', 1, '\\mathbb{E}', ''],
          Esub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{E}}_{#1}', ''],
          Var: ['[', ']', '{#1}', 1, '\\mathrm{Var}', ''],
          cov: ['[', ']', '{#1}', 1, '\\mathrm{cov}', ''],
          Ent: ['[', ']', '{#2}', 2, '\\mathop{\\mathrm{Ent}}_{#1}', '']
        }
      }
    }
  };
</script>
<!-- <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script> -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>


  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Sam D. Buchanan</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Home</a>
      
        
        <a class="page-link" href="/publications/">Publications</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="/misc/">Misc.</a>
      
        
        <a class="page-link" href="/assets/cv.pdf">CV</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Speeding Up Manifold Muon with ADMM</h1>
    
    <p class="post-meta"><time datetime="2025-10-20T00:00:00+00:00" itemprop="datePublished">Oct 20, 2025</time> •
  
    
    
      
    
      
    
      
        <a href="/categories/optimization/">optimization</a>
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In a recent blog post, Jeremy
Bernstein (Thinking Machines) gave an algorithm for optimizing matrix-valued
parameters—say, weights in a transformer—that ensures that both the update
directions and the parameters themselves are well-conditioned <a class="citation" href="#bernstein2025manifolds">(Bernstein, 2025)</a>.
This algorithm, called <em>manifold Muon</em>, has an inner loop that is relatively slow to
converge, and requires some hyperparameter tuning to get right.</p>

<figure>
    <img src="/assets/blog/admm-dual-ascent.png" alt="Comparing
    dual ascent to ADMM rate of convergence" />
    <figcaption>Comparison of the default manifold Muon inner loop solver (dual ascent)
    to a solver based on ADMM, for one weight matrix in a neural network being trained.
    ADMM converges much more rapidly, without needing hyperparameter tuning!</figcaption>
</figure>

<p>In this post, we’ll describe a modification of the manifold Muon algorithm that greatly
speeds up convergence—by more than a factor of two in wall-clock time in our experiments
on 1x H100! To realize this, we apply a venerable algorithm from convex optimization
called ADMM (the “alternating direction method of multipliers”) to the optimization
problem at the heart of the manifold Muon update.</p>

<p>The rest of the post will re-introduce the manifold Muon subproblem, give some intuition
for why the <a href="https://github.com/thinking-machines-lab/manifolds">existing
implementation</a> leaves some room for
improvement on the table, and walk through the ADMM derivation for manifold Muon. We’ll
verify the improved algorithm on the same toy model used in the linked Github
repository. We hope this improvement will facilitate larger-scale experimentation
with manifold Muon!</p>

<p>If you haven’t already, you’re encouraged to read <a href="https://thinkingmachines.ai/blog/modular-manifolds/">Jeremy’s blog
post</a> on manifold Muon before
proceeding. It gives an excellent technical overview and derivation of the method, and
accessible, well-written intuition on the deep links between optimization and geometry.</p>

<h1 id="background-manifold-muon">Background: Manifold Muon</h1>

<p>Manifold Muon is a first-order method for optimizing a matrix-valued parameter $\vW \in
\bbR^{m \times n}$ (we’ll assume $m \geq n$ throughout), such that it satisfies the
quadratic constraint $\vW^\top \vW = \vI$. Given a putative update direction $\vG \in
\bbR^{m \times n}$—say, back-propagated gradients of some loss with respect to
$\vW$—and a step size $\eta &gt; 0$, we choose an update $\vA$ by solving the
convex optimization problem<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">1</a></sup></p>

\[\begin{equation}\label{eq:manifold-muon}
    \min_{\vA \in \bbR^{m \times n}}\, \ip{\vA}{\vG}
    \quad
    \text{subject to}
    \quad
    \norm{\vA} \leq \eta,
    \quad
    \vA^\top \vW + \vW^\top \vA  = \Zero,
    \end{equation}\]

<p>then perform the update</p>

\[\vW \mapsto \mathrm{msign} \left( \vW + \vA  \right) ,\]

<p>where $\mathrm{msign}(\vX) = \vX (\vX^\top \vX)^{-1/2}$ is the matrix sign function.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">2</a></sup></p>

<p>The manifold Muon subproblem \eqref{eq:manifold-muon} does not
have a closed-form solution in general, so Jeremy proposed
an iterative solver for it, based on the equivalent <em>unconstrained</em> formulation<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

\[\begin{equation}\label{eq:manifold-muon-dual}
    \min_{\vLambda \in \bbR^{n \times n}}\,
    \norm*{\vG + \vW\left( \vLambda + \vLambda^\top \right)}_*.
    \end{equation}\]

<p>One can obtain an optimal solution $\vA_\star$ to \eqref{eq:manifold-muon} from an
optimal solution $\vLambda_\star$ to \eqref{eq:manifold-muon-dual} via $\vA_\star =
-\eta \mathop{\mathrm{msign}}( \vG + \vW(\vLambda_\star + \vLambda_\star^\top))$.</p>

<p>In practice, we solve \eqref{eq:manifold-muon-dual} with subgradient descent:
we pick an iteration count $K \in \bbN$ and step sizes $\gamma_k &gt; 0$ for $k = 1, \dots, K$,
and repeatedly perform the update</p>

\[\begin{equation}\label{eq:subgradient}
\begin{split}
    \vGamma_{k} &amp;=
    \mathop{\mathrm{msign}}( \vG + \vW(\vLambda_k + \vLambda_k^\top))^\top \vW
    + \vW^\top\mathop{\mathrm{msign}}( \vG + \vW(\vLambda_k + \vLambda_k^\top)); \\
    \vLambda_{k+1} &amp;= \vLambda_k
    -
    \gamma_k \vGamma_k.
    \end{split}
\end{equation}\]

<p>The step sizes $\gamma_k$ are chosen to decay as $k \to K$ to guarantee convergence.
The gradient $\vGamma_k$ is computed efficiently on modern accelerators using
Newton-Schulz iterations, or optimal algorithms such as the “Polar Express” algorithm
<a class="citation" href="#Amsel2025-qj">(Amsel et al., 2025)</a>, as in the <a href="https://github.com/thinking-machines-lab/manifolds/">Github for Jeremy’s
blog</a>.</p>

<h1 id="drawbacks-of-subgradient-descent">Drawbacks of Subgradient Descent</h1>

<p>The subgradient descent iteration \eqref{eq:subgradient} is guaranteed to converge, but
it does so <strong>very slowly</strong>!
In fact, its worst-case rate of convergence is $O(1/\sqrt{k})$ <a class="citation" href="#book-wright-ma">(Wright &amp; Ma, 2022)</a>,
in contrast to the standard $O(1/k)$ rate for gradient descent on smooth objectives.</p>

<p>Even worse, this is <em>typical</em> behavior for this algorithm, not just worst-case.
It stems from the fact that subgradient descent is an algorithm for <em>nonsmooth</em>
problems, and nonsmoothness leads to a ‘chattering’ behavior of iterates. Here is a
quick simulation on a canonical example, the absolute value function $\abs{\spcdot}$ in
dimension one, to illustrate:<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">4</a></sup></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">math</span> <span class="kn">import</span> <span class="n">sqrt</span>

<span class="n">init</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">iters</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">it</span> <span class="o">=</span> <span class="n">init</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">it</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">it</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="n">iters</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<figure>
    <img src="/assets/blog/subgradient_descent.gif" alt="Subgradient
    descent on the absolute value function" />
    <figcaption>Subgradient descent on the absolute value function, with $1/\sqrt{k}$ LR
    decay.</figcaption>
</figure>

<p>The nuclear norm objective in \eqref{eq:manifold-muon-dual} is nondifferentiable
at every point $\vLambda$ for which $\vG + \vW(\vLambda + \vLambda^\top)$ is not full
rank. Even if the iterates $\vLambda_k$ never witness such a point of
nondifferentiability, the lack of continuity of the gradient near these points leads to
the same ‘chattering’ behavior of the iterates as in the toy example above.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">5</a></sup></p>

<p>Fortunately, we can do better, with a bit of algorithmic cleverness (and without too
much more compute)!</p>

<h1 id="a-faster-algorithm-from-admm-with-splitting">A Faster Algorithm from ADMM with Splitting</h1>

<p>ADMM is an optimization algorithm for solving problems of the form</p>

\[\begin{equation}\label{eq:admm}
\begin{split}
    \min_{\vx \in \bbR^{m}, \vz \in \bbR^{n}}\, &amp;f(\vx) + g(\vz) \\
    \text{subject to}\,\, &amp;\vA \vx + \vB \vz = \vc,
    \end{split}
\end{equation}\]

<p>where $f$ and $g$ are convex.
It was originally developed in the 1970s, and was re-popularized with a generation or two
of researchers by the excellent monograph of Boyd, Parikh et al. <a class="citation" href="#Boyd2011-yb">(Boyd et al., 2011)</a>,
which we recommend as a general reference beyond the rapid overview we give here.
The key empirical property that makes ADMM very well suited for use in solving the
manifold Muon subproblem \eqref{eq:manifold-muon-dual} is its <em>rapid initial
convergence</em> to a solution of reasonable quality—the algorithm’s worst-case
convergence rate is only $O(1/k)$, but this initial rapid convergence often allows it to
be stopped much earlier.</p>

<p>The ADMM algorithm solves problems of the form \eqref{eq:admm} as follows. For a penalty
parameter $\rho &gt; 0$, we define the “augmented Lagrangian”</p>

\[\sL_{\rho}(\vx, \vz, \vlambda)
    = f(\vx) + g(\vz) + \ip{\vlambda}{\vA \vx + \vB \vz - \vc} + \frac{\rho}{2} \norm*{
        \vA \vx + \vB \vz - \vc
    }_2^2,\]

<p>then iteratively perform the updates</p>

\[\begin{equation}\label{eq:admm-update}
\begin{split}
    \vx_{k+1} &amp;= \argmin_{\vx}\, \sL_{\rho}(\vx, \vz_k, \vlambda_k) \\
    \vz_{k+1} &amp;= \argmin_{\vz}\, \sL_{\rho}(\vx_{k+1}, \vz, \vlambda_k) \\
    \vlambda_{k+1} &amp;= \vlambda_k + \rho \left( \vA \vx_{k+1} + \vB \vz_{k+1} - \vc \right).
    \end{split}
\end{equation}\]

<p>The algorithm is reminiscent of a block coordinate descent method, with gradient ascent
on the dual variable $\vlambda$ (with step size $\rho$).
It converges for any setting of the penalty parameter $\rho$. Empirically, this
parameter can be tuned to accelerate convergence.</p>

<p>ADMM is ideally suited for problems where the minimization subproblems in the $\vx$ and
$\vz$ updates in \eqref{eq:admm-update} have closed-form solutions. For the manifold
Muon subproblem \eqref{eq:manifold-muon-dual}, we need to pass two related obstacles:</p>

<ol>
  <li>How do we apply ADMM to \eqref{eq:manifold-muon-dual}? It doesn’t seem to have the
ADMM-amenable structure of \eqref{eq:admm}.</li>
  <li>Having resolved this, do the minimization subproblems in the ADMM update
\eqref{eq:admm-update} applied to \eqref{eq:manifold-muon-dual} have closed-form
solutions that can be computed efficiently on GPUs/TPUs?</li>
</ol>

<h2 id="the-immortal-splitting-trick">The Immortal Splitting Trick</h2>

<p>To resolve the first issue, we apply a trick known as <em>variable splitting</em>. We add an
auxiliary variable $\vX \in \bbR^{m \times n}$ to the manifold Muon subproblem
\eqref{eq:manifold-muon-dual} along with an extra constraint to give the equivalent
problem</p>

\[\begin{equation}\label{eq:manifold-muon-split}
\begin{split}
    \min_{\vLambda \in \bbR^{n \times n}, \vX \in \bbR^{m \times n}}\, &amp;\norm*{\vX}_* \\
    \text{subject to}\,\, &amp;\vX = \vG + \vW(\vLambda + \vLambda^\top).
\end{split}
\end{equation}\]

<p>This may initially seem counterintuitive—didn’t we work hard to reduce the original
manifold Muon problem \eqref{eq:manifold-muon} to an unconstrained form? But the
advantage is that this problem is now amenable to ADMM, since it has the form
of \eqref{eq:admm} after we re-interpret matrices as vectors!</p>

<p>For this problem, given that we’ve already used $\vLambda$ for a dual variable, we’ll
write the ADMM dual variable as $\vOmega \in \bbR^{m \times n}$, giving the augmented
Lagrangian</p>

\[\begin{equation}\label{eq:augmented-lagrangian}
\begin{split}
    \sL_{\rho}(\vLambda, \vX, \vOmega)
    = \norm*{\vX}_* &amp;+ \ip{\vOmega}{\vX - \vW(\vLambda + \vLambda^\top) - \vG}  \\
    &amp;+ \frac{\rho}{2} \norm*{ {\vX - \vW(\vLambda + \vLambda^\top) - \vG} }_{\frob}^2.
    \end{split}
\end{equation}\]

<p>Now we can work on the ADMM subproblems for this augmented Lagrangian.</p>

<h2 id="admm-subproblems-for-manifold-muon">ADMM Subproblems for Manifold Muon</h2>

<p>In deriving ADMM algorithms for specific problems, it’s often useful to “complete the
square” to more easily see the solutions to the minimization subproblems. We rewrite
\eqref{eq:augmented-lagrangian} as</p>

\[\begin{equation}
\begin{split}
    \sL_{\rho}(\vLambda, \vX, \vOmega)
    &amp;= \norm*{\vX}_* + \frac{\rho}{2}\norm*{\frac{1}{\rho}\vOmega}_{\frob}^2
    - \frac{\rho}{2}\norm*{\frac{1}{\rho}\vOmega}_{\frob}^2 +
    \ip{\vOmega}{\vX - \vW(\vLambda + \vLambda^\top) - \vG}  \\
    &amp;\qquad\qquad+ \,\frac{\rho}{2} \norm*{ {\vX - \vW(\vLambda + \vLambda^\top) - \vG} }_{\frob}^2 \\
    &amp;= \norm*{\vX}_*
    - \frac{\rho}{2}\norm*{\frac{1}{\rho}\vOmega}_{\frob}^2 +
      \frac{\rho}{2} \norm*{ \frac{1}{\rho} \vOmega + {\vX - \vW(\vLambda + \vLambda^\top) - \vG} }_{\frob}^2.
    \end{split}
\end{equation}\]

<p>Now the dependence on $\vLambda$ is an easy-to-parse quadratic function, and the
dependence on $\vX$ is also simple.</p>

<h3 id="vlambda-subproblem">$\vLambda$ Subproblem</h3>

<p>To solve</p>

\[\argmin_{\vLambda}\, \sL_{\rho}(\vLambda, \vX_k, \vOmega_k)
    =
    \argmin_{\vLambda}\,
    \norm*{ \frac{1}{\rho} \vOmega_k + {\vX_k - \vW(\vLambda + \vLambda^\top) - \vG} }_{\frob}^2,\]

<p>it’s helpful to define $\Sym(\vLambda) = \half ( \vLambda + \vLambda^\top)$. This is
a linear operator, and in fact is an <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Projection_matrix">orthogonal
projection</a>.
Then because $\vW^\top \vW = \vI$, we have</p>

\[\argmin_{\vLambda}\, \sL_{\rho}(\vLambda, \vX_k, \vOmega_k)
    =
    \argmin_{\vLambda}\,
    \norm*{ \Sym(\vLambda) - \frac{1}{2}\vW^\top\left(
    \frac{1}{\rho} \vOmega_k + \vX_k - \vG
    \right)
    }_{\frob}^2.\]

<p>It then follows that</p>

\[\frac{1}{2}\Sym\left(\vW^\top\left(
    \frac{1}{\rho} \vOmega_k + \vX_k - \vG
    \right)\right)
    \in
    \argmin_{\vLambda}\, \sL_{\rho}(\vLambda, \vX_k, \vOmega_k),\]

<p>and that this is the minimum Euclidean norm solution to this problem.</p>

<p>In particular, we can solve the $\vLambda$ subproblem in closed form, using only matrix
multiplies and transposes!</p>

<h3 id="vx-subproblem">$\vX$ Subproblem</h3>

<p>We have to solve</p>

\[\begin{split}
    \argmin_{\vX}\, &amp;\sL_{\rho}(\vLambda_{k+1}, \vX, \vOmega_k)
    =
    \\
    &amp;\qquad\frac{1}{\rho}\norm*{\vX}_*
    + \frac{1}{2} \norm*{ \vX + \frac{1}{\rho} \vOmega_k - \vW(\vLambda_{k+1} + \vLambda_{k+1}^\top) - \vG }_{\frob}^2.
\end{split}\]

<p>This optimization problem actually has a closed-form solution called <em>singular value
thresholding</em>, which can be derived by considering the SVD of everything that doesn’t
depend on $\vX$ in the quadratic term.
Namely, if we define $\vM = \vW(\vLambda_{k+1} + \vLambda_{k+1}^\top) + \vG - \frac{1}{\rho} \vOmega_k$
for concision and let $\vU$, $\diag(\vs)$, $\vV$
denote an economy SVD of $\vM$,
then it can be shown that <a class="citation" href="#book-wright-ma">(Wright &amp; Ma, 2022)</a></p>

\[\argmin_{\vX}\, \sL_{\rho}(\vLambda_{k+1}, \vX, \vOmega_k)
    =
    \vU \diag \left( \max \set*{ \vs - \frac{1}{\rho}, 0 } \right)\vV^\top,\]

<p>with the natural broadcasting interpretation. Thus, the $\vX$ subproblem can be solved
in closed-form with a singular value decomposition!
Unfortunately, this is not enough for our purposes: SVD algorithms are not as
well-suited for scaling up on GPUs/TPUs as algorithms that rely purely on matrix
multiplications.</p>

<p>Fortunately, it turns out that we can compute singular value thresholding <em>using only
the $\mathrm{msign}$ algorithm</em>, letting us exploit efficient algorithms for this
operation that have already been developed for Muon!<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> There are likely to be better
custom solutions for this problem, but this quick-and-dirty approach will get us on our
way. Here’s how:</p>

<ol>
  <li>
    <p>First, we make a simple observation: <em>if $s_i - 1/\rho \geq 0$ for every singular
value $s_i$, then</em></p>

\[\vU \diag \left( \max \set*{ \vs - \frac{1}{\rho}, 0 } \right)\vV^\top
 =
 \vM - \frac{1}{\rho} \vU \vV^\top.\]

    <p>Since $\vU\vV^\top = \mathrm{msign}(\vM)$, we’re done!</p>
  </li>
  <li>
    <p>We cannot rely on every singular value being larger than $1/\rho$ in general. But,
observe that those singular values that are smaller simply need to be set to $0$,
which we can achieve by other means.
More precisely, since $n \leq m$, consider the matrix
$\mathrm{msign}(\vM^\top \vM - \frac{1}{\rho^2} \vI)$.
Let $\vV_{\geq}$ denote the submatrix of (right) singular vectors of $\vM$
associated to singular values that are greater than or equal to $1/\rho$, and
$\vV_{&lt;}$ the complementary submatrix. Then one has</p>

\[\mathrm{msign}(\vM^\top \vM - \frac{1}{\rho^2} \vI)
=
\vV_{\geq} \vV_{\geq}^\top - \vV_{&lt;} \vV_{&lt;}^\top.\]

    <p>Each of the (unsigned) factors in this sum is an orthogonal projection matrix. Hence
we have</p>

\[\frac{1}{2} \left( \vI + \mathrm{msign}(\vM^\top \vM - \frac{1}{\rho^2} \vI) \right)
=
\frac{1}{2} \left(\vV_{\geq} \vV_{\geq}^\top + \left(\vI - \vV_{&lt;} \vV_{&lt;}^\top \right) \right)
=
\vV_{\geq} \vV_{\geq}^\top.\]

    <p>The matrix $\vV_{\geq} \vV_{\geq}^\top$ <em>projects onto the right singular vectors of
$\vM$ associated to singular values at least as large as $1/\rho$</em>. This is exactly
what we need to implement the zero clipping operation in singular value
thresholding!</p>
  </li>
</ol>

<p>In summary, we’ve shown how to compute singular value thresholding using only matrix
sign operations. Zooming out, we’ve shown that the solution to the $\vX$ subproblem in
our manifold Muon ADMM algorithm is</p>

\[\argmin_{\vX}\, \sL_{\rho}(\vLambda_{k+1}, \vX, \vOmega_k)
    =
    \frac{1}{2}\left( \vM - \frac{1}{\rho} \mathrm{msign}(\vM) \right)
    \left( \vI + \mathrm{msign}\left(\vM^\top \vM - \frac{1}{\rho^2} \vI\right) \right).\]

<p>This can be computed using only matrix multiplications and matrix sign operations, which
themselves are efficiently approximated using matrix multiplications. Almost as
hardware-efficient as we could hope for!</p>

<p>We can instantiate a special case of the manifold Muon ADMM solver using our
work above in order to compare to the</p>

<h2 id="admm-for-manifold-muon-final-algorithm">ADMM for Manifold Muon: Final Algorithm</h2>

<p>Plugging our derivations above into the basic ADMM skeleton
\eqref{eq:admm-update} leads to the following final algorithm, here in math
form:<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup></p>

\[\begin{equation}\label{eq:admm-update-mfld-muon}
\begin{split}
    \vLambda_{k+1} &amp;=
    \frac{1}{2}\Sym\left(\vW^\top\left(
    \frac{1}{\rho} \vOmega_k + \vX_k - \vG
    \right)\right) \\
    \vM_{k+1} &amp;=
    2\vW\vLambda_{k+1} + \vG - \frac{1}{\rho} \vOmega_k
    \\
    \vX_{k+1} &amp;=
    \frac{1}{2}\left( \vM_{k+1} - \frac{1}{\rho} \mathrm{msign}(\vM_{k+1}) \right)
    \left( \vI + \mathrm{msign}\left(\vM_{k+1}^\top \vM_{k+1} - \frac{1}{\rho^2} \vI\right) \right) \\
    \vOmega_{k+1} &amp;= \vOmega_k + \rho \left( \vX_{k+1} - 2\vW\vLambda_{k+1} - \vG \right).
    \end{split}
\end{equation}\]

<p>When we’re done iterating, just as we did for dual ascent,
we return $\vA_\star =
-\eta \mathop{\mathrm{msign}}( \vG + 2\vW\vLambda_\star)$.</p>

<p>In code form, implementing the ADMM update we have derived above in PyTorch and
combining it with the remainder of the manifold Muon algorithm leads to the
following. This code is available in <a href="https://github.com/sdbuch/thinky-manifolds">my fork of Jeremy’s
Github repository</a>, and it’s
a modification of Jeremy’s original code. Disclaimer: it has not been heavily
optimized, beyond choosing sensible defaults for the hyperparameters via
experiments we’ll share below!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">manifold_muon_admm</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">4.0</span><span class="p">):</span>
    <span class="c1"># Ensure that W and G are both tall matrices
</span>    <span class="n">should_tranpose</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">should_tranpose</span><span class="p">:</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">T</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">G</span><span class="p">.</span><span class="n">T</span>
    <span class="c1"># Initialize the lagrangian, slack, and dual variable
</span>    <span class="n">Lambda</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">G</span> <span class="o">+</span> <span class="n">G</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">W</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">G</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">W</span> <span class="o">@</span> <span class="n">Lambda</span>
    <span class="n">Omega</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># Solve the dual problem with ADMM to find the update direction A
</span>    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="c1"># Update for Lambda (orthonormal least-squares solve)
</span>        <span class="n">P</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">mT</span> <span class="o">@</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Omega</span> <span class="o">+</span> <span class="n">X</span> <span class="o">-</span> <span class="n">G</span><span class="p">)</span>
        <span class="n">Lambda_upd</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="p">(</span><span class="n">P</span> <span class="o">+</span> <span class="n">P</span><span class="p">.</span><span class="n">mT</span><span class="p">)</span>
        <span class="c1"># Update for X (singular value thresholding)
</span>        <span class="n">B</span> <span class="o">=</span> <span class="n">G</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">W</span> <span class="o">@</span> <span class="n">Lambda_upd</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">Omega</span>
        <span class="n">eye</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">B</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">B</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">P_pos</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">eye</span> <span class="o">+</span> <span class="nf">msign</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">mT</span> <span class="o">@</span> <span class="n">B</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">eye</span><span class="p">))</span>
        <span class="n">X_upd</span> <span class="o">=</span> <span class="p">(</span><span class="n">B</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">rho</span> <span class="o">*</span> <span class="nf">msign</span><span class="p">(</span><span class="n">B</span><span class="p">))</span> <span class="o">@</span> <span class="n">P_pos</span>
        <span class="c1"># Update for Omega (dual ascent)
</span>        <span class="n">Omega_upd</span> <span class="o">=</span> <span class="n">Omega</span> <span class="o">+</span> <span class="n">rho</span> <span class="o">*</span> <span class="p">(</span><span class="n">X_upd</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">W</span> <span class="o">@</span> <span class="n">Lambda_upd</span> <span class="o">-</span> <span class="n">G</span><span class="p">)</span>
        <span class="n">Lambda</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Omega</span> <span class="o">=</span> <span class="n">Lambda_upd</span><span class="p">,</span> <span class="n">X_upd</span><span class="p">,</span> <span class="n">Omega_upd</span>
    <span class="c1"># Calculate A from final ADMM solution
</span>    <span class="c1"># (at convergence, G + 2 * W @ Lambda \approx X)
</span>    <span class="n">A</span> <span class="o">=</span> <span class="nf">msign</span><span class="p">(</span><span class="n">G</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">W</span> <span class="o">@</span> <span class="n">Lambda</span><span class="p">)</span>
    <span class="c1"># Descend on the primal problem
</span>    <span class="n">new_W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">A</span>
    <span class="c1"># Retract to the manifold
</span>    <span class="n">new_W</span> <span class="o">=</span> <span class="nf">msign</span><span class="p">(</span><span class="n">new_W</span><span class="p">)</span>
    <span class="c1"># Restore the shape of the solution and return
</span>    <span class="k">return</span> <span class="n">new_W</span><span class="p">.</span><span class="n">T</span> <span class="k">if</span> <span class="n">should_tranpose</span> <span class="k">else</span> <span class="n">new_W</span>
</code></pre></div></div>

<p>In this code, <code class="language-plaintext highlighter-rouge">msign</code> calls Jeremy’s implementation of the “Polar Express”
algorithm <a class="citation" href="#Amsel2025-qj">(Amsel et al., 2025)</a>, which is a fairly aggressive algorithm for
efficiently approximating the matrix sign function. It’s worth keeping this in
mind, as a source of numerical inexactness in our subsequent experiments: for
ADMM to be useful to us, it has to be able to cope well with these
approximations!
There are also some ‘reparameterizations’ of learning rates relative to the
notation we’ve used above, but the algorithm is equivalent.</p>

<h2 id="intuition-for-the-speedup-over-dual-ascent">Intuition for the Speedup over Dual Ascent</h2>

<p>As a final sanity check, we can use our derivations above to visualize a toy
case of the manifold Muon ADMM algorithm, and contrast it with the pathological
behavior of subgradient descent we visualized before.
If we pick $m = n = 1$, $W = 1$ and $G = 0$ and simplify with algebra, the
$\Lambda_k$ and $\Omega_k$ variables actually become redundant, and we end up
with the very simple equivalent ADMM update</p>

\[X_{k+1} = \sign(X_k) \max \set{\abs{X_k} - 1/\rho, 0}.\]

<p>It should be intuitively clear that this iteration converges stably to zero: we reduce
the magnitude of $X_k$ by $1/\rho$ until it drops below zero, then it stays there!
On an intuitive level, this is because the $\vX$ update in the ADMM algorithm
we derived for manifold Muon is <em>continuous</em>, whereas the subgradient descent update is
<em>discontinuous</em>.<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">8</a></sup></p>

<figure>
    <img src="/assets/blog/admm_descent.gif" alt="ADMM
    (simplified) on the absolute value function" />
    <figcaption>ADMM on the absolute value function with splitting (after algebraic
    simplifications), with $\rho = 16.0$.</figcaption>
</figure>

<h1 id="experimental-results">Experimental Results</h1>

<p>We base our experiments on the experimental setup in the <a href="https://github.com/thinking-machines-lab/manifolds/">Thinking Machines
Github repository</a> for the
blog post: a simple three-layer MLP is trained on CIFAR-10 for five epochs, and
per-epoch timing information is reported.</p>

<p>Here’s a summary of what we’ll look at below:</p>

<ol>
  <li>We perform a sweep over the ADMM penalty parameter $\rho$ and the number of
ADMM iterations for each manifold Muon solve. This gives us a sensible choice of
hyperparameters for ADMM for this setup, and lets us verify performance.</li>
  <li>Since the model being trained is so small-scale, we
also plot full dual ascent and ADMM inner loop loss curves for each iteration
of the outer loop and eyeball these to assess how fast each algorithm is
converging.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">9</a></sup> We believe this ranking will be more indicative of properties of
the algorithms that persist at scale (since raw test error may not be a suitable
proxy, based on what we’ll see below).</li>
  <li>Having convinced ourselves of sensible ADMM performance at different choices
of inner loop iterations, we look at timing!</li>
  <li>We finally look at some other relevant metrics for ADMM, such as feasibility
(how well are we satisfying the splitting constraint).</li>
</ol>

<p>The code for these experiments is available in <a href="https://github.com/sdbuch/thinky-manifolds/tree/admm">the <code class="language-plaintext highlighter-rouge">admm</code> branch of my fork
of Jeremy’s Github repository</a>.</p>

<h2 id="parameter-sweep">Parameter Sweep</h2>

<p>The main takeaway of this experiment is that the model training might be a bit too
small scale! In general, for this model, manifold Muon leads to a fairly similar
test error for all methods examined, including running zero inner loop
iterations. This suggests a need to examine behaviors further at larger scale.
Nevertheless, we sweep over values in $\set{0, 5, 10, 25, 50, 100}$ for the
number of inner loop iterations to run, and for ADMM, we sweep over $\rho \in
\set{2, 4, 8, 16}$. We also run dual ascent as a baseline, with the same
initialization as in Jeremy’s repo.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">10</a></sup></p>

<figure>
    <img src="/assets/blog/accuracy_comparison.png" alt="Plot
    of test accuracy vs. ADMM penalty parameter (or dual ascent) for different
    total inner loop step counts." />
    <figcaption> Plot
    of test accuracy vs. ADMM penalty parameter (or "DA" for dual ascent) for
    different total inner loop step counts. </figcaption>
</figure>

<p>From this plot, we see that test performance is fairly tightly clustered around
the same nominal value, suggesting that we should not read too much into
differences in absolute performance or even rankings for this setting.
Nonetheless, we can use these results to infer a good setting of hyperparameters
for this experiment, then dig further into convergence behavior for these
settings. We pick (steps, $\rho$) pairs of $(5, 4.0)$, $(10, 4.0)$, $(25, 8.0)$,
$(50, 16.0)$, and $(100, \text{DA})$ for taking a closer look. The following
table copies the results from the plot above for these values (and adds training
accuracy).</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Manifold Steps</th>
      <th style="text-align: center">ADMM $\rho$</th>
      <th style="text-align: center">Test Accuracy</th>
      <th style="text-align: center">Train Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">52.32%</td>
      <td style="text-align: center">65.22%</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">4.0</td>
      <td style="text-align: center">52.88%</td>
      <td style="text-align: center">66.06%</td>
    </tr>
    <tr>
      <td style="text-align: center">10</td>
      <td style="text-align: center">4.0</td>
      <td style="text-align: center">52.82%</td>
      <td style="text-align: center">65.92%</td>
    </tr>
    <tr>
      <td style="text-align: center">25</td>
      <td style="text-align: center">8.0</td>
      <td style="text-align: center">52.85%</td>
      <td style="text-align: center">65.85%</td>
    </tr>
    <tr>
      <td style="text-align: center">50</td>
      <td style="text-align: center">16.0</td>
      <td style="text-align: center">53.07%</td>
      <td style="text-align: center">65.71%</td>
    </tr>
    <tr>
      <td style="text-align: center">100</td>
      <td style="text-align: center">N/A</td>
      <td style="text-align: center">52.34%</td>
      <td style="text-align: center">65.85%</td>
    </tr>
  </tbody>
</table>

<p>For ADMM, note that we can achieve good performance with a small number
of inner loop iterations. Next, we’ll make sure we are actually converging
with ADMM at these iteration counts—which should convince us that the
iteration counts we later evaluate for timing are reasonable.</p>

<h2 id="inner-loop-convergence">Inner Loop Convergence</h2>

<p>We compare the inner loop losses for the best-performing hyperparameter settings
above. We also compare them to dual ascent.</p>

<p>We can choose to visualize the inner loop loss for one of three possible weight
matrices (first layer, second layer, third layer), for any one of the outer loop
iterations (we do a total of five epochs over the data).</p>

<figure>
    <img src="/assets/blog/fc1-e00-d00.png" alt="Dual ascent losses
    for layer 1 weight, epoch 0, step 0" />
    <img src="/assets/blog/fc2-e00-d00.png" alt="Dual ascent losses
    for layer 2 weight, epoch 0, step 0" />
    <img src="/assets/blog/fc3-e00-d00.png" alt="Dual ascent losses
    for layer 3 weight, epoch 0, step 0" />
    <figcaption>Dual ascent losses for each weight matrix in the network, for the
    nonzero-steps hyperparameter choices shown in the table above. Traces can be
    identified based on where they terminate.</figcaption>
</figure>

<p>Notice that both nonsmoothness and initial scale of the learning rate for dual ascent
being too large both seem to play a role in the slow convergence of dual ascent in the
plots above, which are evaluated for each weight matrix at the first epoch and first
iteration of optimization. Dual ascent needs all 100 iterations to reach a good loss
value, whereas the ADMM algorithms converge much more quickly. The ADMM algorithms only
require setting the penalty parameter, and are not very sensitive to it in this
experiment, as long as it is sufficiently large: we found smaller values of the penalty
parameter to lead to instabilities in inner loop loss convergence, although outer loop
performance was not significantly affected.<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup></p>

<p>For the ADMM iteration counts, ten iterations is aggressive, but reasonable: only for
<code class="language-plaintext highlighter-rouge">fc3</code> does it end up a bit far from convergence, and this seems to be due to the setting
of the penalty parameter we’ve chosen more than anything else.</p>

<p>These plots are more or less representative for all iterations of training. In later
iterations, the loss decrease is often less dramatic than at the first iteration, but
trends persist: dual ascent is slower, ten ADMM iterations is aggressive, but not
unreasonable (see below).</p>

<figure>
    <img src="/assets/blog/fc1-e01-d23.png" alt="Dual ascent losses
    for layer 1 weight, epoch 1, step 23" />
    <figcaption>Dual ascent losses for weights in the first layer of the network, for
    the nonzero-steps hyperparameter choices shown in the table above. Traces can be
    identified based on where they terminate.</figcaption>
</figure>

<h2 id="timing-and-other-diagnostics">Timing and Other Diagnostics</h2>

<p>We profile the code with the pre-implemented timing commands in the original repository.
It would be good to do this in a more rigorous setting, also after optimizing the code a
bit more (e.g., adding compilation to unroll and fuse the inner loop where possible).
But this test gives us a rough sense of how much we stand to improve.</p>

<p>Here are outputs for the dual ascent algorithm (100 iterations), run on a 1x H100 VM.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ubuntu@flaky-muskox-1:~/thinky-manifolds<span class="nv">$ </span>uv run src/main.py
Training with: manifold_muon
Epochs: 5 <span class="nt">---</span> LR: 0.1
Epoch 1, Loss: 1.6895257502186054, Time: 11.8445 seconds
Epoch 2, Loss: 1.4129536565469236, Time: 11.7680 seconds
Epoch 3, Loss: 1.2799630311070656, Time: 11.7151 seconds
Epoch 4, Loss: 1.183229492635143, Time: 11.6794 seconds
Epoch 5, Loss: 1.1131725724862547, Time: 12.1164 seconds
Accuracy of the network on the 10000 <span class="nb">test </span>images: 52.58 %
Accuracy of the network on the 50000 train images: 65.936 %
</code></pre></div></div>

<p>And here are outputs for the ADMM algorithm (10 iterations).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ubuntu@flaky-muskox-1:~/thinky-manifolds<span class="nv">$ </span>uv run src/main.py <span class="nt">--update</span> manifold_muon_admm
Training with: manifold_muon_admm
Epochs: 5 <span class="nt">---</span> LR: 0.1
Epoch 1, Loss: 1.6916365696459401, Time: 5.1556 seconds
Epoch 2, Loss: 1.4166164446850211, Time: 5.1545 seconds
Epoch 3, Loss: 1.2806039537702287, Time: 5.0617 seconds
Epoch 4, Loss: 1.1837433649569142, Time: 5.0579 seconds
Epoch 5, Loss: 1.1136368123852476, Time: 5.0620 seconds
Accuracy of the network on the 10000 <span class="nb">test </span>images: 52.62 %
Accuracy of the network on the 50000 train images: 66.086 %
</code></pre></div></div>

<p>We see a 2.3x speedup—not bad! We’ve cut the iteration count by a factor of 10, but
ADMM uses more FLOPs per iteration because of the extra variables from splitting. With
further optimizations (e.g., better fusing), it’s likely the ADMM runtime would better
reflect the total iteration count and improve further.</p>

<p>Another natural question is the extent to which we’re obtaining ADMM iterates
$(\vLambda_\star, \vX_\star)$ that satisfy the desired ADMM constraint $\vX = \vG +
\vW(\vLambda + \vLambda^\top)$. We show a plot below from epoch 0, iteration 0 for the
first layer (other layers/steps are similar, or better). It demonstrates that the
residual norm is always rather small, but still non-negligible before convergence. Ten
iterations of ADMM is not quite converged for this layer, but strikes a reasonable tradeoff between convergence and efficiency.</p>

<figure>
    <img src="/assets/blog/fc1-e00-d00-feas.png" alt="Feasibility
    (ell-2 norm) for layer 1, epoch 0, step 0" />
    <figcaption>Feasibility ($\ell^2$ norm) for the ADMM algorithm, for
    the nonzero-steps hyperparameter choices shown in the table above. Traces can be
    identified based on where they terminate.</figcaption>
</figure>

<h1 id="conclusions">Conclusions</h1>

<p>We’ve managed to speed up manifold Muon by a significant amount over the dual ascent
baseline by deriving and implementing ADMM for this problem! ADMM converges faster and
requires less hyperparameter tuning, allowing us to safely use a much smaller number of
inner loop iterations.</p>

<p>You can download the code and try it yourself <a href="https://github.com/sdbuch/thinky-manifolds">from my
fork</a>. I’m excited to run larger-scale
experiments with this algorithm to see how it works!</p>

<p>As a closing thought: here’s a quote from Jeremy’s original blog post:</p>

<blockquote>
  <p>Manifold Muon increased the wall clock time per step compared to AdamW, although this could be improved by running fewer steps of dual ascent or adding momentum to the algorithm and running dual ascent online. Depending on other systems bottlenecks, the overhead may not be an issue.</p>
</blockquote>

<p>I think the switch from dual ascent to ADMM dovetails with all of these considerations!
In particular, the results on this experiment (highly clustered performance) suggest
that incremental / online manifold Muon might be a natural choice for efficiency’s sake,
and our ADMM algorithm should work out-of-the-box in this regime.</p>

<h1 id="cite">Cite</h1>

<p>If you found this post or the code useful, please consider citing it:</p>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">buchanan2025mmuonadmm</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Buchanan, Sam}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Speeding Up Manifold Muon with {ADMM}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="m">2025</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{\url{https://sdbuchanan.com/blog/manifold-muon/}}</span>
<span class="p">}</span>
</code></pre></div></div>

<h1 id="postscript-other-quadratic-constraints">Postscript: Other Quadratic Constraints</h1>

<p>Another recent blog post by Ben
Keigwin, Dhruv Pai, and Nathan Chen at Tilde Research describes how the essential
structure of the manifold Muon algorithm is preserved under replacements of the
constraint $\vW^\top \vW = \vI$ by similar constraints (e.g., constraining only the
diagonal) <a class="citation" href="#keigwin2025gramspace">(Keigwin et al., 2025)</a>. As a corollary, the ADMM algorithm we derived
above can be applied to constraints from this family with little extra effort! We leave
this as an exercise to
the reader.</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to <a href="https://jeremybernste.in/">Jeremy Bernstein</a> for helpful discussions.
Thanks to the <a href="https://sites.research.google/trc/about/">TRC program</a>,
<a href="https://hyperbolic.ai">Hyperbolic</a>, and <a href="https://mithril.ai">Mithril</a>
for compute.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:2" role="doc-endnote">

      <p>Here and below, we write $\ip{\vA}{\vB} = \tr(\vA^\top \vB) = \sum_{i, j} A_{ij}
B_{ij}$ to denote the Frobenius inner product on matrices, which treats them as if
they were vectors,
and $\norm{\spcdot}$ to denote the operator norm (or spectral norm, or Schatten
$\infty$-norm), which is the largest singular value of its (matrix) argument. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">

      <p>As written, the matrix sign function is only defined for inputs that have no zero
singular values. It turns out that as long as $\vA$ satisfies the tangent constraint
$ \vA^\top \vW + \vW^\top \vA = \Zero$, the update $\vW -\eta \vA$ <em>never</em> has zero
singular values! This can be derived straightforwardly via contradiction, and
provides a strong motivation for the need to <em>exactly</em> solve the manifold Muon
subproblem. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">

      <p>Here and below, we write $\norm{\spcdot}_*$ to denote the nuclear norm (or Schatten
1-norm), which is the sum of the singular values of its (matrix) argument. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">

      <p>Less naive algorithms for nonsmooth optimization can ameliorate this pathological
behavior, sometimes to a significant extent. See, for example, work of Damek Davis
and collaborators <a class="citation" href="#Davis2024-ef">(Davis et al., 2024)</a>. These algorithms definitely merit
a closer look in the context of manifold Muon! But we will stick our neck out and
suggest that it might be hard to beat ADMM’s solution quality vs. iteration count
tradeoff, based on the experiments to come—although we would be happy to be proven
wrong. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>We’ll see this is indeed the case empirically in our experiments below! <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">

      <p>This is a relatively straightforward argument, but at the same time, I haven’t
seen it anywhere in the compressed sensing literature previously (and this
literature applied singular value thresholding for <em>almost everything</em> one could).
If you know of a reference that has applied this previously, please let me know! <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">

      <p>After writing down these updates all together, it becomes clear that
$\vLambda_{k+1}$ is always a symmetric matrix. We use this invariant to
simplify some steps in the algorithm here and below! <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>More precisely, it’s straightforward to prove (using the SVD) that singular value
thresholding is a $1$-Lipschitz continuous function of the input matrix with respect
to the operator norm. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">

      <p>It may be obvious, but it’s worth repeating: the Manifold Muon runs an
iterative optimization solver after <em>every forward/backward pass</em> of the
model being trained, for every weight matrix in the model.
This leads to some interesting debugging issues, such
as small differences in the final solution obtained by different manifold
Muon solvers at iteration zero leading to significant differences in the
behavior of subsequent gradients/inner loop solves (even though everything
is convex!). These differences seem to mostly be restricted to inner loop
loss, as final val error does not seem to very much, but it will be
interesting to study this behavior at larger scales. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">

      <p>For the sake of comparison, we also use this initialization for our
$\vLambda$ in our ADMM algorithm. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>This is likely because of the fact that we
compute <code class="language-plaintext highlighter-rouge">msign</code> with an approximate algorithm, leading to unstable behavior when we
aggressively threshold singular values in the $\vX$ update of ADMM. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  
    <h2>References</h2>
    
      <ol class="bibliography"><li><div class="bibcompact" id="Amsel2025-qj">
  <span class="author">Amsel, N., Persson, D., Musco, C., & Gower, R.</span>
  <span class="year">(2025).</span>
  <span class="title"><em>The Polar Express: Optimal matrix sign methods and their
                   application to the Muon algorithm</em>.</span>
  <span class="publication">
    
      
        <em><a href="http://arxiv.org/abs/2505.16932">arXiv [cs.LG]</a></em>.
      
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="bernstein2025manifolds">
  <span class="author">Bernstein, J.</span>
  <span class="year">(2025).</span>
  <span class="title"><em>Modular Manifolds</em>.</span>
  <span class="publication">
    
      Retrieved from <a href="https://thinkingmachines.ai/blog/modular-manifolds/">https://thinkingmachines.ai/blog/modular-manifolds/</a>
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="keigwin2025gramspace">
  <span class="author">Keigwin, B., Pai, D., & Chen, N.</span>
  <span class="year">(2025).</span>
  <span class="title"><em>Gram-Space Manifold Muon</em>.</span>
  <span class="publication">
    
      Retrieved from <a href="https://www.tilderesearch.com/vignettes/gram-space">https://www.tilderesearch.com/vignettes/gram-space</a>
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="Davis2024-ef">
  <span class="author">Davis, D., Drusvyatskiy, D., & Charisopoulos, V.</span>
  <span class="year">(2024).</span>
  <span class="title"><em>Stochastic algorithms with geometric step decay converge linearly
               on sharp functions</em>.</span>
  <span class="publication">
    
      
        <em><a href="http://dx.doi.org/10.1007/s10107-023-02003-w">Mathematical programming</a></em>, <em>207</em>(1-2), 145–190.
      
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="book-wright-ma">
  <span class="author">Wright, J. & Ma, Y.</span>
  <span class="year">(2022).</span>
  <span class="title"><em>High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications</em>.</span>
  <span class="publication">
    
      <em>Cambridge University Press</em>, 2022.
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="Boyd2011-yb">
  <span class="author">Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J.</span>
  <span class="year">(2011).</span>
  <span class="title"><em>Distributed Optimization and Statistical Learning via the
               Alternating Direction Method of Multipliers</em>.</span>
  <span class="publication">
    
      
        <em><a href="http://dx.doi.org/10.1561/2200000016">Foundations and Trends® in Machine Learning</a></em>, <em>3</em>(1), 1–122.
      
    
  </span>
</div>
</li></ol>
    
  

  

</article>


      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      


<div id="icon-container">
  <div>
    <a href=mailto:sam@ttic.edu style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" />
    </a>
  </div>
  <div>
    <a href=https://scholar.google.com/citations?user=5WT38A0AAAAJ style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" />
    </a>
  </div>

  <div>
    <a href=https://twitter.com/_sdbuchanan
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" />
    </a>
  </div>

  <div>
    <a href=https://www.linkedin.com/in/sam-buchanan-4507a6b3
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/linkedin.svg" alt="LinkedIn icon" />
    </a>
  </div>

  <div>
    <a href=https://github.com/sdbuch
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" />
    </a>
  </div>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Littlefoot for footnotes and citations -->
<script src="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.css">

<script>
document.addEventListener('DOMContentLoaded', function() {
  // Initialize Littlefoot for standard footnotes
  littlefoot.littlefoot({
    buttonTemplate: '<button aria-expanded="false" class="littlefoot__button" id="<% id %>" title="See Footnote"><% number %></button>',
    activateDelay: 100,
    activateCallback: function(popover, button) {
      // Render MathJax in the footnote popup
      if (window.MathJax && window.MathJax.typesetPromise) {
        window.MathJax.typesetPromise([popover]).catch(function (err) {
          console.log('MathJax typeset failed: ' + err.message);
        });
      }
    },
    allowMultiple: true,
    dismissDelay: 500,
    hoverDelay: 250,
    numberResetSelector: 'article',
    scope: 'body'
  });

  // Custom handler for jekyll-scholar citations
  const citeLinks = document.querySelectorAll('a[href*="#"]');
  citeLinks.forEach(function(link) {
    // Check if this is a citation link (points to bibliography)
    const href = link.getAttribute('href');
    if (href && href.startsWith('#') && href.match(/^#[A-Za-z]/)) {
      const targetId = href.substring(1);
      const targetElement = document.getElementById(targetId);
      
      // If target exists and looks like a bibliography entry
      if (targetElement && (targetElement.closest('.bibliography') || targetElement.tagName === 'LI')) {
        link.addEventListener('mouseenter', function(e) {
          showCitationPopup(e, targetElement);
        });
        
        link.addEventListener('mouseleave', function() {
          hideCitationPopup();
        });
        
        // Prevent default click behavior for hover-only interaction
        link.addEventListener('click', function(e) {
          e.preventDefault();
        });
      }
    }
  });
});

let citationPopup = null;
let hideTimeout = null;
let showTimeout = null;

function showCitationPopup(event, targetElement) {
  // Clear any pending show/hide operations
  if (hideTimeout) {
    clearTimeout(hideTimeout);
    hideTimeout = null;
  }
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Immediately hide any existing popup
  if (citationPopup && citationPopup.parentNode) {
    citationPopup.parentNode.removeChild(citationPopup);
    citationPopup = null;
  }
  
  // Show new popup after a short delay to prevent flicker
  showTimeout = setTimeout(function() {
    // Create popup
    citationPopup = document.createElement('div');
    citationPopup.className = 'littlefoot__popover citation-popup';
    citationPopup.innerHTML = '<div class="littlefoot__content citation-content">' + targetElement.innerHTML + '</div>';
    
    // Position popup
    const rect = event.target.getBoundingClientRect();
    citationPopup.style.position = 'absolute';
    citationPopup.style.top = (rect.bottom + window.scrollY + 5) + 'px';
    citationPopup.style.left = rect.left + 'px';
    citationPopup.style.zIndex = '1000';
    citationPopup.style.maxWidth = '400px';
    
    document.body.appendChild(citationPopup);
    
    // Render MathJax in the popup
    if (window.MathJax && window.MathJax.typesetPromise) {
      window.MathJax.typesetPromise([citationPopup]).catch(function (err) {
        console.log('MathJax typeset failed: ' + err.message);
      });
    }
    
    // Add hover handlers to keep popup open
    citationPopup.addEventListener('mouseenter', function() {
      if (hideTimeout) {
        clearTimeout(hideTimeout);
        hideTimeout = null;
      }
    });
    
    citationPopup.addEventListener('mouseleave', function() {
      hideCitationPopup();
    });
    
    showTimeout = null;
  }, 50);
}

function hideCitationPopup() {
  // Clear any pending show operation
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Clear any existing hide timeout
  if (hideTimeout) {
    clearTimeout(hideTimeout);
  }
  
  hideTimeout = setTimeout(function() {
    if (citationPopup && citationPopup.parentNode) {
      citationPopup.parentNode.removeChild(citationPopup);
      citationPopup = null;
    }
    hideTimeout = null;
  }, 100);
}
</script>


</html>
