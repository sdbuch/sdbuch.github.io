<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>SPMD in JAX #1: Sharding</title>
  <meta name="description" content="This post will be the first in a series on programming with JAX, for training models like transformers. I’m experimenting with these as an alternative to my usual scratch paper or LaTeX notes while learning, in the hope that it will help me with recall and perhaps be useful to others learning this material.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://sdbuchanan.com/blog/jax-1/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Sam D. Buchanan" href="http://sdbuchanan.com/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js"></script>

<!-- MathJax -->
<script type="text/javascript">
  const macros = {};

  // Generate BB letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`bb${letter}`] = `{\\mathbb{${letter}}}`;
  }

  // Generate script letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`c${letter}`] = `{\\mathscr{${letter}}}`;
  }

  // Generate calligraphic letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`s${letter}`] = `{\\mathcal{${letter}}}`;
  }

  // Generate vector uppercase letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Generate vector lowercase letters
  for (let i = 97; i <= 122; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Add Greek letters
  const greekLetters = [
    'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'varepsilon', 'zeta', 'eta',
    'theta', 'vartheta', 'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi',
    'pi', 'varpi', 'rho', 'varrho', 'sigma', 'varsigma', 'tau', 'upsilon',
    'phi', 'varphi', 'chi', 'psi', 'omega', 'Gamma', 'Delta', 'Theta',
    'Lambda', 'Xi', 'Pi', 'Sigma', 'varSigma', 'Upsilon', 'Phi', 'Psi',
    'Omega', 'ell'
  ];
  greekLetters.forEach(letter => {
    macros[`v${letter}`] = `{\\boldsymbol{\\${letter}}}`;
  });

  // Add these to your macros object
  const additionalMacros = {
    // Basic macros
    "Beta": "{\\mathrm{B}}",
    "eps": "{\\epsilon}",
    "Diff": "{\\mathop{}\\!\\mathrm{D}}",
    "diff": "{\\mathop{}\\!\\mathrm{d}}",
    "Partial": ["{\\frac{\\partial #1}{\\partial #2}}", 2],
    "PartialN": ["{\\frac{\\partial^{#3} #1}{\\partial {#2}^{#3}}}", 3],
    "dPartial": ["{\\dfrac{\\partial #1}{\\partial #2}}", 2],
    "PPartial": ["{\\tfrac{\\partial}{\\partial #1}}", 1],
    "dac": "{\\left.\\frac{\\partial}{\\partial t}\\right|_{t=0}}",
    "half": "{\\tfrac{1}{2}}",
    "third": "{\\tfrac{1}{3}}",
    "fourth": "{\\tfrac{1}{4}}",
    "sixth": "{\\tfrac{1}{6}}", // Fixed typo (was 1/4)
    "eighth": "{\\tfrac{1}{8}}",

    // Accents and decorations
    "overbar": ["{\\mkern 1.5mu\\overline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "underbar": ["{\\mkern 1.5mu\\underline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "conj": ["{\\overbar{#1}}", 1],
    "wh": "{\\widehat}",
    "wt": "{\\widetilde}",
    "ol": ["{\\overbar{#1}}", 1],
    "kron": "{\\otimes}",
    "elwise": "{\\odot}", // Using \odot as circleddot is not standard
    "dsum": "{\\oplus}",
    "spcdot": "{\\,\\cdot\\,}",

    // Special symbols
    "iu": "{\\mathfrak{i}}",
    "given": [' \\, \\vert \\, ', 0],
    "llangle": ['\\langle\\!\\langle', 0],
    "rrangle": ['\\rangle\\!\\rangle', 0],

      // Analysis operators
    "Lip": "{\\mathrm{Lip}}",
    "mem": "{\\mathrm{mem}}",
    "softmax": "{\\operatorname{\\mathrm{softmax}}}",
    "equid": "{\\overset{d}{=}}",
    "xor": "{\\oplus}",
    "bigxor": "{\\bigoplus}",
    "minimize": ["{\\underset{#1}{\\operatorname{minimize}}}", 1],
    "maximize": ["{\\underset{#1}{\\operatorname{maximize}}}", 1],
    "argmin": "{\\mathop{\\mathrm{arg\\,min}}}",
    "argmax": "{\\mathop{\\mathrm{arg\\,max}}}",
    "orth": "{\\operatorname{orth}}",
    "ow": "{\\mathrm{otherwise}}",
    "iid": "{\\mathrm{i.i.d.}}",
    "wass": "{\\mathrm{W}}",
    "TV": "{\\mathrm{TV}}",
    "as": "{\\mathrm{a.s.}}",
    "whp": "{\\mathrm{w.h.p.}}",
    "simiid": "{\\sim_{\\iid}}",
    "ltsim": "{\\lesssim}",
    "gtsim": "{\\gtrsim}",
    "ltsimwhp": "{\\underset{\\whp}{\\lesssim}}",
    "gtsimwhp": "{\\underset{\\whp}{\\gtrsim}}",
    "psdgeq": "{\\succcurlyeq}",
    "psdleq": "{\\preccurlyeq}",
    "defn": "{\\overset{\\text{def}}{=}}",
    "normsubdiff": ["{\\partial\\norm{}_{#1}}", 1],
    "normalize": ["{\\frac{#1}{\\norm*{#1}_2}}", 1],
    "normalizeabs": ["{\\frac{#1}{\\abs*{#1}}}", 1],
    "iter": ["{#1^{(#2)}}", 2],
    "prox": ["{\\operatorname{prox}}_{#1}", 1],

    // Vector operators
    "tr": "{\\operatorname{tr}}",
    "diag": "{\\operatorname{diag}}",
    "Diag": "{\\operatorname{Diag}}",
    "vect": "{\\operatorname{vec}}",
    "vec": "{\\operatorname{vec}}",
    "Sym": "{\\operatorname{sym}}",
    "Symm": "{\\operatorname{sym}}",
    "Skew": "{\\operatorname{skew}}",
    "rank": "{\\operatorname{rank}}",
    "krank": "{\\operatorname{krank}}",
    "sign": "{\\operatorname{sign}}",
    "sgn": "{\\operatorname{sgn}}",
    "supp": "{\\operatorname{supp}}",
    "esssup": "{\\mathop{\\operatorname{ess\\,sup}}}",
    "vol": "{\\operatorname{vol}}",
    "Vol": "{\\operatorname{Vol}}",
    "tp": "{^{\\mathrm{T}}}",
    "adj": "{^{\\ast}}",
    "inv": "{^{-1}}",
    "One": "{\\mathbf{1}}",
    "Zero": "{\\mathbf{0}}",
    "Id": "{\\operatorname{\\mathrm{Id}}}",
    "conv": "{\\mathbin{\\ast}}",
    "iconv": "{\\mathbin{\\square}}",
    "xcorr": "{\\mathbin{\\star}}",
    "cconv": "{\\mathbin{\\circledast}}",
    "frob": "{\\mathrm{F}}",
    "HS": "{\\mathrm{HS}}",

    // Trig stuff
    "acos": "{\\operatorname{\\cos\\inv}}",
    "asin": "{\\operatorname{\\sin\\inv}}",
    "atan": "{\\operatorname{\\tan\\inv}}",
    "sech": "{\\operatorname{sech}}",
    "csch": "{\\operatorname{csch}}",

    // Calculus/geometry operators
    "Hess": "{\\operatorname{Hess}}",
    "grad": "{\\operatorname{grad}}",
    "Div": "{\\operatorname{div}}",
    "curl": "{\\operatorname{curl}}",
    "downto": "{\\searrow}",
    "upto": "{\\nearrow}",

    // CS operators
    "polylog": "{\\operatorname{polylog}}",
    "poly": "{\\operatorname{poly}}",

    // Stats operators
    "Ind": ["{\\mathds{1}}_{#1}", 1],
    "stddev": "{\\operatorname{stddev}}",
    "Unif": ["{\\operatorname{Unif}(#1)}", 1],
    "Bern": ["{\\operatorname{Bern}(#1)}", 1],
    "Pois": ["{\\operatorname{Pois}(#1)}", 1],
    "Binom": ["{\\operatorname{Binom}(#1, #2)}", 2],
    "Exp": "{\\operatorname{Exp}}",
    "BG": "{\\operatorname{BG}}",
    "Law": "{\\mathrm{Law}}",
    "indep": "{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}",
    "independenT": ["{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}", 2],

    // Topology / set operators
    "compl": "{\\mathsf{c}}",
    "bd": "{\\operatorname{bd}}",
    "relbd": "{\\operatorname{relbd}}",
    "cl": "{\\operatorname{cl}}",
    "Conv": "{\\operatorname{conv}}",
    "dom": "{\\operatorname{dom}}",
    "epi": "{\\operatorname{epi}}",
    "aff": "{\\operatorname{aff}}",
    "cone": "{\\operatorname{cone}}",
    "ri": "{\\operatorname{ri}}",
    "im": "{\\operatorname{im}}",
    "Hom": "{\\operatorname{Hom}}",
    "End": "{\\operatorname{End}}",
    "Aut": "{\\operatorname{Aut}}",
    "Null": "{\\operatorname{null}}",
    "Span": "{\\operatorname{span}}",
    "row": "{\\operatorname{row}}",
    "col": "{\\operatorname{col}}",
    "range": "{\\operatorname{range}}",
    "Ran": "{\\operatorname{ran}}",
    "diam": "{\\operatorname{diam}}",
    "len": "{\\operatorname{len}}",
    "dist": "{\\operatorname{dist}}",
    "nnz": "{\\operatorname{nnz}}",
    "RE": "{\\operatorname{RE}}",
    "err": "{\\operatorname{err}}",
    "circulant": "{\\operatorname{circ}}",
    "tre": "{\\operatorname{tre}}",
    "etr": "{\\operatorname{etr}}",
    "proj": ["{\\operatorname{proj}_{#1}}", 1]
  };

  const delimitersToDefine = [
    // Simple paired delimiters
    "\\DeclarePairedDelimitersX{\\abs[1]}{\\lvert}{\\rvert}{ \ifblank{#1}{\:\cdot\:}{#1} }",
  ];

  const all_macros = { ...macros, ...additionalMacros };

  window.MathJax = {
    loader: {
      load: ['[tex]/boldsymbol', '[tex]/mathtools', '[tex]/ams', '[tex]/color']
    },
    tex: {
      packages: {'[+]': ['boldsymbol', 'mathtools', 'ams', 'color']},
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      tags: 'ams',
      macros: all_macros,
      mathtools: {
        pairedDelimiters: {
          // General pairing commands
          abs: ['\\lvert', '\\rvert', '{#1}', 1, '', ''],
          norm: ['\\lVert', '\\rVert', '{#1}', 1, '', ''],
          nnorm: ['\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '{#1}', 1, '', ''],
          ip: ['\\langle', '\\rangle', '{#1}, {#2}', 2, '', ''],
          iip: ['\\llangle', '\\rrangle', '{#1}, {#2}', 2, '', ''],
          ceil: ['\\lceil', '\\rceil', '{#1}', 1, '', ''],
          floor: ['\\lfloor', '\\rfloor', '{#1}', 1, '', ''],
          KL: ['(', ')', '{#1} \\:\\|\\: {#2}', 2, '\\mathop{\\mathrm{KL}}', ''],

          // Set type commands
          set: ['\\{', '\\}', '{#1}', 1, '', ''],
          Pr: ['[', ']', '{#1}', 1, '\\mathbb{P}', ''],
          Prsub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{P}}_{#1}', ''],
          E: ['[', ']', '{#1}', 1, '\\mathbb{E}', ''],
          Esub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{E}}_{#1}', ''],
          Var: ['[', ']', '{#1}', 1, '\\mathrm{Var}', ''],
          cov: ['[', ']', '{#1}', 1, '\\mathrm{cov}', ''],
          Ent: ['[', ']', '{#2}', 2, '\\mathop{\\mathrm{Ent}}_{#1}', '']
        }
      }
    }
  };
</script>
<!-- <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script> -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>


  
  <meta property="og:title" content="SPMD in JAX #1: Sharding">
  <meta property="og:site_name" content="Sam D. Buchanan">
  <meta property="og:url" content="http://sdbuchanan.com/blog/jax-1/">
  <meta property="og:description" content="This post will be the first in a series on programming with JAX, for training models like transformers. I’m experimenting with these as an alternative to my usual scratch paper or LaTeX notes while learning, in the hope that it will help me with recall and perhaps be useful to others learning this material.">
  
  
    <meta property="og:image" content="/assets/sam-2023.jpeg">
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="SPMD in JAX #1: Sharding">
  <meta name="twitter:description" content="This post will be the first in a series on programming with JAX, for training models like transformers. I’m experimenting with these as an alternative to my usual scratch paper or LaTeX notes while...">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Sam D. Buchanan</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Home</a>
      
        
        <a class="page-link" href="/publications/">Publications</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="/misc/">Misc.</a>
      
        
        <a class="page-link" href="/assets/cv.pdf">CV</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">SPMD in JAX #1: Sharding</h1>
    
    <p class="post-meta"><time datetime="2025-08-20T00:00:00-07:00" itemprop="datePublished">Aug 20, 2025</time> •
  
    
    
      
        <a href="/categories/jax/">jax</a>
      
    
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>This post will be the first in a series on programming with JAX, for training
models like transformers. I’m experimenting with these as an alternative to my
usual scratch paper or LaTeX notes while learning, in the hope that it will help
me with recall and perhaps be useful to others learning this material.</p>

<p>The focus in this post is on building a mental model (mathematical) for
sharding and communication based on linear algebra, in particular block
matrices, and on studying some low-level code for different communication
primitives.
The notes are based on two tutorials on sharding: <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>
and <a class="citation" href="#jax_sharded_computation">(JAX Team, 2025)</a>, as well as some ‘original research’.</p>

<ul id="markdown-toc">
  <li><a href="#setup" id="markdown-toc-setup">Setup</a></li>
  <li><a href="#sharding" id="markdown-toc-sharding">Sharding</a>    <ul>
      <li><a href="#valid-shardings" id="markdown-toc-valid-shardings">Valid Shardings</a></li>
    </ul>
  </li>
  <li><a href="#example-matrix-multiplication" id="markdown-toc-example-matrix-multiplication">Example: Matrix Multiplication</a></li>
  <li><a href="#types-of-inter-device-communication-with-sharded-computation" id="markdown-toc-types-of-inter-device-communication-with-sharded-computation">Types of Inter-Device Communication with Sharded Computation</a>    <ul>
      <li><a href="#communication-primitives-through-matrix-multiplication" id="markdown-toc-communication-primitives-through-matrix-multiplication">Communication Primitives through Matrix Multiplication</a>        <ul>
          <li><a href="#case-1-unsharded-contracting-dimension-valid-output" id="markdown-toc-case-1-unsharded-contracting-dimension-valid-output">Case 1: Unsharded Contracting Dimension, Valid Output</a></li>
          <li><a href="#case-2-one-multiplicand-sharded-along-contracting-dimension-allgather" id="markdown-toc-case-2-one-multiplicand-sharded-along-contracting-dimension-allgather">Case 2: One Multiplicand Sharded Along Contracting Dimension (AllGather)</a></li>
          <li><a href="#case-3-both-multiplicands-sharded-along-contracting-dimension-allreduce" id="markdown-toc-case-3-both-multiplicands-sharded-along-contracting-dimension-allreduce">Case 3: Both Multiplicands Sharded Along Contracting Dimension (AllReduce)</a></li>
          <li><a href="#case-4-sharded-non-contracting-dimensions-same-axis" id="markdown-toc-case-4-sharded-non-contracting-dimensions-same-axis">Case 4: Sharded Non-Contracting Dimensions, Same Axis</a></li>
        </ul>
      </li>
      <li><a href="#alltoall-communication" id="markdown-toc-alltoall-communication">AllToAll Communication</a></li>
    </ul>
  </li>
  <li><a href="#example-communication-primitives-in-jax" id="markdown-toc-example-communication-primitives-in-jax">Example: Communication Primitives in JAX</a>    <ul>
      <li><a href="#allgather" id="markdown-toc-allgather">AllGather</a></li>
      <li><a href="#alltoall" id="markdown-toc-alltoall">AllToAll</a></li>
      <li><a href="#allreduce-and-reducescatter" id="markdown-toc-allreduce-and-reducescatter">AllReduce and ReduceScatter</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#acknowledgments" id="markdown-toc-acknowledgments">Acknowledgments</a></li>
</ul>

<h3 id="setup">Setup</h3>

<p>Here’s the environment we will be working in—using Python 3.13 and JAX 0.7.1,
on a single TPU v4 host.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
</code></pre></div></div>

<h1 id="sharding">Sharding</h1>

<p>When we want to perform a high-throughput computation involving some data and
some mathematical operations (e.g., passing data through layers of a neural
network) using multiple hardware accelerators (e.g., TPUs), we need to, at
a minimum:</p>
<ol>
  <li>Have a scheme for indexing hardware accelerators relative to their physical
(spatial) layout.</li>
  <li>Specify what data and what parameters (inputs) go on which accelerators.</li>
  <li>Specify where the outputs of the computation should go.</li>
</ol>

<p>In JAX, these three steps are abstracted into creation of a <code class="language-plaintext highlighter-rouge">Mesh</code> specifying
device layout and a <code class="language-plaintext highlighter-rouge">PartitionSpec</code> specifying how data is split across devices
relative to this mesh. Such a splitting is called a sharding.</p>

<p>Here’s how the first two steps look in code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">))</span>  <span class="c1"># 1x v4 tpu host
</span><span class="n">sharding</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">))</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">sharding</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
┌──────────┬──────────┐
│          │          │
│  TPU 0   │  TPU 1   │
│          │          │
│          │          │
├──────────┼──────────┤
│          │          │
│  TPU 2   │  TPU 3   │
│          │          │
│          │          │
└──────────┴──────────┘
</code></pre></div></div>

<p>Above,</p>
<ul>
  <li>The <code class="language-plaintext highlighter-rouge">Mesh</code> indexes devices based on spatial axes. JAX will optimize this
indexing based on the actual device topology.</li>
  <li>The <code class="language-plaintext highlighter-rouge">NamedSharding</code> creates a sharding specification relative to the mesh,
which is defined by the <code class="language-plaintext highlighter-rouge">PartitionSpec</code> we provide. Here, we specify that a 2D
array has its first dimension sharded along the <code class="language-plaintext highlighter-rouge">x</code> dimension of the mesh and
its second dimension along <code class="language-plaintext highlighter-rouge">y</code>.</li>
  <li>For the <code class="language-plaintext highlighter-rouge">PartitionSpec</code>, we give <code class="language-plaintext highlighter-rouge">None</code> for dimensions that should not be
sharded. That means this dimension will be present on all devices.</li>
  <li>If a mesh axis does not appear in the sharding, then the sharded array is
fully replicated over that axis. In the extreme case, a <code class="language-plaintext highlighter-rouge">NamedSharding</code> of all
<code class="language-plaintext highlighter-rouge">None</code>s puts the full array on every device.</li>
  <li>The compiler will complain about invalid shardings.</li>
</ul>

<p>We can also move data around after creation on, say, one host.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">))</span>
<span class="n">B_sharded</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">sharding</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">B_sharded</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
┌───────────────────────┐
│                       │
│                       │
│                       │
│                       │
│         TPU 0         │
│                       │
│                       │
│                       │
│                       │
└───────────────────────┘
┌──────────┬──────────┐
│          │          │
│  TPU 0   │  TPU 1   │
│          │          │
│          │          │
├──────────┼──────────┤
│          │          │
│  TPU 2   │  TPU 3   │
│          │          │
│          │          │
└──────────┴──────────┘
</code></pre></div></div>

<p>In addition, we can ‘combine’ physical device axes into one or more aggregated
axes. This is a common operation, for example in parallelizing a batch axis
across all devices. Since the <code class="language-plaintext highlighter-rouge">Mesh</code> is our link between partition indices and
physical devices, we do this through the mesh:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh_flat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">((</span><span class="mi">4</span><span class="p">,),</span> <span class="p">(</span><span class="sh">"</span><span class="s">xy</span><span class="sh">"</span><span class="p">,))</span>
<span class="n">sharding_flat</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh_flat</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">xy</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">sharding_flat</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
┌───────────────────────┐
│         TPU 0         │
├───────────────────────┤
│         TPU 2         │
├───────────────────────┤
│         TPU 1         │
├───────────────────────┤
│         TPU 3         │
└───────────────────────┘
</code></pre></div></div>

<p>When we perform computations on data, we can either let the JAX compiler infer
the output sharding, or we can specify it ourselves.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">f_contract</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="nd">@partial</span><span class="p">(</span>
    <span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">,</span>
    <span class="n">out_shardings</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span>
        <span class="n">mesh</span><span class="p">,</span>
        <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span>
            <span class="bp">None</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">f_contract_replicated</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="n">result</span> <span class="o">=</span> <span class="nf">f_contract</span><span class="p">(</span><span class="n">B_sharded</span><span class="p">)</span>
<span class="n">result_replicated</span> <span class="o">=</span> <span class="nf">f_contract_replicated</span><span class="p">(</span><span class="n">B_sharded</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Inferred sharding:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Manual sharding (replicated):</span><span class="sh">"</span><span class="p">)</span>
<span class="n">jax</span><span class="p">.</span><span class="n">debug</span><span class="p">.</span><span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">result_replicated</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Inferred sharding:
┌───────┬───────┐
│TPU 0,2│TPU 1,3│
└───────┴───────┘
Manual sharding (replicated):
┌───────────┐
│TPU 0,1,2,3│
└───────────┘
</code></pre></div></div>

<p>Passing a <code class="language-plaintext highlighter-rouge">NamedSharding</code> to <code class="language-plaintext highlighter-rouge">jax.jit</code>’s <code class="language-plaintext highlighter-rouge">out_shardings</code> keyword argument lets
us specify here that the result of the computation should be propagated to all
devices. This entails some communication!</p>

<p>Given a valid input-output sharding for a more complex computation, the JAX
compiler can heuristically optimize the inner bits of the computation
(intermediate computations’ shardings, and necessary communication) to try to
make it run as efficiently as possible. In cases where this doesn’t work, JAX
has ways to hint at the compiler, as well as more advanced programming
paradigms, to explicitly specify intermediate sharding and communications.</p>

<h2 id="valid-shardings">Valid Shardings</h2>

<p>Given a mesh with $M$ axes, and an array with $N$ axes, we can enumerate all
valid shardings as follows. First note:</p>
<ul>
  <li>A sharding corresponds to, for each of the $N$ array axes, the selection of
one of the $M$ mesh axes, or <code class="language-plaintext highlighter-rouge">None</code>.</li>
  <li>Each mesh axis can appear at most once in a valid sharding.</li>
</ul>

<p>Then for each $k = 0, \dots, \min \set{M, N}$, pick $\binom{M}{k}$ of the $M$
mesh axes and $\binom{N}{k}$ of the $N$ array axes, then consider all possible
permutations of those $k$ selected axes. This is a total of</p>

\[\sum_{k=0}^{\min \set{M, N}} \binom{M}{k} \binom{N}{k} k!\]

<p>possible shardings. This is actually not too large—roughly on the order of
$N^M$ when $N$ is large.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h1 id="example-matrix-multiplication">Example: Matrix Multiplication</h1>

<p>The most common operations in neural networks are related to matrix
multiplications, and so thinking in terms of sharding data with two axes goes
a long way. Say we have matrices $\vA \in \bbR^{m \times n}$ and $\vB \in
\bbR^{n \times p}$. A sharding of a 2D array (matrix) corresponds to two things:
a partitioning of the matrix into sub-matrices, and a splitting of those
sub-matrices across devices. In JAX, these two operations are coupled through
the <code class="language-plaintext highlighter-rouge">Mesh</code>/<code class="language-plaintext highlighter-rouge">NamedSharding</code> interface, as we described above:</p>
<ul>
  <li>If an array dimension is sharded across a mesh axis, that dimension is
partitioned, and the partitioned components will be split across that
mesh axis.</li>
  <li>If an array dimension is not sharded, that dimension is not partitioned, and
it appears across all devices.</li>
  <li>If a mesh axis does not appear in a sharding specification, the sharded data
is copied across that mesh axis.</li>
</ul>

<p>Let’s consider the special case of a 2D mesh, in particular $2 \times 2$. For
simplicity, let’s place the first mesh axis in correspondence with the first
array dimension, and the second mesh axis in correspondence with the second
array dimension.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> Then every partitioning of $\vA$ or $\vB$, for example</p>

\[\begin{equation}\label{eq:partitioning-0}
\begin{bmatrix}
  \vA_{00} &amp; \vA_{01} \\
  \vA_{10} &amp; \vA_{11}
\end{bmatrix},
\end{equation}\]

<p>corresponds to an arrangement of the matrix across devices. In the above example,
we have $\vA_{ij} \in \bbR^{(m/2) \times (n/2)}$, and the coordinates $(i, j)$
of the submatrix correspond to the coordinates of the device it is stored on.
For a different partitioning, for example</p>

\[\begin{bmatrix}
  \vA_{0} \\
  \vA_{1}
\end{bmatrix},\]

<p>we need to take copying into account: here we have $\vA_{i} \in \bbR^{(m/2)
\times n}$, and the physical device layout is</p>

\[\begin{equation}\label{eq:partitioning-1-dev}
\begin{bmatrix}
  \vA_{0} &amp; \vA_{0} \\
  \vA_{1} &amp; \vA_{1}
\end{bmatrix}.
\end{equation}\]

<p>Notice that $\eqref{eq:partitioning-0}$ and $\eqref{eq:partitioning-1-dev}$ are
incomparable: the second one uses twice as much memory! But there is a simple
‘algorithm’ to go from a partitioning to a device layout: if an axis is not
partitioned, simply copy the array along that axis to reach the full mesh size
(and remember that this leads to additional memory usage).</p>

<p>Now, what happens if we want to compute the product $\vA \vB$ when $\vA$ is
sharded in some way, for example as in $\eqref{eq:partitioning-0}$?
First, thinking abstractly: by properties of block matrix multiplication, we
have</p>

\[\vA \vB
=
\begin{bmatrix}
  \vA_{11} &amp; \vA_{12} \\
  \vA_{21} &amp; \vA_{22}
\end{bmatrix}
\begin{bmatrix}
  \vB_{11} &amp; \vB_{12} \\
  \vB_{21} &amp; \vB_{22}
\end{bmatrix}
=
\begin{bmatrix}
  \vA_{11}\vB_{11} + \vA_{12} \vB_{21} &amp; \vA_{11}\vB_{12} + \vA_{12} \vB_{22} \\
  \vA_{21}\vB_{11} + \vA_{22} \vB_{21} &amp; \vA_{21}\vB_{12} + \vA_{22} \vB_{22} \\
\end{bmatrix}.\]

<p>So no matter how $\vB$ is sharded, we need to perform communication such that we
compute each of the matrix products appearing above. We can think about two
cases:</p>
<ul>
  <li><strong>No communication involving $\vA$</strong>: Here, we can use the RHS of the above
display to tell us what needs to be computed. Processor $(0, 0)$ needs the
first (block) row of $\vB$, processor $(0, 1)$ needs the second (block) row of
$\vB$, etc.—so the sharding of $\vB$ had better support this, or
communication will be necessary. <em>Regardless</em>, some communication between
devices is necessary to accumulate the block matrix products: e.g., we need to
add results from processor $(0, 0)$ and $(0, 1)$ to get the top-left block of
the product.</li>
  <li>
    <p><strong>We can re-structure how $\vA$ is sharded</strong>: It might be possible, given
a specific sharding for $\vB$, to have a more efficient computation by
sharding $\vA$ differently (and vice versa). Thinking in terms of
block partitions, this is the same as observing that there are many ways of
partitioning a matrix into blocks, each of which gives a different
decomposition for the matrix product. For example, this is also a compatible
partitioning (with suitable definitions of the blocks):</p>

\[\begin{equation}\label{eq:nice-sharding}
\vA \vB
=
\begin{bmatrix}
  \vA_{1} \\
  \vA_{2}
\end{bmatrix}
\begin{bmatrix}
  \vB_{1} &amp; \vB_{2}
\end{bmatrix}
=
\begin{bmatrix}
  \vA_{1}\vB_{1} &amp; \vA_{1}\vB_{2} \\
  \vA_{2}\vB_{1} &amp; \vA_{2}\vB_{2}
\end{bmatrix},
\end{equation}\]

    <p>and so is</p>

\[\vA \vB
=
\begin{bmatrix}
  \vA_{1} &amp; \vA_{2}
\end{bmatrix}
\begin{bmatrix}
  \vB_{1} \\
  \vB_{2}
\end{bmatrix}
=
\vA_1 \vB_1 + \vA_2 \vB_2.\]

    <p>Re-sharding requires communication, though, and so this may or may not be more
efficient.</p>
  </li>
</ul>

<p>As an exercise, verify that $\eqref{eq:nice-sharding}$ corresponds to
a zero-communication sharding for a $2 \times 2$ output sharding.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">3</a></sup></p>

<p>Now, if $\vA$ and $\vB$ both have their own sharding, in general these
correspond to (possibly incompatible) block matrix partitions of each matrix.
For each individual partition, we can, as above, write out the blockwise matrix products
and accumulation operations needed to compute the product, and try to intuit
what computation/communication should happen—in particular involving
re-sharding one or both arrays. In JAX, the XLA compiler will be recruited to
perform this optimization; as algorithm designers, we can try to provide
sensible shardings up front in order to make sure we end up with an efficient
system.</p>

<h1 id="types-of-inter-device-communication-with-sharded-computation">Types of Inter-Device Communication with Sharded Computation</h1>

<p>The matrix multiplication example suggests that different forms of communication
naturally arise in distributed computation. We will describe the ones that are
implemented in XLA here. The JAX scaling book <a class="citation" href="#scaling-book">(Austin et al., 2025)</a> gives
a nice way to think about matrix multiplication of sharded arrays using
a variant of named-axis notation. Since this is closer to code, we’ll describe
this briefly below.</p>

<h2 id="communication-primitives-through-matrix-multiplication">Communication Primitives through Matrix Multiplication</h2>

<p>We saw in the previous section how to think about sharding of 2D arrays in terms
of block matrices:</p>
<ul>
  <li>Sharded array axes are partitioned; non-sharded array axes aren’t.</li>
  <li>Any mesh axis that isn’t used means the partitioned array is copied across
that axis.
This gives us a scheme to map a block matrix to the corresponding arrangement of
that matrix across physical devices.</li>
</ul>

<p>With named index notation, things are more compact. Consider a 2D mesh with axes
$x$ and $y$, and let us write $A[I, J]$ for the 2D array $\vA$ (likewise for
$\vB$) we looked at in the previous section. We write $A[I_x, J]$ (etc.) to
denote the sharding of $A$ along its first dimension with respect to the mesh
axis $x$. The use of $I$, $J$, etc.\ for ‘dummy’ indices is in analogy to
<code class="language-plaintext highlighter-rouge">einsum</code>-type notation, which is very useful for tensor operations.</p>

<p>Now consider a matrix product of sharded matrices $A[I_x, J] \cdot B[J, K_y]$.
Suppose we want the output, say $\vC$, to be sharded as $C[I_x, K_y]$. Then this
matrix product can be computed <em>with no communication</em>: to see why, think in
terms of the correspondence to block matrix partitions and their associated map
to device arrangements in the previous section. However, for an unsharded output
$C[I, K]$, it would be necessary to perform some communication—either
calculating $C[I_x, K_y]$ and then sharing the local results among all devices,
or something else.</p>

<p>The following is a convenient way to think about this, following the previous
example:</p>
<ul>
  <li>Given a sharding of matrix multiplicands $A$, $B$, there is a ‘natural’ output
sharding whenever the contracting dimension is unsharded. (As above, this
comes by ‘removing’ the contracting axis, just like in einsum notation.) If
this sharding is invalid, communication is necessary.</li>
  <li>If the contracting dimension is sharded, different types of communication are
necessary to turn the multiplication into one with an unsharded contracting
dimension. We can identify typical ‘best practices’ for what communication
type to use based on how the contracting dimension is sharded.</li>
  <li>If an output sharding is specified and it is not the ‘natural’ one, some
communication is required to achieve it. We can use the techniques from either
of the two previous bullets to do this.</li>
</ul>

<p>We’ll describe these different cases below.</p>

<h3 id="case-1-unsharded-contracting-dimension-valid-output">Case 1: Unsharded Contracting Dimension, Valid Output</h3>

<p>As above, the ‘natural’ output sharding is just an einsum-style replacement:</p>

\[A[I_x, J] \cdot B[J, K] \to C[I_x, K].\]

<p>If this natural output sharding is invalid, we need to do something else (see
below).</p>

<h3 id="case-2-one-multiplicand-sharded-along-contracting-dimension-allgather">Case 2: One Multiplicand Sharded Along Contracting Dimension (AllGather)</h3>

<p>For a product like</p>

\[A[I, J_x] \cdot B[J, K] \to C[I, K],\]

<p>it isn’t possible to directly perform the multiplication, because in order to
contract, we either need to perform a series of block-matrix multiplies, then
accumulate the results (see below), or have the entire contracting dimension of
both arrays available on all devices (see above).</p>

<p>We use <strong>AllGather</strong> for this, which removes a specified sharded axis:</p>

\[\mathrm{AllGather}_{x}(A[I, J_x]) \to A[I, J].\]

<p>For a compound mesh, AllGather can also just remove one sub-axis:</p>

\[\mathrm{AllGather}_{x}(A[I, J_{xy}]) \to A[I, J_y].\]

<p>AllGather can also be used to satisfy certain output shardings that aren’t
naturally compatible with the input shardings. For example, in the product</p>

\[A[I_x, J] \cdot B[J, K] \to C[I, K],\]

<p>one removes the sharded axis either before or after the computation in order to
achieve the desired output sharding.</p>

<h4 id="allgather-cost">AllGather Cost</h4>

<p><strong>On TPUs</strong> with enough chips to form a cube, AllGather can be performed using
algorithms that exploit the toroidal interconnectivity of TPU devices. For
example, to AllGather a single axis, the following procedure can be used:[^5]</p>
<ol>
  <li>Initialize a local buffer on each device with its local shard.</li>
  <li>Send the buffer contents to the next device; overwrite it with the contents of
  the previous device.</li>
  <li>Store the received shard in memory.</li>
  <li>Repeat.</li>
</ol>

<p>This takes at most $N/2$ rounds of communication if the mesh axis has size $N$.
For an array of size $V$ bytes, each round sends $V/N$ bytes over each link.
Given an ICI bandwidth of $W$ (unidirectional),<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">4</a></sup> the total time this takes is
no more than</p>

\[\frac{N}{2} \cdot \frac{V}{NW} = \frac{V}{2W} \text{ s},\]

<p>which is independent of the size of the mesh.
However, one must also take into account the intrinsic overhead of ICI
communication. Each operation takes around $T_{\min} = 10^{-6} \text{ s}$ <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>, so the total time is actually only bounded by</p>

\[\max \set*{\frac{V}{2W}, \frac{NT_{\min}}{2}}.\]

<p>In particular, there are <em>latency-bound</em> (small array size) and
<em>throughput-bound</em> (large array size) regimes of communication.</p>

<p>AllGathering a compound axis (e.g., $xy$) is similar: one can utilize some
per-device state to efficiently communicate data across the entire
multidimensional mesh. Since each device can then communicate along each mesh
axis simultaneously, the bandwidth increases by a factor proportional to the
number of mesh axes, but the maximum possible latency also increases, since
there are more devices. The total time is no larger than</p>

\[\max \set*{\frac{V}{2n_{\mathrm{axes}}W}, \frac{N_{\mathrm{total}}T_{\min}}{2}},\]

<p>where $N_{\mathrm{total}}$ is the product of the axes lengths.</p>

<h3 id="case-3-both-multiplicands-sharded-along-contracting-dimension-allreduce">Case 3: Both Multiplicands Sharded Along Contracting Dimension (AllReduce)</h3>

<p>In this case, we want to compute a matrix product like</p>

\[A[I, J_x] \cdot B[J_x, K] \to C[I, K].\]

<p>Thinking in terms of block matrices, we know that computing this matrix product
entails a sum of the block matrix multiplications corresponding to the shards of
both $A$ and $B$, and since the contracting axis is sharded, we can:</p>
<ol>
  <li>Compute per-device multiplications of the sharded matrices.</li>
  <li>Add these up.</li>
</ol>

<p>The communication primitive that performs this reduction is called
<strong>AllReduce</strong>. It is similar to AllGather, but it sums up the components being
communicated, and distributes the resulting sum to all devices.</p>

<p>We will use the notation from <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>: we write</p>

\[C[I, K]\{ U_x \}\]

<p>to denote an array $C$ which is “unreduced” over the mesh axis $x$, i.e. the
partial products are sharded over the $x$ mesh axis, which gives</p>

\[\mathrm{AllReduce}_y\left( A[I_x, J]\{ U_y \} \right) \to A[I_x, J]\]

<p>as the signature for AllReduce.</p>

<h4 id="decomposition-in-terms-of-reducescatter">Decomposition in Terms of ReduceScatter</h4>

<p>Generally, an AllReduce is about <em>two times as expensive as an AllGather</em>. One way
to see why is to decompose it into another useful communication primitive: the
<strong>ReduceScatter</strong> operation.</p>

<p>ReduceScatter has signature</p>

\[\mathrm{ReduceScatter}_{y, J}\left( A[I_x, J]\{ U_y \} \right) \to A[I_x, J_y].\]

<p>It takes an array unreduced along a mesh axis, then shards the array along
a specified array axis with respect to that mesh axis. To compute an AllReduce,
we can compose a ReduceScatter with an AllGather:</p>

\[\mathrm{AllReduce}_y\left( A[I_x, J]\{ U_y \} \right)
=
\mathrm{AllGather}_{y}\left(
\mathrm{ReduceScatter}_{y, J}\left( A[I_x, J]\{ U_y \} \right)
\right).\]

<p>For multidimensional arrays, there is a possibility to choose the array axis
that is ReduceScattered to optimize performance.</p>

<p>ReduceScatter can be performed by an algorithm similar to the approach we
discussed for AllGather. The process is somewhat difficult to describe
algorithmically, but there is a very good pictorial representation in <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>.</p>

<p><img src="/assets/blog/reduce-scatter.gif" alt="Reduce Scatter" /></p>

<p>The upshot is that this similarity persists to the latency and
throughput analysis of the algorithm, making it identical to that of AllGather.
This demonstrates the cost of AllReduce as twice that of AllGather.</p>

<h4 id="aside-the-backward-pass">Aside: The Backward Pass</h4>

<p>There is a more fundamental relationship between the AllGather and ReduceScatter
operations, suggested by the representation of AllReduce above: they can be
represented as adjoints of one another, which means that an AllGather in the
forward pass will trigger a ReduceScatter in the backward pass!</p>

<p>To see this, let’s go back to our more visual block matrix example. As above,
consider a simplified setting where a matrix is sharded along a 1D mesh
corresponding to the first array dimension. With mesh size $3$ for simplicity,
we represent this as the block matrix</p>

\[\vA = \begin{bmatrix}
\vA_0 \\
\vA_1 \\
\vA_2
\end{bmatrix}.\]

<p>Remember that the partitioning of the block matrix corresponds to the device
layout in this model.
Now, if we perform an AllGather, we have the ($3\times$ larger) block matrix
output</p>

\[\mathrm{AllGather}(\vA) = \begin{bmatrix}
\vA \\
\vA \\
\vA
\end{bmatrix}
=
\begin{bmatrix}
\begin{bmatrix}
\vA_0 \\
\vA_1 \\
\vA_2
\end{bmatrix}
\\
\begin{bmatrix}
\vA_0 \\
\vA_1 \\
\vA_2
\end{bmatrix}
\\
\begin{bmatrix}
\vA_0 \\
\vA_1 \\
\vA_2
\end{bmatrix}
\end{bmatrix}.\]

<p>Let’s imagine we perform some subsequent computations on the AllGathered output,
and we want to compute a backward pass. In general, each individual device will
have different gradients, which we can write in partitioned form as</p>

\[\begin{bmatrix}
\Delta \vA^0 \\
\Delta \vA^1 \\
\Delta \vA^2
\end{bmatrix}
=
\begin{bmatrix}
\begin{bmatrix}
\Delta \vA_0^0 \\
\Delta \vA_1^0 \\
\Delta \vA_2^0
\end{bmatrix}
\\
\begin{bmatrix}
\Delta \vA_0^1 \\
\Delta \vA_1^1 \\
\Delta \vA_2^1
\end{bmatrix}
\\
\begin{bmatrix}
\Delta \vA_0^2 \\
\Delta \vA_1^2 \\
\Delta \vA_2^2
\end{bmatrix}
\end{bmatrix}.\]

<p>Then because AllGather behaves in the sense of linear algebra as a copying
operation, its backward pass involves its adjoint operation, which is a sum.
We end up with<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">5</a></sup></p>

\[\mathrm{dAllGather}^* \left(
\begin{bmatrix}
\Delta \vA^0 \\
\Delta \vA^1 \\
\Delta \vA^2
\end{bmatrix}
\right)
=
\begin{bmatrix}
\sum_{i=1}^3\Delta \vA^i_0 \\
\sum_{i=1}^3 \Delta \vA^i_1 \\
\sum_{i=1}^3 \Delta \vA^i_2
\end{bmatrix}.\]

<p>This is exactly what we’d get if we:</p>
<ol>
  <li>Interpret the per-device gradients as unreduced partial sums with respect to
the mesh axis of the AllGather;</li>
  <li>Perform a ReduceScatter with respect to this mesh axis and the array axis
that was sharded before the AllGather.</li>
</ol>

<p>So, an AllGather in the forward pass produces a corresponding ReduceScatter in
the backward pass!</p>

<p>Similarly, if there is a ReduceScatter in the forward pass that produces an
array sharded along one of its axes, then in the backward pass, there will be an
AllGather operation along that axis in the backward pass.
And finally, because an AllReduce can be expressed as the composition of
an AllGather with a ReduceScatter, <em>the backward pass of an AllReduce is an
AllReduce</em>!</p>

<h3 id="case-4-sharded-non-contracting-dimensions-same-axis">Case 4: Sharded Non-Contracting Dimensions, Same Axis</h3>

<p>A direct reduction to the natural sharding for this case would lead to an
invalid sharding:</p>

\[A[I_x, J] \cdot B[J, K_x] \to C[I_x, K_x]\]

<p>A mesh axis cannot appear multiple times in a sharding specification.</p>

<p>To resolve this, we just AllGather one of the axes in the multiplicands:</p>

\[\begin{split}
\mathrm{AllGather}_x(A[I_x, J]) \to A[I, J] \\
A[I, J] \cdot B[J, K_x] \to C[I, K_x].
\end{split}\]

<p>We can select which depending on the downstream shardings needed.</p>

<h2 id="alltoall-communication">AllToAll Communication</h2>

<p>This primitive can be thought of as a resharding operation. It has signature</p>

\[\mathrm{AllToAll}_{x, J}( A[I_x, J] ) \to A[I, J_x].\]

<p>AllToAll is actually significantly cheaper than an AllGather, by a factor of 4.
This is another one where the excellent picture from <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>
describes where this efficiency is coming from well.</p>

<p><img src="/assets/blog/all-to-all.gif" alt="All To All" /></p>

<p>Notice that the message
that is being sent over each link is composed of pieces of different lengths
from each shard. The communication is balanced, but later messages are shorter,
which may cause issues with latency-boundedness of the communication.</p>

<h1 id="example-communication-primitives-in-jax">Example: Communication Primitives in JAX</h1>

<p>Let’s test out what we learned above in the JAX sandbox we started this post
with. For slightly more granular control of sharding, we’ll switch to Explicit
mode sharding (see e.g. <a class="citation" href="#jax-explicit-sharding">(JAX Team, 2025)</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">),</span> <span class="n">axis_types</span><span class="o">=</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">AxisType</span><span class="p">.</span><span class="n">Explicit</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="p">)</span>  <span class="c1"># 1x v4 tpu host
</span><span class="n">jax</span><span class="p">.</span><span class="nf">set_mesh</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">8192</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">)))</span>

<span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
ShapedArray(bfloat16[2048@x,8192@y])
</code></pre></div></div>

<h2 id="allgather">AllGather</h2>

<p>First, a simple AllGather implementation along the <code class="language-plaintext highlighter-rouge">y</code> mesh axis—we compile
the resharding operation and inspect the “high-level operations” (HLO) to see
what actually gets implemented on the device.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nf">reshard</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">)))</span>


<span class="n">lowered</span> <span class="o">=</span> <span class="n">all_gather</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">compiled</span> <span class="o">=</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">B</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output type: </span><span class="sh">"</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">B</span><span class="p">))</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">StableHLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">XLA HLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">compiled</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Output type:  bfloat16[2048@x,8192]

StableHLO:
 module @jit_all_gather attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = &lt;["x"=2, "y"=2]&gt;
  func.func public @main(%arg0: tensor&lt;2048x8192xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {"y"}]&gt;}) -&gt; (tensor&lt;2048x8192xbf16&gt; {jax.result_info = "result", sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {}]&gt;}) {
    %0 = sdy.sharding_constraint %arg0 &lt;@mesh, [{"x"}, {}]&gt; : tensor&lt;2048x8192xbf16&gt;
    return %0 : tensor&lt;2048x8192xbf16&gt;
  }
}

XLA HLO:
 HloModule jit_all_gather, is_scheduled=true, entry_computation_layout={(bf16[1024,4096]{1,0:T(8,128)(2,1)})-&gt;bf16[1024,8192]{1,0:T(8,128)(2,1)}}, num_partitions=4
ENTRY %main.5_spmd (param: bf16[1024,4096]) -&gt; bf16[1024,8192] {
  %param = bf16[1024,4096]{1,0:T(8,128)(2,1)} parameter(0), sharding={devices=[2,2]&lt;=[4]}, metadata={op_name="x"}
  %all-gather = bf16[1024,8192]{1,0:T(8,128)(2,1)S(3)} all-gather(%param), channel_id=1, replica_groups=[2,2]&lt;=[4], dimensions={1}, use_global_device_ids=true, metadata={op_name="jit(all_gather)/reshard" source_file="/tmp/ipykernel_794398/1925679859.py" source_line=3}, backend_config={"flag_configs":[],"barrier_config":{"barrier_type":"CUSTOM","id":"0"},"scoped_memory_configs":[],"collective_algorithm_config":{"emitter":"1DAllGatherNonMajorDim","debug":"\ngroup_size = 2 \nhas_reordering_map: false \nper_stride_size = 65536 bytes \nshard_size = 8388608 bytes "},"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"0"}],"retry_config":{"retry_count":"0"}}
  ROOT %copy.3 = bf16[1024,8192]{1,0:T(8,128)(2,1)} copy(%all-gather), backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":[],"output_window_bounds":["16","64"],"input_window_bounds":[],"estimated_cycles":"58232","iteration_bounds":["8","1"]},"megacore_config":{"megacore_split_dim":"0"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"8388608"}],"retry_config":{"retry_count":"0"}}
}
</code></pre></div></div>

<p>The first representation above (output of <code class="language-plaintext highlighter-rouge">lower</code>) is in XLA’s StableHLO
language. It is slightly imposing, but there is a clearly written
<a href="https://github.com/openxla/stablehlo/blob/main/docs/spec.md">spec</a> that helps
with parsing it after a bit of studying. It’s still too high level to clearly
show what communication primitives are being generated, though.</p>

<p>The second representation (output of <code class="language-plaintext highlighter-rouge">compile</code>) is the result of the XLA
compiler’s first device-independent compilation stage; it’s in its own dialect
of HLO, which is different from StableHLO and doesn’t have as easily-accessible
of a public spec.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">6</a></sup>
Here’s how to parse it above:</p>
<ul>
  <li>Types are relatively self-explanatory, aside from the text in braces—the
first sequence of integers here is most important, and it specifies the memory
layout of the array (see <code class="language-plaintext highlighter-rouge">layout.h</code> in the XLA source). Axis indices are
listed in minor-to-major order (i.e., <code class="language-plaintext highlighter-rouge">1, 0</code> denotes row-major layout).</li>
  <li>The <code class="language-plaintext highlighter-rouge">all-gather</code> semantics are clear at a high level: we gather along
dimension $1$ of our array. The <code class="language-plaintext highlighter-rouge">replica_groups</code> argument specifies which
devices communicate in the gather (see <code class="language-plaintext highlighter-rouge">tile_assignment.h</code> in the XLA
source)—compare to our input <code class="language-plaintext highlighter-rouge">%param</code>’s <code class="language-plaintext highlighter-rouge">sharding</code> specified above to see
that these agree.</li>
  <li>Independent of these lower-level details, we can see that the input and output
shapes are as we expect.</li>
</ul>

<h2 id="alltoall">AllToAll</h2>

<p>We can test out AllToAll with a similar high-level API. Let’s start by
transposing the sharding axis of the output of the previous AllGather.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">all_to_all</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nf">reshard</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">)))</span>


<span class="n">lowered</span> <span class="o">=</span> <span class="n">all_to_all</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">compiled</span> <span class="o">=</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">C</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output type: </span><span class="sh">"</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">StableHLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">XLA HLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">compiled</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
</code></pre></div></div>
<p>The HLO that we get is more complex:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Output type:  bfloat16[2048,8192@x]
StableHLO:
 module @jit_all_to_all attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = &lt;["x"=2, "y"=2]&gt;
  func.func public @main(%arg0: tensor&lt;2048x8192xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {}]&gt;}) -&gt; (tensor&lt;2048x8192xbf16&gt; {jax.result_info = "result", sdy.sharding = #sdy.sharding&lt;@mesh, [{}, {"x"}]&gt;}) {
    %0 = sdy.sharding_constraint %arg0 &lt;@mesh, [{}, {"x"}]&gt; : tensor&lt;2048x8192xbf16&gt;
    return %0 : tensor&lt;2048x8192xbf16&gt;
  }
}
XLA HLO:
 HloModule jit_all_to_all, is_scheduled=true, entry_computation_layout={(bf16[1024,8192]{1,0:T(8,128)(2,1)})-&gt;bf16[2048,4096]{1,0:T(8,128)(2,1)}}, num_partitions=4
%fused_computation (param_0.1: bf16[128,2,8,4096]) -&gt; bf16[1024,2,4096] {
  %param_0.1 = bf16[128,2,8,4096]{3,2,1,0:T(8,128)(2,1)} parameter(0)
  %copy.4 = bf16[128,2,8,4096]{3,2,0,1:T(8,128)(2,1)} copy(%param_0.1), metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/2971478236.py" source_line=3}
  ROOT %bitcast.5 = bf16[1024,2,4096]{2,0,1:T(8,128)(2,1)S(3)} bitcast(%copy.4), metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/2971478236.py" source_line=3}
}
ENTRY %main.5_spmd (param: bf16[1024,8192]) -&gt; bf16[2048,4096] {
  %param = bf16[1024,8192]{1,0:T(8,128)(2,1)} parameter(0), sharding={devices=[2,1,2]&lt;=[4] last_tile_dim_replicate}, metadata={op_name="x"}
  %bitcast.7 = bf16[128,2,8,4096]{3,2,1,0:T(8,128)(2,1)} bitcast(%param)
  %copy_bitcast_fusion = bf16[1024,2,4096]{2,0,1:T(8,128)(2,1)S(3)} fusion(%bitcast.7), kind=kLoop, calls=%fused_computation, metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/2971478236.py" source_line=3}, backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":[],"output_window_bounds":["1","32","32"],"input_window_bounds":[],"estimated_cycles":"78212","iteration_bounds":["2","4","1"]},"megacore_config":{"megacore_split_dim":"1"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"8388608"}],"retry_config":{"retry_count":"0"}}
  %all-to-all = bf16[1024,2,4096]{2,0,1:T(8,128)(2,1)S(3)} all-to-all(%copy_bitcast_fusion), channel_id=1, replica_groups=[2,2]&lt;=[2,2]T(1,0), dimensions={1}, metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/2971478236.py" source_line=3}, backend_config={"flag_configs":[],"barrier_config":{"barrier_type":"GLOBAL","id":"-1"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"262144"}],"retry_config":{"retry_count":"0"}}
  %bitcast.6 = bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)} bitcast(%all-to-all)
  ROOT %copy.6 = bf16[2048,4096]{1,0:T(8,128)(2,1)} copy(%bitcast.6), backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":[],"output_window_bounds":["32","32"],"input_window_bounds":[],"estimated_cycles":"58232","iteration_bounds":["8","1"]},"megacore_config":{"megacore_split_dim":"0"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"8388608"}],"retry_config":{"retry_count":"0"}}
}
</code></pre></div></div>

<p>Some amount of the above amounts to memory optimizations that are necessary to
actually implement the communication in physical memory.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">7</a></sup> We can more easily
interpret what’s going on by thinking in terms of block matrices again:
recall that we started with a $2048 \times 8192$ array $\vA$, sharded two ways:</p>

\[\vA = \begin{bmatrix}
\vA_{00} &amp; \vA_{01} \\
\vA_{10} &amp; \vA_{11}
\end{bmatrix},\]

<p>and we performed an AllGather along the second axis, leading to (we’ll use
$\oplus_i$ for concatenation along dimension $i$)</p>

\[\vB = \begin{bmatrix}
\vA_{00} \oplus_1 \vA_{01} &amp; \vA_{00} \oplus_1 \vA_{01} \\
\vA_{10} \oplus_1 \vA_{11} &amp; \vA_{10} \oplus_1 \vA_{11}
\end{bmatrix}.\]

<p>The output of the AllToAll reshards along the trailing axis, <em>but with respect
to the <code class="language-plaintext highlighter-rouge">'x'</code> mesh axis</em>, which gives</p>

\[\vC = \begin{bmatrix}
\vA_{00} \oplus_0 \vA_{10} &amp; \vA_{00} \oplus_0 \vA_{10}  \\
\vA_{01} \oplus_0 \vA_{11} &amp; \vA_{01} \oplus_0 \vA_{11}
\end{bmatrix}.\]

<p>To go from $\vB$ to $\vC$, we can do the following, which is essentially what
the HLO above is doing (modulo memory operations): unbind the concatenated
arrays, to give (in a rough approximation of tensor notation)</p>

\[\begin{bmatrix}
(\vA_{00}, \vA_{01}) &amp; (\vA_{00}, \vA_{01}) \\
(\vA_{10}, \vA_{11}) &amp; (\vA_{10}, \vA_{11})
\end{bmatrix},\]

<p>then communicate ‘vertically’, swapping $01$ and $10$ on the left and right, to
get</p>

\[\begin{bmatrix}
(\vA_{00}, \vA_{10}) &amp; (\vA_{00}, \vA_{10}) \\
(\vA_{01}, \vA_{11}) &amp; (\vA_{01}, \vA_{11})
\end{bmatrix}.\]

<p>If we concatenate these in the right order, we end up with $\vC$!
We can see that this is what the HLO is doing:</p>
<ul>
  <li>After some memory manipulations, an extra axis of size $2$ is split off from
the second dimension, making the local array shaped like $1024 \times 2 \times
4096$.</li>
  <li>The <code class="language-plaintext highlighter-rouge">all-to-all</code> function has to be interpreted relative to the input
sharding. Here <code class="language-plaintext highlighter-rouge">{devices=[2,1,2]&lt;=[4] last_tile_dim_replicate}</code> denotes a <code class="language-plaintext highlighter-rouge">[2,
1]</code> tile size, replicated twice; <code class="language-plaintext highlighter-rouge">&lt;=[4]</code> is equivalent to a device list <code class="language-plaintext highlighter-rouge">{0,
1, 2, 3}</code>. This is as we expect—the input is sharded over <code class="language-plaintext highlighter-rouge">'x'</code>.</li>
  <li>Now, in <code class="language-plaintext highlighter-rouge">all-to-all</code>, <code class="language-plaintext highlighter-rouge">replica_groups=[2,2]&lt;=[2,2]T(1,0)</code> specifies how the
communication is done across devices (relative to the <code class="language-plaintext highlighter-rouge">sharding</code> previously
specified). This corresponds to a specification of <code class="language-plaintext highlighter-rouge">{{0, 2}, {1,
3}}</code> (see <code class="language-plaintext highlighter-rouge">hlo_sharding.h</code> in the XLA source), which means that
each replica’s array gets split into two parts along dimension $1$ (the axis
of length $2$), then communication occurs across the specified groups. This
gets us what we wanted above.</li>
</ul>

<p>It’s slightly interesting to note that different input sharding and an AllToAll
leads to different memory operations in the HLO:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">all_to_all</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nf">reshard</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">)))</span>


<span class="n">lowered</span> <span class="o">=</span> <span class="n">all_to_all</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">compiled</span> <span class="o">=</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">D</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output type: </span><span class="sh">"</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">StableHLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">XLA HLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">compiled</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Output type:  bfloat16[2048@x,8192]
StableHLO:
 module @jit_all_to_all attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = &lt;["x"=2, "y"=2]&gt;
  func.func public @main(%arg0: tensor&lt;2048x8192xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{}, {"x"}]&gt;}) -&gt; (tensor&lt;2048x8192xbf16&gt; {jax.result_info = "result", sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {}]&gt;}) {
    %0 = sdy.sharding_constraint %arg0 &lt;@mesh, [{"x"}, {}]&gt; : tensor&lt;2048x8192xbf16&gt;
    return %0 : tensor&lt;2048x8192xbf16&gt;
  }
}
XLA HLO:
 HloModule jit_all_to_all, is_scheduled=true, entry_computation_layout={(bf16[2048,4096]{1,0:T(8,128)(2,1)})-&gt;bf16[1024,8192]{1,0:T(8,128)(2,1)}}, num_partitions=4
ENTRY %main.5_spmd (param: bf16[2048,4096]) -&gt; bf16[1024,8192] {
  %param = bf16[2048,4096]{1,0:T(8,128)(2,1)} parameter(0), sharding={devices=[1,2,2]&lt;=[4] last_tile_dim_replicate}, metadata={op_name="x"}
  %bitcast.5 = bf16[2,1024,4096]{2,1,0:T(8,128)(2,1)} bitcast(%param)
  %all-to-all = bf16[2,1024,4096]{2,1,0:T(8,128)(2,1)S(3)} all-to-all(%bitcast.5), channel_id=1, replica_groups=[2,2]&lt;=[2,2]T(1,0), dimensions={0}, metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/814736672.py" source_line=3}, backend_config={"flag_configs":[],"barrier_config":{"barrier_type":"GLOBAL","id":"-1"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"0"}],"retry_config":{"retry_count":"0"}}
  %copy.3 = bf16[2,1024,4096]{1,2,0:T(8,128)(2,1)S(3)} copy(%all-to-all), metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/814736672.py" source_line=3}, backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":[],"output_window_bounds":["1","48","8"],"input_window_bounds":["1","128","3"],"estimated_cycles":"47696","iteration_bounds":["1","11","1"]},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"3145728"}],"retry_config":{"retry_count":"0"}}
  %bitcast.4 = bf16[1024,8192]{0,1:T(8,128)(2,1)S(3)} bitcast(%copy.3)
  ROOT %copy.4 = bf16[1024,8192]{1,0:T(8,128)(2,1)} copy(%bitcast.4), metadata={op_name="jit(all_to_all)/reshard" source_file="/tmp/ipykernel_874047/814736672.py" source_line=3}, backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":[],"output_window_bounds":["64","6"],"input_window_bounds":["96","4"],"estimated_cycles":"67782","iteration_bounds":["1","11"]},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"3182592"}],"retry_config":{"retry_count":"0"}}
}
</code></pre></div></div>

<p>The HLO for this operation is more concise, but neither is able to avoid
a <code class="language-plaintext highlighter-rouge">copy</code> operation.</p>

<h2 id="allreduce-and-reducescatter">AllReduce and ReduceScatter</h2>

<p>To trigger an AllReduce/ReduceScatter, let’s follow our matrix multiplication example
above—we’ll multiply two arrays that are both sharded along the contracting
dimension.
In the AllToAll section, we’ve generated <code class="language-plaintext highlighter-rouge">C</code>, which is sharded along <code class="language-plaintext highlighter-rouge">'x'</code> in
its 1st dimension. We can try computing a Gram matrix associated to <code class="language-plaintext highlighter-rouge">C</code>, and
specifying the <code class="language-plaintext highlighter-rouge">out_sharding</code> (singular!) in the <code class="language-plaintext highlighter-rouge">dot</code> operation to avoid the
compiler complaining:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">gram</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">mT</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()))</span>


<span class="n">lowered</span> <span class="o">=</span> <span class="n">gram</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="n">compiled</span> <span class="o">=</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">E</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output type: </span><span class="sh">"</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">E</span><span class="p">))</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">StableHLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">XLA HLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">compiled</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Output type:  bfloat16[2048,2048]
StableHLO:
 module @jit_gram attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = &lt;["x"=2, "y"=2]&gt;
  func.func public @main(%arg0: tensor&lt;2048x8192xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{}, {"x"}]&gt;}) -&gt; (tensor&lt;2048x2048xbf16&gt; {jax.result_info = "result", sdy.sharding = #sdy.sharding&lt;@mesh, [{}, {}]&gt;}) {
    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor&lt;2048x8192xbf16&gt;) -&gt; tensor&lt;8192x2048xbf16&gt;
    %1 = sdy.sharding_constraint %0 &lt;@mesh, [{"x"}, {}]&gt; : tensor&lt;8192x2048xbf16&gt;
    %2 = stablehlo.dot_general %arg0, %1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor&lt;2048x8192xbf16&gt;, tensor&lt;8192x2048xbf16&gt;) -&gt; tensor&lt;2048x2048xbf16&gt;
    %3 = sdy.sharding_constraint %2 &lt;@mesh, [{}, {}]&gt; : tensor&lt;2048x2048xbf16&gt;
    return %3 : tensor&lt;2048x2048xbf16&gt;
  }
}
XLA HLO:
 HloModule jit_gram, is_scheduled=true, entry_computation_layout={(bf16[2048,4096]{1,0:T(8,128)(2,1)})-&gt;bf16[2048,2048]{1,0:T(8,128)(2,1)}}, num_partitions=4
%add.clone (x.1: bf16[], y.1: bf16[]) -&gt; bf16[] {
  %y.1 = bf16[]{:T(256)} parameter(1)
  %x.1 = bf16[]{:T(256)} parameter(0)
  ROOT %add.1 = bf16[]{:T(256)} add(%x.1, %y.1)
}
%bitcast_fusion (bitcast_input: bf16[2048,4096]) -&gt; bf16[2048,4096] {
  %bitcast_input = bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)} parameter(0)
  ROOT %bitcast = bf16[2048,4096]{1,0:T(8,128)(2,1)} bitcast(%bitcast_input)
}
%bitcast_fusion.1 (bitcast_input.1: bf16[2048,4096]) -&gt; bf16[2048,4096] {
  %bitcast_input.1 = bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)} parameter(0)
  ROOT %bitcast.1 = bf16[2048,4096]{1,0:T(8,128)(2,1)} bitcast(%bitcast_input.1)
}
%fused_computation (param_0: bf16[2048,4096]) -&gt; bf16[2048,2048] {
  %param_0 = bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)} parameter(0)
  %fusion.1 = bf16[2048,4096]{1,0:T(8,128)(2,1)} fusion(%param_0), kind=kLoop, calls=%bitcast_fusion
  %fusion.2 = bf16[2048,4096]{1,0:T(8,128)(2,1)} fusion(%param_0), kind=kLoop, calls=%bitcast_fusion.1
  ROOT %convolution.1 = bf16[2048,2048]{1,0:T(8,128)(2,1)S(3)} convolution(%fusion.1, %fusion.2), dim_labels=bf_oi-&gt;bf, metadata={op_name="jit(gram)/dot_general" source_file="/tmp/ipykernel_874047/670048905.py" source_line=3}
}
ENTRY %main.8_spmd (param: bf16[2048,4096]) -&gt; bf16[2048,2048] {
  %param = bf16[2048,4096]{1,0:T(8,128)(2,1)} parameter(0), sharding={devices=[1,2,2]&lt;=[4] last_tile_dim_replicate}, metadata={op_name="x"}
  %copy-start = (bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)}, bf16[2048,4096]{1,0:T(8,128)(2,1)}, u32[]{:S(2)}) copy-start(%param), cross_program_prefetch_index=0
  %copy-done = bf16[2048,4096]{1,0:T(8,128)(2,1)S(3)} copy-done(%copy-start)
  %fusion = bf16[2048,2048]{1,0:T(8,128)(2,1)S(3)} fusion(%copy-done), kind=kOutput, calls=%fused_computation, metadata={op_name="jit(gram)/dot_general" source_file="/tmp/ipykernel_874047/670048905.py" source_line=3}, backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":["64","32"],"output_window_bounds":["64","4"],"input_window_bounds":["64","32"],"estimated_cycles":"149352","iteration_bounds":["4","4","1"]},"megacore_config":{"megacore_split_dim":"0"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"5734400"}],"retry_config":{"retry_count":"0"},"convolution_algorithm_config":{"emitter":"EmitAllBatchInSublanes"}}
  ROOT %all-reduce = bf16[2048,2048]{1,0:T(8,128)(2,1)} all-reduce(%fusion), channel_id=1, replica_groups=[2,2]&lt;=[2,2]T(1,0), use_global_device_ids=true, to_apply=%add.clone, metadata={op_name="jit(gram)/dot_general" source_file="/tmp/ipykernel_874047/670048905.py" source_line=3}, backend_config={"flag_configs":[],"barrier_config":{"barrier_type":"CUSTOM","id":"0"},"scoped_memory_configs":[{"memory_space":"0","offset":"0","size":"67108864"}],"collective_algorithm_config":{"emitter":"RotatedPincerEmitter","strategy":"UniDirection1DRingStrategy","debug":"\nUniDirection1DRingStrategy{colors:2 phases:1 cores:{2},{2} nophase0:0 reserved_sflags:0 cross_module_on_2d_plane:0 has_reordering_map:0 use_routing_table_indices:0}"},"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"12582912"}],"retry_config":{"retry_count":"0"}}
}
</code></pre></div></div>

<p>The above is relatively clear! A <code class="language-plaintext highlighter-rouge">copy</code> operation accounts for the fact that
we’re multiplying the matrix with itself to calculate the Gram matrix, then the
actual multiplication is performed with the XLA <code class="language-plaintext highlighter-rouge">convolution</code> operation. It’s
performed on each local shard, and then the local shards are accumulated and
shared with an AllReduce.</p>

<p>We expect to be able to trigger a ReduceScatter instead of an AllReduce by
asking the computation output to be sharded in a particular way (rather than
fully replicated, as above). Let’s try to verify this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">dot_custom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)))</span>

<span class="n">F</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">8192</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">)))</span>

<span class="n">lowered</span> <span class="o">=</span> <span class="n">dot_custom</span><span class="p">.</span><span class="nf">lower</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span>
<span class="n">compiled</span> <span class="o">=</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">compile</span><span class="p">()</span>
<span class="n">G</span> <span class="o">=</span> <span class="nf">compiled</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span>


<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Output type: </span><span class="sh">"</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">typeof</span><span class="p">(</span><span class="n">G</span><span class="p">))</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">StableHLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">lowered</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">XLA HLO:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">compiled</span><span class="p">.</span><span class="nf">as_text</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Output type:  bfloat16[2048@x,4096@y]
StableHLO:
 module @jit_dot_custom attributes {mhlo.num_partitions = 4 : i32, mhlo.num_replicas = 1 : i32} {
  sdy.mesh @mesh = &lt;["x"=2, "y"=2]&gt;
  func.func public @main(%arg0: tensor&lt;2048x8192xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {"y"}]&gt;}, %arg1: tensor&lt;8192x4096xbf16&gt; {sdy.sharding = #sdy.sharding&lt;@mesh, [{"y"}, {}]&gt;}) -&gt; (tensor&lt;2048x4096xbf16&gt; {jax.result_info = "result", sdy.sharding = #sdy.sharding&lt;@mesh, [{"x"}, {"y"}]&gt;}) {
    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor&lt;2048x8192xbf16&gt;, tensor&lt;8192x4096xbf16&gt;) -&gt; tensor&lt;2048x4096xbf16&gt;
    %1 = sdy.sharding_constraint %0 &lt;@mesh, [{"x"}, {"y"}]&gt; : tensor&lt;2048x4096xbf16&gt;
    return %1 : tensor&lt;2048x4096xbf16&gt;
  }
}
XLA HLO:
 HloModule jit_dot_custom, is_scheduled=true, entry_computation_layout={(bf16[1024,4096]{1,0:T(8,128)(2,1)}, bf16[4096,4096]{1,0:T(8,128)(2,1)})-&gt;bf16[1024,2048]{1,0:T(8,128)(2,1)}}, allow_spmd_sharding_propagation_to_parameters={false,false}, num_partitions=4
%add.1.clone (x.3: bf16[], y.3: bf16[]) -&gt; bf16[] {
  %y.3 = bf16[]{:T(256)} parameter(1)
  %x.3 = bf16[]{:T(256)} parameter(0)
  ROOT %add.3 = bf16[]{:T(256)} add(%x.3, %y.3)
}
%all-reduce-scatter (input: bf16[1024,4096]) -&gt; bf16[1024,2048] {
  %input = bf16[1024,4096]{1,0:T(8,128)(2,1)S(3)} parameter(0)
  %all-reduce.2 = bf16[1024,4096]{1,0:T(8,128)(2,1)} all-reduce(%input), channel_id=3, replica_groups={{0,1},{2,3}}, use_global_device_ids=true, to_apply=%add.1.clone, frontend_attributes={from-cross-replica-sharding="true"}, backend_config={"flag_configs":[],"barrier_config":{"barrier_type":"CUSTOM","id":"0"},"scoped_memory_configs":[],"used_scoped_memory_configs":[]}
  %constant.13 = u32[] constant(0)
  %constant.shard_id_table = u32[4]{0:T(128)} constant({0, 1, 0, 1})
  %partition-id.1 = u32[] partition-id()
  %dynamic-slice.3 = u32[1]{0:T(128)} dynamic-slice(%constant.shard_id_table, %partition-id.1), dynamic_slice_sizes={1}, backend_config={"flag_configs":[],"scoped_memory_configs":[],"indices_config":{"index_known_bits":[{"zeroes":"0","ones":"0","bitwidth":"32"}],"is_index_aligned":[false]},"used_scoped_memory_configs":[]}
  %bitcast.1 = u32[]{:T(128)} bitcast(%dynamic-slice.3)
  %constant.14 = u32[] constant(2048)
  %multiply.3 = u32[]{:T(128)} multiply(%bitcast.1, %constant.14)
  ROOT %dynamic-slice.4 = bf16[1024,2048]{1,0:T(8,128)(2,1)} dynamic-slice(%all-reduce.2, %constant.13, %multiply.3), dynamic_slice_sizes={1024,2048}, backend_config={"flag_configs":[],"scoped_memory_configs":[],"indices_config":{"index_known_bits":[{"zeroes":"4294967295","ones":"0","bitwidth":"32"},{"zeroes":"4294961151","ones":"0","bitwidth":"32"}],"is_index_aligned":[true,false]},"used_scoped_memory_configs":[]}
}
%bitcast_fusion (bitcast_input: bf16[1024,4096]) -&gt; bf16[1024,4096] {
  %bitcast_input = bf16[1024,4096]{1,0:T(8,128)(2,1)} parameter(0)
  ROOT %bitcast.2 = bf16[1024,4096]{1,0:T(8,128)(2,1)} bitcast(%bitcast_input)
}
%bitcast_fusion.1 (bitcast_input.1: bf16[4096,4096]) -&gt; bf16[4096,4096] {
  %bitcast_input.1 = bf16[4096,4096]{1,0:T(8,128)(2,1)S(3)} parameter(0)
  ROOT %bitcast.3 = bf16[4096,4096]{1,0:T(8,128)(2,1)} bitcast(%bitcast_input.1)
}
%fused_computation (param_0: bf16[1024,4096], param_1: bf16[4096,4096]) -&gt; bf16[1024,4096] {
  %param_0 = bf16[1024,4096]{1,0:T(8,128)(2,1)} parameter(0)
  %fusion.2 = bf16[1024,4096]{1,0:T(8,128)(2,1)} fusion(%param_0), kind=kLoop, calls=%bitcast_fusion
  %param_1 = bf16[4096,4096]{1,0:T(8,128)(2,1)S(3)} parameter(1)
  %fusion.3 = bf16[4096,4096]{1,0:T(8,128)(2,1)} fusion(%param_1), kind=kLoop, calls=%bitcast_fusion.1
  ROOT %convolution.1 = bf16[1024,4096]{1,0:T(8,128)(2,1)S(3)} convolution(%fusion.2, %fusion.3), dim_labels=bf_io-&gt;bf, metadata={op_name="jit(dot_custom)/dot_general" source_file="/tmp/ipykernel_874047/871715024.py" source_line=3}
}
ENTRY %main.7_spmd (param: bf16[1024,4096], param.1: bf16[4096,4096]) -&gt; bf16[1024,2048] {
  %param.1 = bf16[4096,4096]{1,0:T(8,128)(2,1)} parameter(1), sharding={devices=[2,1,2]&lt;=[2,2]T(1,0) last_tile_dim_replicate}, metadata={op_name="y"}
  %copy-start = (bf16[4096,4096]{1,0:T(8,128)(2,1)S(3)}, bf16[4096,4096]{1,0:T(8,128)(2,1)}, u32[]{:S(2)}) copy-start(%param.1), cross_program_prefetch_index=0
  %param = bf16[1024,4096]{1,0:T(8,128)(2,1)} parameter(0), sharding={devices=[2,2]&lt;=[4]}, metadata={op_name="x"}
  %copy-done = bf16[4096,4096]{1,0:T(8,128)(2,1)S(3)} copy-done(%copy-start)
  %fusion.1 = bf16[1024,4096]{1,0:T(8,128)(2,1)S(3)} fusion(%param, %copy-done), kind=kOutput, calls=%fused_computation, metadata={op_name="jit(dot_custom)/dot_general" source_file="/tmp/ipykernel_874047/871715024.py" source_line=3}, backend_config={"flag_configs":[],"window_config":{"kernel_window_bounds":["512","4"],"output_window_bounds":["32","4"],"input_window_bounds":["32","32"],"estimated_cycles":"144544","iteration_bounds":["8","4","1"]},"megacore_config":{"megacore_split_dim":"1"},"scoped_memory_configs":[],"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"5591040"}],"retry_config":{"retry_count":"0"},"convolution_algorithm_config":{"emitter":"EmitAllBatchInSublanes"}}
  ROOT %fusion = bf16[1024,2048]{1,0:T(8,128)(2,1)} fusion(%fusion.1), kind=kCustom, calls=%all-reduce-scatter, metadata={op_name="jit(dot_custom)/dot_general" source_file="/tmp/ipykernel_874047/871715024.py" source_line=3}, backend_config={"flag_configs":[],"scoped_memory_configs":[{"memory_space":"0","offset":"0","size":"67108864"}],"collective_algorithm_config":{"emitter":"SingleInputAllReduceScatterFusion","strategy":"StrategyRing","debug":"\nStrategyRing{colors:1 phases:1 cores:{2} nophase0:0 reserved_sflags:0 cross_module_on_2d_plane:0 has_reordering_map:0 use_routing_table_indices:0}\nStrategyRing{colors:1 phases:1 cores:{2} nophase0:0 reserved_sflags:0 cross_module_on_2d_plane:0 has_reordering_map:0 use_routing_table_indices:0}\nType: 1D phase_count: 1; color_count: 1; sharded_partitions: 4; original_shape: bf16[1024,4096]{1,0:T(8,128)(2,1)S(3)}; per_color_shard_counts: 2; color_dim: -1; sharding_dim: 1; sharding_type: minor; convert_all_gather_output_to_bf16: 0; formatting steps: ()\nall-reduce-scatter fusion ND: span_size:8192*k512Byte, shard count:2, span_count:2, total_size:16384*k512Byte, valid_granules:16384*k512Byte"},"used_scoped_memory_configs":[{"memory_space":"1","offset":"0","size":"12582912"}],"retry_config":{"retry_count":"0"}}
}
</code></pre></div></div>

<p>I actually haven’t found a way to generate a ReduceScatter operation in the
HLO—if you know how to do this, please let me know! Above, I’ve picked
different shardings and arrays, and we end up triggering an operation similar in
effect to a ReduceScatter; but the HLO implementation instead uses <code class="language-plaintext highlighter-rouge">all-reduce</code>
and <code class="language-plaintext highlighter-rouge">dynamic-slice</code> to split up the reduced array.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This turned out to be on the longer side! But we have two valuable takeaways:</p>
<ol>
  <li>A linear-algebraic mental model for sharding in JAX—sharded dimensions
partition data, and unsharded dimensions lead to replication in the
underlying mesh! All of this can be understood in terms of block matrices and
their algebra.</li>
  <li>The ability to read HLO and parse the communications primitives that arise
therein when we manipulate sharded arrays.</li>
</ol>

<p>We also recapped information from the TPU Scaling book about how to
interpret performance tradeoffs between these different forms of communication.</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to the <a href="https://sites.research.google/trc/about/">TRC program</a> for
compute support.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>One way to generalize this to simultaneously considering different meshes
is to consider that the mesh simply indexes different physical devices axes,
or ‘merges’ of them. Hence if there are $M$ physical device axes (e.g.,
devices with 3D interconnects), one can count shardings across meshes by
allowing the $k$ selected axes to be merged together. This is
a noncommutative merging, as the traversal order for the devices will be
different based on the ordering of the merged axes. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>To go beyond this restriction, it suffices to think of the mapping between
block indices and devices as being performed by a pre-configured mesh.
Algebraically, the mesh axis corresponds to a sort of ‘block permutation
matrix’ (i.e., a tensor product of a permutation matrix with the identity),
which left-multiplies for sharding the first dimension of the matrix, and
right-multiplies for the second. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Note that, algebraically, if we perform copying to represent a partitioned
matrix in terms of devices, then <em>elementwise</em> matrix multiplication
corresponds to the within-device matrix products!) <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>E.g., about 42 GB/s for <code class="language-plaintext highlighter-rouge">v4</code> TPUs. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Since AllGather is linear, its derivative doesn’t involve any cached
activations from the forward pass, so we omit these from the notation. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>But the translator is open source, and the admissible operations can be
gleaned from the source code (use Google’s LLM for help with this!).
A high-level specification of operations can be found <a href="https://openxla.org/xla/operation_semantics">here</a>. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Some high-level takeaways about these operations: think of <code class="language-plaintext highlighter-rouge">bitcast</code> like
a view of the array, at least in this context, whereas <code class="language-plaintext highlighter-rouge">copy</code> doesn’t change
the underlying tensor, but does change the memory layout. (Notice that the
<code class="language-plaintext highlighter-rouge">copy</code> operation involves a transposition of the layout axes.) <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  
    <h2>References</h2>
    
      <ol class="bibliography"><li><div class="bibcompact" id="jax-explicit-sharding">
  <span class="author">JAX Team</span>
  <span class="year">(2025).</span>
  <span class="title"><em>Explicit sharding (a.k.a. “sharding in types”)</em>.</span>
  <span class="publication">
    
      Retrieved from <a href="https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html">https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html</a>. (Accessed 2025-08-26)
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="jax_sharded_computation">
  <span class="author">JAX Team</span>
  <span class="year">(2025).</span>
  <span class="title"><em>Introduction to parallel programming</em>.</span>
  <span class="publication">
    
      Retrieved from <a href="https://docs.jax.dev/en/latest/sharded-computation.html">https://docs.jax.dev/en/latest/sharded-computation.html</a>. (Accessed 2025-08-20)
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="scaling-book">
  <span class="author">Austin, J., Douglas, S., Frostig, R., Levskaya, A., Chen, C., Vikram, S., Lebron, F., Choy, P., Ramasesh, V., Webson, A., & Pope, R.</span>
  <span class="year">(2025).</span>
  <span class="title"><em>How to Scale Your Model</em>.</span>
  <span class="publication">
    
      
        Retrieved from <a href="https://jax-ml.github.io/scaling-book/">https://jax-ml.github.io/scaling-book/</a>
      
    
  </span>
</div>
</li></ol>
    
  

  

</article>


      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      


<div id="icon-container">
  <div>
    <a href=mailto:sam@ttic.edu style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" />
    </a>
  </div>
  <div>
    <a href=https://scholar.google.com/citations?user=5WT38A0AAAAJ style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" />
    </a>
  </div>

  <div>
    <a href=https://twitter.com/_sdbuchanan
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" />
    </a>
  </div>

  <div>
    <a href=https://www.linkedin.com/in/sam-buchanan-4507a6b3
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/linkedin.svg" alt="LinkedIn icon" />
    </a>
  </div>

  <div>
    <a href=https://github.com/sdbuch
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" />
    </a>
  </div>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Littlefoot for footnotes and citations -->
<script src="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.css">

<script>
document.addEventListener('DOMContentLoaded', function() {
  // Initialize Littlefoot for standard footnotes
  littlefoot.littlefoot({
    buttonTemplate: '<button aria-expanded="false" class="littlefoot__button" id="<% id %>" title="See Footnote"><% number %></button>',
    activateDelay: 100,
    activateCallback: function(popover, button) {
      // Render MathJax in the footnote popup
      if (window.MathJax && window.MathJax.typesetPromise) {
        window.MathJax.typesetPromise([popover]).catch(function (err) {
          console.log('MathJax typeset failed: ' + err.message);
        });
      }
    },
    allowMultiple: true,
    dismissDelay: 500,
    hoverDelay: 250,
    numberResetSelector: 'article',
    scope: 'body'
  });

  // Custom handler for jekyll-scholar citations
  const citeLinks = document.querySelectorAll('a[href*="#"]');
  citeLinks.forEach(function(link) {
    // Check if this is a citation link (points to bibliography)
    const href = link.getAttribute('href');
    if (href && href.startsWith('#') && href.match(/^#[A-Za-z]/)) {
      const targetId = href.substring(1);
      const targetElement = document.getElementById(targetId);
      
      // If target exists and looks like a bibliography entry
      if (targetElement && (targetElement.closest('.bibliography') || targetElement.tagName === 'LI')) {
        link.addEventListener('mouseenter', function(e) {
          showCitationPopup(e, targetElement);
        });
        
        link.addEventListener('mouseleave', function() {
          hideCitationPopup();
        });
        
        // Prevent default click behavior for hover-only interaction
        link.addEventListener('click', function(e) {
          e.preventDefault();
        });
      }
    }
  });
});

let citationPopup = null;
let hideTimeout = null;
let showTimeout = null;

function showCitationPopup(event, targetElement) {
  // Clear any pending show/hide operations
  if (hideTimeout) {
    clearTimeout(hideTimeout);
    hideTimeout = null;
  }
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Immediately hide any existing popup
  if (citationPopup && citationPopup.parentNode) {
    citationPopup.parentNode.removeChild(citationPopup);
    citationPopup = null;
  }
  
  // Show new popup after a short delay to prevent flicker
  showTimeout = setTimeout(function() {
    // Create popup
    citationPopup = document.createElement('div');
    citationPopup.className = 'littlefoot__popover citation-popup';
    citationPopup.innerHTML = '<div class="littlefoot__content citation-content">' + targetElement.innerHTML + '</div>';
    
    // Position popup
    const rect = event.target.getBoundingClientRect();
    citationPopup.style.position = 'absolute';
    citationPopup.style.top = (rect.bottom + window.scrollY + 5) + 'px';
    citationPopup.style.left = rect.left + 'px';
    citationPopup.style.zIndex = '1000';
    citationPopup.style.maxWidth = '400px';
    
    document.body.appendChild(citationPopup);
    
    // Render MathJax in the popup
    if (window.MathJax && window.MathJax.typesetPromise) {
      window.MathJax.typesetPromise([citationPopup]).catch(function (err) {
        console.log('MathJax typeset failed: ' + err.message);
      });
    }
    
    // Add hover handlers to keep popup open
    citationPopup.addEventListener('mouseenter', function() {
      if (hideTimeout) {
        clearTimeout(hideTimeout);
        hideTimeout = null;
      }
    });
    
    citationPopup.addEventListener('mouseleave', function() {
      hideCitationPopup();
    });
    
    showTimeout = null;
  }, 50);
}

function hideCitationPopup() {
  // Clear any pending show operation
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Clear any existing hide timeout
  if (hideTimeout) {
    clearTimeout(hideTimeout);
  }
  
  hideTimeout = setTimeout(function() {
    if (citationPopup && citationPopup.parentNode) {
      citationPopup.parentNode.removeChild(citationPopup);
      citationPopup = null;
    }
    hideTimeout = null;
  }, 100);
}
</script>


</html>
