<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>SPMD in JAX #2: Transformers in Bare-Metal JAX</title>
  <meta name="description" content="In this second post in a series on programming with JAX for training models like transformers, we write and train a transformer with a decent set of bells and whistles, then benchmark and scale it as much as we can on our TPU v4 half-cube!">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://sdbuchanan.com/blog/jax-2/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Sam D. Buchanan" href="http://sdbuchanan.com/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js"></script>

<!-- MathJax -->
<script type="text/javascript">
  const macros = {};

  // Generate BB letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`bb${letter}`] = `{\\mathbb{${letter}}}`;
  }

  // Generate script letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`c${letter}`] = `{\\mathscr{${letter}}}`;
  }

  // Generate calligraphic letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`s${letter}`] = `{\\mathcal{${letter}}}`;
  }

  // Generate vector uppercase letters
  for (let i = 65; i <= 90; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Generate vector lowercase letters
  for (let i = 97; i <= 122; i++) {
    const letter = String.fromCharCode(i);
    macros[`v${letter}`] = `{\\boldsymbol{${letter}}}`;
  }

  // Add Greek letters
  const greekLetters = [
    'alpha', 'beta', 'gamma', 'delta', 'epsilon', 'varepsilon', 'zeta', 'eta',
    'theta', 'vartheta', 'iota', 'kappa', 'lambda', 'mu', 'nu', 'xi',
    'pi', 'varpi', 'rho', 'varrho', 'sigma', 'varsigma', 'tau', 'upsilon',
    'phi', 'varphi', 'chi', 'psi', 'omega', 'Gamma', 'Delta', 'Theta',
    'Lambda', 'Xi', 'Pi', 'Sigma', 'varSigma', 'Upsilon', 'Phi', 'Psi',
    'Omega', 'ell'
  ];
  greekLetters.forEach(letter => {
    macros[`v${letter}`] = `{\\boldsymbol{\\${letter}}}`;
  });

  // Add these to your macros object
  const additionalMacros = {
    // Basic macros
    "Beta": "{\\mathrm{B}}",
    "eps": "{\\epsilon}",
    "Diff": "{\\mathop{}\\!\\mathrm{D}}",
    "diff": "{\\mathop{}\\!\\mathrm{d}}",
    "Partial": ["{\\frac{\\partial #1}{\\partial #2}}", 2],
    "PartialN": ["{\\frac{\\partial^{#3} #1}{\\partial {#2}^{#3}}}", 3],
    "dPartial": ["{\\dfrac{\\partial #1}{\\partial #2}}", 2],
    "PPartial": ["{\\tfrac{\\partial}{\\partial #1}}", 1],
    "dac": "{\\left.\\frac{\\partial}{\\partial t}\\right|_{t=0}}",
    "half": "{\\tfrac{1}{2}}",
    "third": "{\\tfrac{1}{3}}",
    "fourth": "{\\tfrac{1}{4}}",
    "sixth": "{\\tfrac{1}{6}}", // Fixed typo (was 1/4)
    "eighth": "{\\tfrac{1}{8}}",

    // Accents and decorations
    "overbar": ["{\\mkern 1.5mu\\overline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "underbar": ["{\\mkern 1.5mu\\underline{\\mkern-1.5mu#1\\mkern-1.5mu}\\mkern 1.5mu}", 1],
    "conj": ["{\\overbar{#1}}", 1],
    "wh": "{\\widehat}",
    "wt": "{\\widetilde}",
    "ol": ["{\\overbar{#1}}", 1],
    "kron": "{\\otimes}",
    "elwise": "{\\odot}", // Using \odot as circleddot is not standard
    "dsum": "{\\oplus}",
    "spcdot": "{\\,\\cdot\\,}",

    // Special symbols
    "iu": "{\\mathfrak{i}}",
    "given": [' \\, \\vert \\, ', 0],
    "llangle": ['\\langle\\!\\langle', 0],
    "rrangle": ['\\rangle\\!\\rangle', 0],

      // Analysis operators
    "Lip": "{\\mathrm{Lip}}",
    "mem": "{\\mathrm{mem}}",
    "softmax": "{\\operatorname{\\mathrm{softmax}}}",
    "equid": "{\\overset{d}{=}}",
    "xor": "{\\oplus}",
    "bigxor": "{\\bigoplus}",
    "minimize": ["{\\underset{#1}{\\operatorname{minimize}}}", 1],
    "maximize": ["{\\underset{#1}{\\operatorname{maximize}}}", 1],
    "argmin": "{\\mathop{\\mathrm{arg\\,min}}}",
    "argmax": "{\\mathop{\\mathrm{arg\\,max}}}",
    "orth": "{\\operatorname{orth}}",
    "ow": "{\\mathrm{otherwise}}",
    "iid": "{\\mathrm{i.i.d.}}",
    "wass": "{\\mathrm{W}}",
    "TV": "{\\mathrm{TV}}",
    "as": "{\\mathrm{a.s.}}",
    "whp": "{\\mathrm{w.h.p.}}",
    "simiid": "{\\sim_{\\iid}}",
    "ltsim": "{\\lesssim}",
    "gtsim": "{\\gtrsim}",
    "ltsimwhp": "{\\underset{\\whp}{\\lesssim}}",
    "gtsimwhp": "{\\underset{\\whp}{\\gtrsim}}",
    "psdgeq": "{\\succcurlyeq}",
    "psdleq": "{\\preccurlyeq}",
    "defn": "{\\overset{\\text{def}}{=}}",
    "normsubdiff": ["{\\partial\\norm{}_{#1}}", 1],
    "normalize": ["{\\frac{#1}{\\norm*{#1}_2}}", 1],
    "normalizeabs": ["{\\frac{#1}{\\abs*{#1}}}", 1],
    "iter": ["{#1^{(#2)}}", 2],
    "prox": ["{\\operatorname{prox}}_{#1}", 1],

    // Vector operators
    "tr": "{\\operatorname{tr}}",
    "diag": "{\\operatorname{diag}}",
    "Diag": "{\\operatorname{Diag}}",
    "vect": "{\\operatorname{vec}}",
    "vec": "{\\operatorname{vec}}",
    "Sym": "{\\operatorname{sym}}",
    "Symm": "{\\operatorname{sym}}",
    "Skew": "{\\operatorname{skew}}",
    "rank": "{\\operatorname{rank}}",
    "krank": "{\\operatorname{krank}}",
    "sign": "{\\operatorname{sign}}",
    "sgn": "{\\operatorname{sgn}}",
    "supp": "{\\operatorname{supp}}",
    "esssup": "{\\mathop{\\operatorname{ess\\,sup}}}",
    "vol": "{\\operatorname{vol}}",
    "Vol": "{\\operatorname{Vol}}",
    "tp": "{^{\\mathrm{T}}}",
    "adj": "{^{\\ast}}",
    "inv": "{^{-1}}",
    "One": "{\\mathbf{1}}",
    "Zero": "{\\mathbf{0}}",
    "Id": "{\\operatorname{\\mathrm{Id}}}",
    "conv": "{\\mathbin{\\ast}}",
    "iconv": "{\\mathbin{\\square}}",
    "xcorr": "{\\mathbin{\\star}}",
    "cconv": "{\\mathbin{\\circledast}}",
    "frob": "{\\mathrm{F}}",
    "HS": "{\\mathrm{HS}}",

    // Trig stuff
    "acos": "{\\operatorname{\\cos\\inv}}",
    "asin": "{\\operatorname{\\sin\\inv}}",
    "atan": "{\\operatorname{\\tan\\inv}}",
    "sech": "{\\operatorname{sech}}",
    "csch": "{\\operatorname{csch}}",

    // Calculus/geometry operators
    "Hess": "{\\operatorname{Hess}}",
    "grad": "{\\operatorname{grad}}",
    "Div": "{\\operatorname{div}}",
    "curl": "{\\operatorname{curl}}",
    "downto": "{\\searrow}",
    "upto": "{\\nearrow}",

    // CS operators
    "polylog": "{\\operatorname{polylog}}",
    "poly": "{\\operatorname{poly}}",

    // Stats operators
    "Ind": ["{\\mathds{1}}_{#1}", 1],
    "stddev": "{\\operatorname{stddev}}",
    "Unif": ["{\\operatorname{Unif}(#1)}", 1],
    "Bern": ["{\\operatorname{Bern}(#1)}", 1],
    "Pois": ["{\\operatorname{Pois}(#1)}", 1],
    "Binom": ["{\\operatorname{Binom}(#1, #2)}", 2],
    "Exp": "{\\operatorname{Exp}}",
    "BG": "{\\operatorname{BG}}",
    "Law": "{\\mathrm{Law}}",
    "indep": "{\\protect\\mathpalette{\\protect\\independenT}{\\perp}}",
    "independenT": ["{\\mathrel{\\rlap{$#1#2$}\\mkern2mu{#1#2}}}", 2],

    // Topology / set operators
    "compl": "{\\mathsf{c}}",
    "bd": "{\\operatorname{bd}}",
    "relbd": "{\\operatorname{relbd}}",
    "cl": "{\\operatorname{cl}}",
    "Conv": "{\\operatorname{conv}}",
    "dom": "{\\operatorname{dom}}",
    "epi": "{\\operatorname{epi}}",
    "aff": "{\\operatorname{aff}}",
    "cone": "{\\operatorname{cone}}",
    "ri": "{\\operatorname{ri}}",
    "im": "{\\operatorname{im}}",
    "Hom": "{\\operatorname{Hom}}",
    "End": "{\\operatorname{End}}",
    "Aut": "{\\operatorname{Aut}}",
    "Null": "{\\operatorname{null}}",
    "Span": "{\\operatorname{span}}",
    "row": "{\\operatorname{row}}",
    "col": "{\\operatorname{col}}",
    "range": "{\\operatorname{range}}",
    "Ran": "{\\operatorname{ran}}",
    "diam": "{\\operatorname{diam}}",
    "len": "{\\operatorname{len}}",
    "dist": "{\\operatorname{dist}}",
    "nnz": "{\\operatorname{nnz}}",
    "RE": "{\\operatorname{RE}}",
    "err": "{\\operatorname{err}}",
    "circulant": "{\\operatorname{circ}}",
    "tre": "{\\operatorname{tre}}",
    "etr": "{\\operatorname{etr}}",
    "proj": ["{\\operatorname{proj}_{#1}}", 1]
  };

  const delimitersToDefine = [
    // Simple paired delimiters
    "\\DeclarePairedDelimitersX{\\abs[1]}{\\lvert}{\\rvert}{ \ifblank{#1}{\:\cdot\:}{#1} }",
  ];

  const all_macros = { ...macros, ...additionalMacros };

  window.MathJax = {
    loader: {
      load: ['[tex]/boldsymbol', '[tex]/mathtools', '[tex]/ams', '[tex]/color']
    },
    tex: {
      packages: {'[+]': ['boldsymbol', 'mathtools', 'ams', 'color']},
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true,
      tags: 'ams',
      macros: all_macros,
      mathtools: {
        pairedDelimiters: {
          // General pairing commands
          abs: ['\\lvert', '\\rvert', '{#1}', 1, '', ''],
          norm: ['\\lVert', '\\rVert', '{#1}', 1, '', ''],
          nnorm: ['\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '\\vert\\kern-0.25ex\\vert\\kern-0.25ex\\vert', '{#1}', 1, '', ''],
          ip: ['\\langle', '\\rangle', '{#1}, {#2}', 2, '', ''],
          iip: ['\\llangle', '\\rrangle', '{#1}, {#2}', 2, '', ''],
          ceil: ['\\lceil', '\\rceil', '{#1}', 1, '', ''],
          floor: ['\\lfloor', '\\rfloor', '{#1}', 1, '', ''],
          KL: ['(', ')', '{#1} \\:\\|\\: {#2}', 2, '\\mathop{\\mathrm{KL}}', ''],

          // Set type commands
          set: ['\\{', '\\}', '{#1}', 1, '', ''],
          Pr: ['[', ']', '{#1}', 1, '\\mathbb{P}', ''],
          Prsub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{P}}_{#1}', ''],
          E: ['[', ']', '{#1}', 1, '\\mathbb{E}', ''],
          Esub: ['[', ']', '{#2}', 2, '\\mathop{\\mathbb{E}}_{#1}', ''],
          Var: ['[', ']', '{#1}', 1, '\\mathrm{Var}', ''],
          cov: ['[', ']', '{#1}', 1, '\\mathrm{cov}', ''],
          Ent: ['[', ']', '{#2}', 2, '\\mathop{\\mathrm{Ent}}_{#1}', '']
        }
      }
    }
  };
</script>
<!-- <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script> -->
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>


  
  <meta property="og:title" content="SPMD in JAX #2: Transformers in Bare-Metal JAX">
  <meta property="og:site_name" content="Sam D. Buchanan">
  <meta property="og:url" content="http://sdbuchanan.com/blog/jax-2/">
  <meta property="og:description" content="In this second post in a series on programming with JAX for training models like transformers, we write and train a transformer with a decent set of bells and whistles, then benchmark and scale it as much as we can on our TPU v4 half-cube!">
  
  
    <meta property="og:image" content="/assets/sam-2023.jpeg">
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="SPMD in JAX #2: Transformers in Bare-Metal JAX">
  <meta name="twitter:description" content="In this second post in a series on programming with JAX for training models like transformers, we write and train a transformer with a decent set of bells and whistles, then benchmark and scale it ...">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Sam D. Buchanan</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Home</a>
      
        
        <a class="page-link" href="/publications/">Publications</a>
      
        
        <a class="page-link" href="/blog/">Blog</a>
      
        
        <a class="page-link" href="/misc/">Misc.</a>
      
        
        <a class="page-link" href="/assets/cv.pdf">CV</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">SPMD in JAX #2: Transformers in Bare-Metal JAX</h1>
    
    <p class="post-meta"><time datetime="2025-08-29T00:00:00-07:00" itemprop="datePublished">Aug 29, 2025</time> •
  
    
    
      
        <a href="/categories/jax/">jax</a>,
      
    
      
    
  
    
    
      
    
      
        <a href="/categories/transformers/">transformers</a>
      
    
  



</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In this second post in a series on programming with JAX for training
models like transformers, we write and train a transformer with a decent set of
bells and whistles, then benchmark and scale it as much as we can on our TPU v4
half-cube!</p>

<p>As in previous posts in the series, we make good use of the Google scaling
playbook <a class="citation" href="#scaling-book">(Austin et al., 2025)</a>, as well as some useful JAX tutorials, especially <a class="citation" href="#jax-training-cookbook">(JAX Team, 2025)</a>.</p>

<ul id="markdown-toc">
  <li><a href="#setup" id="markdown-toc-setup">Setup</a></li>
  <li><a href="#design-choices" id="markdown-toc-design-choices">Design Choices</a>    <ul>
      <li><a href="#model-architecture-overview" id="markdown-toc-model-architecture-overview">Model Architecture Overview</a></li>
    </ul>
  </li>
  <li><a href="#building-the-model-first-cut" id="markdown-toc-building-the-model-first-cut">Building the Model: First Cut</a>    <ul>
      <li><a href="#a-quick-note-on-configuration" id="markdown-toc-a-quick-note-on-configuration">A Quick Note on Configuration</a></li>
      <li><a href="#brief-setup" id="markdown-toc-brief-setup">Brief Setup</a></li>
      <li><a href="#data" id="markdown-toc-data">Data</a></li>
      <li><a href="#defining-the-model-mlp" id="markdown-toc-defining-the-model-mlp">Defining the Model: MLP</a></li>
      <li><a href="#defining-the-model-embeddings-and-placeholders" id="markdown-toc-defining-the-model-embeddings-and-placeholders">Defining the Model: Embeddings and Placeholders</a></li>
      <li><a href="#defining-the-model-building-blocks-to-transformer" id="markdown-toc-defining-the-model-building-blocks-to-transformer">Defining the Model: Building Blocks to Transformer</a></li>
      <li><a href="#defining-the-model-initialization" id="markdown-toc-defining-the-model-initialization">Defining the Model: Initialization</a></li>
      <li><a href="#testing-the-basic-model" id="markdown-toc-testing-the-basic-model">Testing the Basic Model</a></li>
      <li><a href="#a-minimal-training-loop" id="markdown-toc-a-minimal-training-loop">A Minimal Training Loop</a></li>
    </ul>
  </li>
  <li><a href="#sampling-from-the-model-first-cut" id="markdown-toc-sampling-from-the-model-first-cut">Sampling from the Model: First Cut</a></li>
  <li><a href="#building-the-model-attention-and-positional-encodings" id="markdown-toc-building-the-model-attention-and-positional-encodings">Building the Model: Attention and Positional Encodings</a></li>
  <li><a href="#building-the-model-sampling-with-a-cache" id="markdown-toc-building-the-model-sampling-with-a-cache">Building the Model: Sampling with a Cache</a></li>
  <li><a href="#conclusions" id="markdown-toc-conclusions">Conclusions</a></li>
  <li><a href="#accumulated-configs" id="markdown-toc-accumulated-configs">Accumulated Configs</a></li>
  <li><a href="#acknowledgments" id="markdown-toc-acknowledgments">Acknowledgments</a></li>
</ul>

<h3 id="setup">Setup</h3>

<p>As usual, we’ll test things out in Python 3.13 and JAX 0.7.1 on a single TPU v4
host.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">()</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),
 TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0),
 TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0),
 TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
</code></pre></div></div>

<h1 id="design-choices">Design Choices</h1>

<p>A project like this is for learning and pedagogy—it’s inevitably behind the
research/infra frontiers. So the code we write is going to lean into this
somewhat. Notably,</p>

<blockquote>
  <p><em>We’ll implement everything in <strong>bare-metal JAX</strong>, rather than using
neural network libraries like Flax/nnx/Equinox.</em></p>
</blockquote>

<p>This will let us expose and
understand all the implementation details, both for infra and architecture, that
we might normally overlook. On the flip side, we’ll take some performance hits
for this (which we’ll attempt to profile and characterize), and it will lead to some
slightly unfortunate code repetition.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> We’ll also be reinventing the wheel
throughout – but for pedagogy’s sake!</p>

<h2 id="model-architecture-overview">Model Architecture Overview</h2>

<p>Our approach to implementing the network architecture is a modification of the approach
used in the JAX training cookbook <a class="citation" href="#jax-training-cookbook">(JAX Team, 2025)</a>, which uses a data
structure like those in Google’s <a href="https://github.com/google/ml_collections">ML
Collections</a> library to store the model. At a
high level, this approach consists of these standard JAX neural net patterns:</p>
<ul>
  <li>Storing the model parameters as a <a href="https://docs.jax.dev/en/latest/pytrees.html">pytree</a>.</li>
  <li>Defining separate functions for initialization and the model’s forward pass which take
such a pytree as input (as well as other relevant inputs).
This pattern promotes functionally-pure code for the model’s initialization and forward pass,
making it easy to <code class="language-plaintext highlighter-rouge">jit</code> for performance.</li>
</ul>

<p>Our implementation tries to move this slightly in the direction of
<a href="https://docs.kidger.site/equinox/">Equinox</a>, without all the powerful features that
library entails. In particular, we will:</p>
<ul>
  <li>Use
<a href="https://docs.python.org/3/library/collections.html#collections.namedtuple">NamedTuples</a>
for parameters and layers (i.e., where you’d normally expect to use a <code class="language-plaintext highlighter-rouge">nn.Module</code> in
Pytorch), enabling good static type checking with e.g. Pyright.
NamedTuples are <a href="https://docs.jax.dev/en/latest/pytrees.html#internal-pytree-handling">automatically registered as
pytrees</a> in JAX!</li>
  <li>Write a top-level function for model initialization, and layer-level functions for
each separate layer’s operation (which, as above, take parameters and inputs as
arguments). This is a ‘bottom-up’ version of the previous design, where we’ll end up
with many different functions, one for each ‘layer’, which are combined bottom-up to
implement the overall model’s forward pass (more like in a typical neural network
library).</li>
</ul>

<h1 id="building-the-model-first-cut">Building the Model: First Cut</h1>

<p>When writing this post, I wrote and debugged the initial model/training code iteratively
in a notebook. Rather than just jumping to the ‘final answer’, we’ll walk through the
development process below.</p>

<p>In this part of the post, we’ll build up to the transformer through a few simple
sub-modules of the overall model. Our targets will be:</p>
<ul>
  <li>We’ll work with a very simple data distribution, described below, which requires
minimal dataloading code (and no tokenization).</li>
  <li>We’ll focus on the training loss, leaving sampling for later. Although simple sampling
gives us an important correctness test (“did we accidentally give the model access to
future tokens?”, for example…), doing it ‘fast’ requires a bit of code, so we’ll
save it for later.</li>
</ul>

<p>On the way to these targets, we’ll build up the following:</p>
<ul>
  <li>Just the MLP, giving us a position-wise model (bigrams with a nonlinearity!).</li>
  <li>A training loop with SGD and cross-entropy (next token prediction) loss.</li>
  <li>Just causal attention, letting us build a transformer that can solve our toy task
(discussed in the next section) with two layers.</li>
  <li>Rotary positional encodings, so that we can solve our task with a one-layer model.</li>
</ul>

<p>We’ll then step back and refactor things with an eye towards training larger models on
actual language data in a distributed setting.</p>

<p>If you’re following along and you notice anything I’ve implemented that seems
suboptimal, please let me know! Drop me an email or a DM on Twitter (links at the bottom
of the page).</p>

<h2 id="a-quick-note-on-configuration">A Quick Note on Configuration</h2>

<p>For overall convenience, we eventually want to combine all our different configuration
options in a large dataclass, then pass our runtime instantiation of the config (with
defaults, overrides, etc.) to all our functions, which can extract just the parameters
they need.</p>

<p>When writing this code, I tend to define the config options I need locally, then go back
later and turn them into attributes of the overall config dataclass. To avoid
frontloading with all the complexity of the config, and to avoid too much unnecessary
description of the refactoring process, we’ll reference different config options below
as attributes of an (as of yet undefined) Config class. Code outputs will also have this
class defined. You can Ctrl+F for type definitions and defaults, which will appear
later, if it feels necessary.</p>

<h2 id="brief-setup">Brief Setup</h2>

<p>We are going to make reference to different sources of randomness below, which we seed
from our Config class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test it out
</span><span class="n">config</span> <span class="o">=</span> <span class="nc">Config</span><span class="p">()</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">key_params</span><span class="p">,</span> <span class="n">key_data</span><span class="p">,</span> <span class="n">key_sampling</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="data">Data</h2>

<p>The data will consist of an (infinite) sequence of consecutive digits (from 0 to 9),
which ‘reflect’ when they reach 0 or 9. I.e.,</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>01234567898765432101...
</code></pre></div></div>

<p>We’ll generate a dataset of fixed-length subsequences of this infinite sequence for
training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">012345678987654321</span><span class="sh">"</span> <span class="o">*</span> <span class="mi">1024</span>
<span class="n">seqs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">text</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">config</span><span class="p">.</span><span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="o">-</span> <span class="n">config</span><span class="p">.</span><span class="n">seq_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>


<span class="n">key_data</span><span class="p">,</span> <span class="n">sk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_data</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">permutation</span><span class="p">(</span><span class="n">sk</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">128</span><span class="p">])</span>

<span class="n">Xtr</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">num_data</span><span class="p">)]</span>
<span class="n">Xdev</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">num_data</span><span class="p">)</span> <span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">num_data</span><span class="p">)]</span>
<span class="n">Xte</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">num_data</span><span class="p">)</span> <span class="p">:]</span>
<span class="n">Xtr</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
[8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8
 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9
 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8
 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7
 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6
 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5
 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 8 7 6]
(14540, 257)
</code></pre></div></div>

<p>We shuffle the sequences above and perform a simple train-dev-test split (although we’ll
just use train for now). We take one more character than the sequence length, so that we
can easily turn these raw sequences into inputs and targets. We do this with a simple
dataloader:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">itertools</span> <span class="k">as</span> <span class="n">it</span>


<span class="k">def</span> <span class="nf">dataloader</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">it</span><span class="p">.</span><span class="nf">count</span><span class="p">():</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">fold_in</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">global_batch_size</span><span class="p">,),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_data</span><span class="p">)</span>
        <span class="nf">yield </span><span class="p">(</span><span class="n">Xtr</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">offsets</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">get</span><span class="p">(),</span> <span class="n">Xtr</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">offsets</span><span class="p">,</span> <span class="mi">1</span><span class="p">:].</span><span class="nf">get</span><span class="p">())</span>


<span class="n">key_data</span><span class="p">,</span> <span class="n">sk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_data</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">batch</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">sharding_data</span><span class="p">),</span>
    <span class="nf">dataloader</span><span class="p">(</span><span class="n">sk</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Here <code class="language-plaintext highlighter-rouge">batch</code> is a map iterator which wraps the dataloader with a <code class="language-plaintext highlighter-rouge">device_put</code> statement to enforce
our desired sharding (e.g., data parallel). When we create our config class, we create a
global mesh and set it as the default mesh, so we can pass a <code class="language-plaintext highlighter-rouge">PartitionSpec</code> object to
<code class="language-plaintext highlighter-rouge">device_put</code> in our single-host setting (see the <a href="/blog/jax-1">previous post</a>).</p>

<p>The batches being loaded have shape
<code class="language-plaintext highlighter-rouge">(config.global_batch_size, config.seq_len)</code>, so expect <code class="language-plaintext highlighter-rouge">config.sharding_data</code> to be
for example <code class="language-plaintext highlighter-rouge">jax.P('dp')</code> for a “data-parallel” mesh axis <code class="language-plaintext highlighter-rouge">'dp'</code>, which leads the batch
to be sharded across accelerators.</p>

<h2 id="defining-the-model-mlp">Defining the Model: MLP</h2>

<p>We’ll implement the overarching structure of our model here, described above in the
“Design Choices” section, but keep the low-level implementations restricted to just the
MLP.</p>

<p>We’ll follow the following conventions:</p>
<ul>
  <li>“Layers” correspond to <code class="language-plaintext highlighter-rouge">NamedTuples</code> (which are pytrees). The elements of such a
“layer” can be either <code class="language-plaintext highlighter-rouge">jax.Array</code>s (parameters/pytree leaves), or other NamedTuples
(for composing layers).</li>
  <li>We’ll name our layers with capital letters, and the functions that implement them
(i.e., their “forward” method in Pytorch lingo) in lower-case, with a leading
underscore.</li>
</ul>

<p>Here’s our MLP layer following these conventions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span>


<span class="k">class</span> <span class="nc">Mlp</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w_up</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">bias_up</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">w_down</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">bias_down</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>


<span class="k">def</span> <span class="nf">_mlp</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Mlp</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">):</span>
    <span class="n">preact</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">w_up</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_mlp_hidden</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_bias_mlp</span><span class="p">:</span>
        <span class="n">preact</span> <span class="o">+=</span> <span class="n">params</span><span class="p">.</span><span class="n">bias_up</span>
    <span class="n">act</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">gelu</span><span class="p">(</span><span class="n">preact</span><span class="p">,</span> <span class="n">approximate</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">w_down</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_bias_mlp</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">params</span><span class="p">.</span><span class="n">bias_down</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<p>We would create parameters for such a layer and apply it as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_mlp</span> <span class="o">=</span> <span class="nc">Mlp</span><span class="p">(</span>
    <span class="n">w_up</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span>
    <span class="n">bias_up</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,)),</span>
    <span class="n">w_down</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">bias_down</span><span class="o">=</span><span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span>
<span class="p">)</span>
<span class="nf">_mlp</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">my_mlp</span><span class="p">,</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Array([0., 0.], dtype=float32)
</code></pre></div></div>

<p>It’s important to note here that we are <strong>not</strong> able to enforce shape information at
this level of implementation, since abstractly speaking an MLP can be implemented for
any compatible weight matrix dimensions. These shapes get enforced later, when we
initialize an <code class="language-plaintext highlighter-rouge">Mlp</code> with settings specified in our <code class="language-plaintext highlighter-rouge">config</code> instance.</p>

<p>More importantly, there is some ambiguity in the types of the arguments <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">params</code>
that <code class="language-plaintext highlighter-rouge">_mlp</code> will apply to – e.g. the above function could apply to <code class="language-plaintext highlighter-rouge">x</code> being a vector
of shape <code class="language-plaintext highlighter-rouge">(d_model,)</code>, or a matrix of shape <code class="language-plaintext highlighter-rouge">(seq_len, d_model)</code>, as long as the
parameters of <code class="language-plaintext highlighter-rouge">Mlp</code> are initialized correctly. In other words, it’s up to us to make
sure we apply this implementation correctly.</p>

<p>In our subsequent implementation, we’re going to follow a helpful principle that JAX
affords – we’ll operate at the finest level of granularity possible at every stage of
the model, and ‘move up’ to the next level of granularity as necessary, using JAX’s
powerful primitives like <code class="language-plaintext highlighter-rouge">vmap</code> and <code class="language-plaintext highlighter-rouge">scan</code>. In particular, we’ll see soon that we’ll
only use the above <code class="language-plaintext highlighter-rouge">_mlp</code> function in cases where <code class="language-plaintext highlighter-rouge">x</code> is a vector of shape <code class="language-plaintext highlighter-rouge">(d_model,)</code>.
Since the MLPs in transformers operate the same on every position in the sequence,
and every sequence in the batch, we’ll just <code class="language-plaintext highlighter-rouge">vmap</code> our <code class="language-plaintext highlighter-rouge">_mlp</code> function when we need to
use it at the next level of granularity! This is a pretty empowering design strategy
that JAX enables: it lets us write our code’s core numerical functionality without
having to plan ahead for exactly how it will be used later, making implementation
simpler and separating complexity nicely.</p>

<p>A few other notes (some drawbacks of this bare metal approach, which
seem unvaoidable):</p>
<ul>
  <li>There’s no ‘state’ in the <code class="language-plaintext highlighter-rouge">_mlp</code> function – it just performs the operation of a MLP
given parameters/inputs. This makes it easy to <code class="language-plaintext highlighter-rouge">jit</code>.</li>
  <li>Initialization is <strong>not</strong> handled here – we’ll have to do it later,
since we want to initialize the entire transformer (top-down) in one shot.</li>
  <li>We enforce shardings for the hidden activations/output of the MLP in the <code class="language-plaintext highlighter-rouge">_mlp</code>
function. We enforce shardings for the <em>parameters</em> of the MLP when we initialize them
(see later).</li>
</ul>

<h2 id="defining-the-model-embeddings-and-placeholders">Defining the Model: Embeddings and Placeholders</h2>

<p>To have a basic working model, we need token embeddings (mapping the raw integer tokens
we generated above to vectors the transformer can operate on), unembeddings (mapping the
vector outputs of the transformer to logits, which we can map to a probability
distribution over tokens to predict with), and normalization layers. We’ll have
attention layers too, eventually; for now, we’ll make a placeholder function for the
attention layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w_qkv</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">w_o</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>


<span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Attn</span><span class="p">,</span>
    <span class="n">x_seq</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">kv_cache</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">x_seq</span><span class="p">,</span> <span class="n">kv_cache</span>


<span class="k">class</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>


<span class="k">def</span> <span class="nf">_embedding</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">token</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">params</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">token</span><span class="p">].</span><span class="nf">get</span><span class="p">(</span><span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">emb</span>


<span class="k">class</span> <span class="nc">Unembedding</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>


<span class="k">def</span> <span class="nf">_unembedding</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Unembedding</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">w</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>


<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>
    <span class="n">beta</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span>


<span class="k">def</span> <span class="nf">_layernorm</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">LayerNorm</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">):</span>
    <span class="n">x_std</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">standardize</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">),</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">eps_ln</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">params</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">x_std</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_bias_ln</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">+=</span> <span class="n">params</span><span class="p">.</span><span class="n">beta</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<p>These are standard unoptimized implementations for these different building blocks.
Looking ahead slightly, we make sure to provide the ability to compute the layer
normalization operation in a higher precision (<code class="language-plaintext highlighter-rouge">config.compute_dtype</code>), since the
variance operation in <code class="language-plaintext highlighter-rouge">standardize</code> can lead to underflow and other losses of precision.
The API for <code class="language-plaintext highlighter-rouge">_attn</code> involves arguments for utilizing a cache for faster inference (we’d
normally leave this out to begin with, and add it only later).</p>

<h2 id="defining-the-model-building-blocks-to-transformer">Defining the Model: Building Blocks to Transformer</h2>

<p>A transformer consists of an embedding, then a sequence of transformer blocks (which
involve normalization + attention and a residual connection, then normalization + MLP
and a residual connection), then the unembedding.
Accordingly, we make a <code class="language-plaintext highlighter-rouge">Block</code> class which combines our low-level building blocks.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>


<span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">norm_attn</span><span class="p">:</span> <span class="n">LayerNorm</span>
    <span class="n">attn</span><span class="p">:</span> <span class="n">Attn</span>
    <span class="n">norm_mlp</span><span class="p">:</span> <span class="n">LayerNorm</span>
    <span class="n">mlp</span><span class="p">:</span> <span class="n">Mlp</span>


<span class="k">def</span> <span class="nf">_block</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Block</span><span class="p">,</span>
    <span class="n">x_seq</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_in</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">att_skip</span> <span class="o">=</span> <span class="n">x_seq</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_layernorm</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">norm_attn</span><span class="p">))(</span><span class="n">x_seq</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="nf">_attn</span><span class="p">(</span>
        <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">attn</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="n">cache_in</span><span class="p">,</span> <span class="n">cache_size</span><span class="o">=</span><span class="n">cache_size</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">att_skip</span>

    <span class="n">mlp_skip</span> <span class="o">=</span> <span class="n">out</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_layernorm</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">norm_mlp</span><span class="p">))(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_mlp</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">mlp</span><span class="p">))(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">mlp_skip</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache_out</span>
</code></pre></div></div>

<p>Notice that we’re following the design principle we mentioned above with the MLP: here
we expect <code class="language-plaintext highlighter-rouge">x_seq</code> to have two dimensions, the first for the sequence length and the
second for the model dimension, and we <code class="language-plaintext highlighter-rouge">vmap</code> the <code class="language-plaintext highlighter-rouge">_mlp</code> and <code class="language-plaintext highlighter-rouge">_layernorm</code> functions to
apply to it correctly.</p>

<p>We can now define the transformer. We’ll do this in a slightly refined way versus just
using a for loop: we’ll expect the <code class="language-plaintext highlighter-rouge">blocks</code> parameter below, of type <code class="language-plaintext highlighter-rouge">Block</code>, to have a
mapped leading axis corresponding to the <em>layer dimension</em>, and we’ll consume this
leading axis using a <code class="language-plaintext highlighter-rouge">scan</code>. That means we expect every parameter in the <code class="language-plaintext highlighter-rouge">Block</code> pytree
we pass to not just have its usual shape, say <code class="language-plaintext highlighter-rouge">param.shape</code> (expected by the <code class="language-plaintext highlighter-rouge">_mlp</code>,
etc. APIs above), but additionally be of shape <code class="language-plaintext highlighter-rouge">(num_layers,) + param.shape</code>. This turns
out to be very easy to guarantee with a <code class="language-plaintext highlighter-rouge">vmap</code> at initialization (see below).</p>

<p>If you aren’t familiar with <code class="language-plaintext highlighter-rouge">scan</code>, you can read
about the API <a href="https://docs.jax.dev/en/latest/_autosummary/jax.lax.scan.html">here</a>.
It takes a function that accepts two arguments – a ‘carry’ and an ‘input’ – and
produces one output, as well as an ‘initial’ carry, and an initial input, which <strong>must</strong>
have an additional mapped axis somewhere relative to the input shape the function
argument expects. It will then iteratively apply the function to the current carry and
the current input; the output carry is passed to the next call of the function. In this
way, it’s like a vectorized for loop – you can also draw a direct analogy to a
recurrent neural network, or other dynamical systems.</p>

<p>In our case, the ‘carry’ for scan is our network’s input and activations (we pass these
from layer to layer sequentially), and the ‘input’ for scan is the mapped <code class="language-plaintext highlighter-rouge">Block</code> pytree
(as well as the mapped KV cache for inference – the updated cache becomes the output –
but we can ignore that for now).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">emb</span><span class="p">:</span> <span class="n">Embedding</span>
    <span class="n">blocks</span><span class="p">:</span> <span class="n">Block</span>  <span class="c1"># vmapped at init
</span>    <span class="n">unemb</span><span class="p">:</span> <span class="n">Unembedding</span>


<span class="k">def</span> <span class="nf">_transformer</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Transformer</span><span class="p">,</span>
    <span class="n">tokens</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_in</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">x_seq</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_embedding</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">emb</span><span class="p">))(</span><span class="n">tokens</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_block_fun</span><span class="p">(</span><span class="n">x_seq</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">params__cache_in</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Block</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">]):</span>
        <span class="n">params</span><span class="p">,</span> <span class="n">cache_in</span> <span class="o">=</span> <span class="n">params__cache_in</span>
        <span class="k">return</span> <span class="nf">_block</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x_seq</span><span class="p">,</span> <span class="n">cache_in</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">)</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span><span class="n">_block_fun</span><span class="p">,</span> <span class="n">x_seq</span><span class="p">,</span> <span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">blocks</span><span class="p">,</span> <span class="n">cache_in</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_unembedding</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">.</span><span class="n">unemb</span><span class="p">))(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache_out</span>
</code></pre></div></div>

<p>Using <code class="language-plaintext highlighter-rouge">scan</code> leads to a very concise forward pass definition for the transformer. Note
that our transformer is still operating on sequences of embeddings – we’ll apply it to
a batch of sequences with <code class="language-plaintext highlighter-rouge">vmap</code> later!</p>

<h2 id="defining-the-model-initialization">Defining the Model: Initialization</h2>

<p>The last step before testing is to initialize the model. This leads to a long
definition, but the intrinsic complexity is not high.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_model_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transformer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">init_embedding</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Embedding</span><span class="p">:</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
            <span class="n">key</span><span class="p">,</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_vocab</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">Embedding</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="n">emb</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_unembedding</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Unembedding</span><span class="p">:</span>
        <span class="n">unemb</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
            <span class="n">key</span><span class="p">,</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_vocab</span><span class="p">),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">Unembedding</span><span class="p">(</span><span class="n">w</span><span class="o">=</span><span class="n">unemb</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_mlp</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Mlp</span><span class="p">:</span>
        <span class="n">k_w_up</span><span class="p">,</span> <span class="n">k_w_down</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">w_up</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
            <span class="n">k_w_up</span><span class="p">,</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">mlp_factor</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_wup</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">w_down</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
                <span class="n">k_w_down</span><span class="p">,</span>
                <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">mlp_factor</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
                <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
                <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_wdown</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">bias_up</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">mlp_factor</span> <span class="o">*</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_mlp_hidden</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">bias_down</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">Mlp</span><span class="p">(</span><span class="n">w_up</span><span class="o">=</span><span class="n">w_up</span><span class="p">,</span> <span class="n">bias_up</span><span class="o">=</span><span class="n">bias_up</span><span class="p">,</span> <span class="n">w_down</span><span class="o">=</span><span class="n">w_down</span><span class="p">,</span> <span class="n">bias_down</span><span class="o">=</span><span class="n">bias_down</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_attn</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Attn</span><span class="p">:</span>
        <span class="n">k_qkv</span><span class="p">,</span> <span class="n">k_o</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">w_qkv</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
            <span class="n">k_qkv</span><span class="p">,</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_wqkv</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">param_std</span> <span class="o">*</span> <span class="p">(</span>
            <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span>
                <span class="n">k_o</span><span class="p">,</span>
                <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">),</span>
                <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
                <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_wo</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">w_qkv</span><span class="o">=</span><span class="n">w_qkv</span><span class="p">,</span> <span class="n">w_o</span><span class="o">=</span><span class="n">w_out</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_layernorm</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">LayerNorm</span><span class="p">:</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">,),</span>
            <span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_block</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Block</span><span class="p">:</span>
        <span class="n">key_attn</span><span class="p">,</span> <span class="n">key_mlp</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="k">return</span> <span class="nc">Block</span><span class="p">(</span>
            <span class="n">norm_attn</span><span class="o">=</span><span class="nf">init_layernorm</span><span class="p">(),</span>
            <span class="n">attn</span><span class="o">=</span><span class="nf">init_attn</span><span class="p">(</span><span class="n">key_attn</span><span class="p">),</span>
            <span class="n">norm_mlp</span><span class="o">=</span><span class="nf">init_layernorm</span><span class="p">(),</span>
            <span class="n">mlp</span><span class="o">=</span><span class="nf">init_mlp</span><span class="p">(</span><span class="n">key_mlp</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="c1"># Make the full network
</span>    <span class="n">key_emb</span><span class="p">,</span> <span class="n">key_blocks</span><span class="p">,</span> <span class="n">key_unemb</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">keys_blocks</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_blocks</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="nc">Transformer</span><span class="p">(</span>
        <span class="n">emb</span><span class="o">=</span><span class="nf">init_embedding</span><span class="p">(</span><span class="n">key_emb</span><span class="p">),</span>
        <span class="n">blocks</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">init_block</span><span class="p">)(</span><span class="n">keys_blocks</span><span class="p">),</span>
        <span class="n">unemb</span><span class="o">=</span><span class="nf">init_unembedding</span><span class="p">(</span><span class="n">key_unemb</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>Notice above how we produce the mapped axis needed for the <code class="language-plaintext highlighter-rouge">scan</code> in the <code class="language-plaintext highlighter-rouge">_transformer</code>
function above: we just split our PRNGKey into an array of keys, one for each layer, and
<code class="language-plaintext highlighter-rouge">vmap</code> the <code class="language-plaintext highlighter-rouge">init_block</code> function over the array of keys. Easy!</p>

<h2 id="testing-the-basic-model">Testing the Basic Model</h2>

<p>We can perform a quick test to make sure everything above is at least syntactically
correct. There are two things we want to see:</p>
<ol>
  <li>Can we run the model in eager mode?</li>
  <li>Can we <code class="language-plaintext highlighter-rouge">jit</code> the model?</li>
</ol>

<p>We really only care about 2 (it implies 1), so let’s test it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key_params</span><span class="p">,</span> <span class="n">sk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_params</span><span class="p">)</span>
<span class="n">tf</span> <span class="o">=</span> <span class="nf">init_model_params</span><span class="p">(</span><span class="n">sk</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of layers in model: </span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Model dimension: </span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>


<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">tokens</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">_transformer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>


<span class="n">out</span><span class="p">,</span> <span class="n">out_cache</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">tf</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
<span class="n">out</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Number of layers in model:  2
Model dimension:  768
Array([[0.894531, -0.482422, 1.41406, 0.194336, -2.76562, -1.27344,
        1.21094, 0.417969, -0.460938, -0.304688],
       [0.0610352, 2.8125, 1.08594, -0.351562, -0.326172, -1.46094,
        0.0354004, 0.283203, 0.796875, -1.42969],
       [0.186523, 1.21875, 0.429688, -1.10156, 0.667969, -2.07812,
        -0.078125, 0.410156, 0.0273438, -2.60938],
       [0.201172, 1.46094, 0.124512, -1.27344, -1.05469, -0.667969,
        0.453125, 1.78906, -1.00781, 1.67188],
       [-0.925781, 0.988281, -1.14844, 0.0942383, 0.0898438, 1.70312,
        1.125, 1.03125, -1.17969, -1.69531],
       [-0.710938, -0.679688, -1.53125, 0.800781, 0.443359, -0.71875,
        -2.29688, 0.488281, -0.169922, -1.38281],
       [1.44531, 0.949219, -0.589844, 0.357422, -1.30469, 2.85938,
        -0.388672, 0.46875, -0.695312, -0.425781],
       [-1.85156, 0.679688, 1.25, 0.109863, -0.667969, -0.361328,
        -0.0405273, -0.964844, 3.01562, 0.333984],
       [-0.304688, 1.83594, 0.734375, 1.16406, -0.114746, -0.800781,
        0.486328, 1, -1.00781, -1.01562],
       [-1.66406, 0.925781, -0.0908203, 1, 0.298828, -2.40625, 0.574219,
        0.945312, -2.54688, -0.123047]], dtype=bfloat16)
</code></pre></div></div>

<p>Looks good! There might be a small issue with the unembedding scaling, as we can see:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Array([[0.163086, 0.0410156, 0.275391, 0.0810547, 0.00418091, 0.0186768,
        0.224609, 0.101562, 0.0419922, 0.0493164],
       [0.0390625, 0.613281, 0.108887, 0.026001, 0.0264893, 0.00848389,
        0.0380859, 0.0488281, 0.081543, 0.00872803],
       [0.100098, 0.279297, 0.126953, 0.02771, 0.161133, 0.010376,
        0.0766602, 0.124512, 0.0849609, 0.006073],
       [0.0583496, 0.204102, 0.0539551, 0.0133057, 0.0164795, 0.0244141,
        0.0751953, 0.285156, 0.017334, 0.253906],
       [0.0227051, 0.15332, 0.0181885, 0.0629883, 0.0629883, 0.314453,
        0.176758, 0.160156, 0.0177002, 0.010437],
       [0.0588379, 0.060791, 0.026123, 0.267578, 0.1875, 0.0588379,
        0.012146, 0.195312, 0.101562, 0.0300293],
       [0.141602, 0.0864258, 0.0184326, 0.0476074, 0.00909424, 0.582031,
        0.022583, 0.0534668, 0.0164795, 0.0218506],
       [0.00500488, 0.0629883, 0.112305, 0.0358887, 0.0164795, 0.0224609,
        0.0307617, 0.012207, 0.65625, 0.0444336],
       [0.0395508, 0.335938, 0.111328, 0.171875, 0.0473633, 0.0239258,
        0.0869141, 0.145508, 0.0194092, 0.0194092],
       [0.0145874, 0.193359, 0.0693359, 0.208008, 0.102539, 0.00689697,
        0.135742, 0.196289, 0.00598145, 0.0673828]], dtype=bfloat16)
</code></pre></div></div>

<p>In particular, the initial outputs are not very uniform, which we would like to have.
However, we should still be able to learn – our network is not very deep.</p>

<h2 id="a-minimal-training-loop">A Minimal Training Loop</h2>

<p>Now we have enough to train a model. A simple training loop follows, using SGD.
First, we set up the training step, <code class="language-plaintext highlighter-rouge">jit</code>ting it for performance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TrainState</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Transformer</span>
    <span class="n">opt</span><span class="p">:</span> <span class="bp">None</span>


<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">init_train_state</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainState</span><span class="p">:</span>
    <span class="k">return</span> <span class="nc">TrainState</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="nf">init_model_params</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">opt</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>


<span class="n">train_state</span> <span class="o">=</span> <span class="nf">init_train_state</span><span class="p">(</span><span class="n">key_params</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span>


<span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">train_state</span><span class="p">:</span> <span class="n">TrainState</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">Transformer</span><span class="p">):</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="nf">partial</span><span class="p">(</span><span class="n">_transformer</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">))(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">cache</span>
        <span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="n">logprobs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">take_along_axis</span><span class="p">(</span><span class="n">logprobs</span><span class="p">,</span> <span class="n">targets</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">train_state</span><span class="p">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="nc">TrainState</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="n">tree</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">config</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_state</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">),</span>
        <span class="n">opt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">new_state</span>
</code></pre></div></div>

<p>In the training step, after loading a batch of sequences from our previously-written
batch loader, we vmap <code class="language-plaintext highlighter-rouge">_transformer</code> over it, then calculate the next-token prediction
loss with the standard bare-metal approach (with possible up-casting for numerical
stability of the softmax). The <code class="language-plaintext highlighter-rouge">donate_argnums</code> argument to <code class="language-plaintext highlighter-rouge">jit</code> lets us mark that the
<code class="language-plaintext highlighter-rouge">train_state</code> buffer can be discarded after the step finishes (since we create and
return a new <code class="language-plaintext highlighter-rouge">TrainState</code> with the results of the SGD step), letting the <code class="language-plaintext highlighter-rouge">XLA</code> compiler
reuse that memory for the output state.</p>

<p>The loop is then simple to write:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prev_metrics</span> <span class="o">=</span> <span class="bp">None</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">num_steps</span><span class="p">):</span>
    <span class="n">cur_metrics</span><span class="p">,</span> <span class="n">train_state</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nf">next</span><span class="p">(</span><span class="n">batch</span><span class="p">),</span> <span class="n">train_state</span><span class="p">)</span>
    <span class="n">log_metrics</span><span class="p">,</span> <span class="n">prev_metrics</span> <span class="o">=</span> <span class="n">prev_metrics</span><span class="p">,</span> <span class="n">cur_metrics</span>
    <span class="k">if</span> <span class="n">log_metrics</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">log_metrics</span> <span class="o">|=</span> <span class="p">{</span><span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span>
        <span class="nf">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">log_metrics</span><span class="p">.</span><span class="nf">items</span><span class="p">()],</span> <span class="n">sep</span><span class="o">=</span><span class="sh">"</span><span class="se">\t</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
loss: 1.1560431718826294	step: 50
loss: 0.9151328206062317	step: 100
loss: 1.004440188407898	    step: 150
loss: 0.8609314560890198	step: 200
loss: 0.9039498567581177	step: 250
loss: 0.8058883547782898	step: 300
loss: 0.7688642144203186	step: 350
loss: 0.6995882987976074	step: 400
loss: 0.6380590796470642	step: 450
loss: 0.6304700374603271	step: 500
loss: 0.6276624798774719	step: 550
loss: 0.6255137920379639	step: 600
loss: 0.6255109310150146	step: 650
loss: 0.6271587610244751	step: 700
loss: 0.6273760795593262	step: 750
loss: 0.6262620687484741	step: 800
loss: 0.6236293911933899	step: 850
loss: 0.6241434216499329	step: 900
loss: 0.6261416673660278	step: 950
</code></pre></div></div>

<p>It’s not great, but it’s indeed training! We don’t expect to be able to solve this task
perfectly with our simple model, since a position-wise MLP can only use one token of
context to predict the next token, and our task requires at least two. To see why, just
note that, for example, the character following <code class="language-plaintext highlighter-rouge">5</code> could be either <code class="language-plaintext highlighter-rouge">6</code> or <code class="language-plaintext highlighter-rouge">4</code>,
depending on whether we’re at a rising or falling part of the sequence. Two characters
of context is enough to estimate this, meaning adding attention should help here!</p>

<p>As an aside, above, we make sure to print metrics for the <em>previous</em> iteration of the
training loop at each step. This helps with correctly pipelining host to data transfers:
see <a class="citation" href="#jax-training-cookbook">(JAX Team, 2025)</a> for a detailed description.</p>

<h1 id="sampling-from-the-model-first-cut">Sampling from the Model: First Cut</h1>

<p>To get a sense of the behavior of our learned model, we can write our sampler for it
now. Normally, efficient sampling requires a cache of past outputs to be maintained (we
already have the API for this), but we’ll discuss this later after we implement
attention.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">sample_one_token</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">key</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Transformer</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_in</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">cache_out</span> <span class="o">=</span> <span class="nf">_transformer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">cache_in</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
    <span class="n">cache_size</span> <span class="o">=</span> <span class="n">cache_size</span> <span class="o">+</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">((</span><span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">categorical</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">),))</span>
    <span class="k">return</span> <span class="n">next_token</span><span class="p">,</span> <span class="n">cache_out</span><span class="p">,</span> <span class="n">cache_size</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config_sampling_args</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">sharding_data</span><span class="sh">"</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()}</span>
<span class="n">config_sampling</span> <span class="o">=</span> <span class="nc">Config</span><span class="p">(</span><span class="o">**</span><span class="n">config_sampling_args</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">tokens_to_generate</span> <span class="o">=</span> <span class="mi">63</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">tokens_to_generate</span><span class="p">):</span>
    <span class="n">key_sampling</span><span class="p">,</span> <span class="n">sk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_sampling</span><span class="p">)</span>
    <span class="n">next_token</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_size</span> <span class="o">=</span> <span class="nf">sample_one_token</span><span class="p">(</span>
        <span class="n">config_sampling</span><span class="p">,</span> <span class="n">sk</span><span class="p">,</span> <span class="n">train_state</span><span class="p">.</span><span class="n">params</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">temperature</span>
    <span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">output</span><span class="p">,</span> <span class="n">next_token</span><span class="p">))</span>
<span class="n">output</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Array([1, 2, 3, 2, 3, 4, 3, 2, 3, 2, 3, 5, 6, 5, 4, 3, 4, 5, 2, 1, 2, 3,
       2, 1, 0, 1, 2, 3, 4, 3, 4, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 5, 6, 5,
       6, 5, 1, 2, 0, 1, 0, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 4, 3, 2],      dtype=int32)
</code></pre></div></div>

<p>We set a relatively high temperature in order to better highlight what the model has
learned. The generated sequence is in line with our hypothesis – the model has lowered
the loss by learning that it needs to predict the number that’s one above or one below
the current input, but it can’t do much better than guessing randomly between these two
possibilities (hence why our final loss is close to $\log(2)$).</p>

<p>Let’s improve this with attention!</p>

<h1 id="building-the-model-attention-and-positional-encodings">Building the Model: Attention and Positional Encodings</h1>

<p>An attention implementation that follows <code class="language-plaintext highlighter-rouge">jax.nn.dot_product_attention</code> shape
conventions follows, with both manual implementations and one using this function.
For now, we omit the cache implementation and positional embedding implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Attn</span><span class="p">,</span>
    <span class="n">x_seq</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">kv_cache</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># s: sequence length
</span>    <span class="c1"># d: embedding dim (config.d_model)
</span>    <span class="c1"># n: attention heads (config.num_heads)
</span>    <span class="c1"># h: head dim (config.d_head)
</span>    <span class="c1"># x_seq: s x d
</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">sd,d3nh-&gt;3snh</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">x_seq</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_qkv</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_att_qkv</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Attention computation
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache_size</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">s</span><span class="p">))[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># broadcast over heads
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_fa</span><span class="p">:</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">snh,tnh-&gt;nst</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="c1"># Scale and causal mask
</span>        <span class="n">logits</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># type: ignore[reportArgumentType]
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">)</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">nst,tnh-&gt;snh</span><span class="sh">"</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">snh,hnd-&gt;sd</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">attn_out</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_o</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">kv_cache</span>
</code></pre></div></div>

<p>Attention takes the embeddings of the input sequences $\vX \in \bbR^{S \times D}$ as
input. It computes $N$ “attention heads”, then combines them to form the output.
Each head, indexed by $n \in [N]$, is generated from different projections of the input
embedding sequences: we calculate</p>

\[\begin{equation*}
\vQ_n = \vX \vW_{Q, n}, \quad \vK_n = \vX \vW_{K, n}, \quad \vV_n = \vX \vW_{V, n},
\end{equation*}\]

<p>where the output dimension of the projections is $H$, then generate the head output as</p>

\[\begin{equation*}
\vA_n = \mathrm{softmax}(\vQ_n \vK_n^\top + \vM) \vV_n,
\end{equation*}\]

<p>with the softmax being taken along rows, and $\vM$ denoting a causal mask:</p>

\[M_{ij} = \begin{cases}
0 &amp; i \leq j \\
-\infty &amp; \ow.
\end{cases}\]

<p>The heads are then concatenated together along the embedding dimension, producing an $S
\times NH$ dimensional result (usually $NH = D$), and then an output projection $\vW_O$
is applied to produce the attention output, which has the same shape as the input $\vX$.
The code above implements these operations concisely with <code class="language-plaintext highlighter-rouge">jnp.einsum</code>.</p>

<p>We can replace our previous placeholder attention implementation with a complete one and
then rerun our training loop, since we already added the proper initialization code
above.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
loss: 0.6685549020767212	step: 50
loss: 0.58647620677948	step: 100
loss: 0.5948932766914368	step: 150
loss: 0.5258455872535706	step: 200
loss: 0.5094199776649475	step: 250
loss: 0.40264642238616943	step: 300
loss: 0.42422279715538025	step: 350
loss: 0.48532211780548096	step: 400
loss: 0.45087528228759766	step: 450
loss: 0.3302219808101654	step: 500
loss: 0.39742058515548706	step: 550
loss: 0.3917175531387329	step: 600
loss: 0.3619387745857239	step: 650
loss: 0.34305423498153687	step: 700
loss: 0.2451237142086029	step: 750
loss: 0.27099400758743286	step: 800
loss: 0.4500095248222351	step: 850
loss: 0.26931577920913696	step: 900
loss: 0.2725009322166443	step: 950
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
loss: 0.15929250419139862	step: 50
loss: 0.14883831143379211	step: 100
loss: 0.165802463889122	step: 150
loss: 0.14946813881397247	step: 200
loss: 0.15420110523700714	step: 250
loss: 0.14391759037971497	step: 300
loss: 0.2680056691169739	step: 350
loss: 0.14069046080112457	step: 400
loss: 0.27090030908584595	step: 450
loss: 0.14207065105438232	step: 500
loss: 0.13676664233207703	step: 550
loss: 0.13658639788627625	step: 600
loss: 0.13079527020454407	step: 650
loss: 0.39731210470199585	step: 700
loss: 0.1367562860250473	step: 750
loss: 0.13199977576732635	step: 800
loss: 0.1323813498020172	step: 850
loss: 0.12948714196681976	step: 900
loss: 0.13398176431655884	step: 950
</code></pre></div></div>

<p>Two back-to-back output traces are posted above. We can get a sense that this model
<em>should</em> be able to learn to solve the task, but the poor optimizer (constant learning
rate SGD) is holding it back. If we rerun the sampling code above with the new model at
low temperature (<code class="language-plaintext highlighter-rouge">temperature=0.1</code>), we do get correct outputs:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Array([1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4,
       5, 6, 7, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8,
       9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8],      dtype=int32)
</code></pre></div></div>

<p>Yet the final loss is suggestive that the model we’ve learned is not very good.
We can improve the model further by incorporating positional encodings. We’ll use the
modern choice, rotary positional encodings (RoPE). There are already many great blogs
explaining the intuition behind these encodings, so I’ll just focus here on the
algebraic ideas.</p>

<p>The basic idea is to consider the dot product of a query-key pair forming one head’s
attention matrix $\vA$, as we defined it above (dropping the subscript $n$ for
concision). Ignoring the mask, for the $ij$-th entry of this attention matrix, we
consider the dot product (“attention logits”) $\vq_i \vk_j^\top$.
A notable property of attention is that the only dependence of its output on the
position $i$ is through the causal mask $\vM$: if we drop the mask, then permuting the
input positions produces a corresponding permutation of the output positions,
because every position receives the same projections $\vW_{QKV}$.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>This is arguably undesirable: the model benefits in many cases from the ability to learn
a sequence-to-sequence mapping that is more position sensitive. For example, in our toy
data setting, the model would be able to learn a good mapping faster if it were able to
selectively focus on only a small number of past tokens (since this is enough to
determine whether we’re in a “rising” or “falling” part of the sequence).</p>

<p>A nice way to incorporate this positional information is through a <em>relative</em> positional
embedding. In such a method, we modify the attention operation so that the attention
logits incorporate the magnitude of the difference in positions $\abs{i - j}$. In RoPE,
this is done by inducing a quadratic form</p>

\[\begin{equation*}
    \vq_i \vR_{\abs{i-j}} \vk_j^\top,
\end{equation*}\]

<p>where $\vR_{\abs{i-j}}$ is a specific rotation matrix.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> RoPE’s choice of this matrix has
the useful property that for fixed-norm keys and queries, the magnitude of the encoded
attention logits <em>decays</em> as a function of $\abs{i-j}$, giving <em>a priori</em> less weight to
logits of positions that are further apart.</p>

<p>It also has the essential property that the encoded logits can be computed efficiently.
It turns out that there are (real-valued) sequences $\vc_i$, $\vs_i$ corresponding to each
position, such that the following holds. Let $\vPi \in \bbR^{H \times H}$ denote the
permutation matrix that reverses the top $H/2$ coordinates of a vector with the bottom
$H/2$ coordinates (assume $H$ is even).<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> Then one has</p>

\[\begin{equation*}
    \vq_i \vR_{\abs{i-j}} \vk_j^\top
    =
    (\vq_i \circ \vc_i + \vPi \vq_i \circ \vs_i)
    (\vk_j \circ \vc_j + \vPi \vk_j \circ \vs_j)^\top,
\end{equation*}\]

<p>where matrix multiplication binds before elementwise multiplication $\circ$. In
particular, we can compute the encoded logits via a small number of elementwise
multiplications, adds, and concatenations involving the raw keys and queries, which
leads to a minimal-complexity implementation. The sequences $\vc_i, \vs_i$ depend only
on position, and can be precomputed and reused.</p>

<p>We give a simple implementation of RoPE below, and its integration into our attention
layer we implemented previously.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_precompute_rope_sincos</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
    <span class="n">freqs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span>
        <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">rope_theta</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="o">*</span> <span class="mi">2</span>
        <span class="o">/</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span>
        <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">d_head</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">())</span>
    <span class="p">)</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">())</span>
    <span class="n">cycles</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">freqs</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">cycles</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">cycles</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_apply_rope</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">cos</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">sin</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">positions</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>  <span class="c1"># S x H
</span><span class="p">):</span>
    <span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
    <span class="n">c</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">cos</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">positions</span><span class="p">].</span><span class="nf">get</span><span class="p">(),</span> <span class="n">sin</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="n">positions</span><span class="p">].</span><span class="nf">get</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
        <span class="p">(</span><span class="n">c</span> <span class="o">*</span> <span class="n">x_1</span> <span class="o">-</span> <span class="n">s</span> <span class="o">*</span> <span class="n">x_2</span><span class="p">,</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x_2</span> <span class="o">+</span> <span class="n">s</span> <span class="o">*</span> <span class="n">x_1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Attn</span><span class="p">,</span>
    <span class="n">x_seq</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">kv_cache</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># s: sequence length
</span>    <span class="c1"># d: embedding dim (config.d_model)
</span>    <span class="c1"># n: attention heads (config.num_heads)
</span>    <span class="c1"># h: head dim (config.d_head)
</span>    <span class="c1"># x_seq: s x d
</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">sd,d3nh-&gt;3snh</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">x_seq</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_qkv</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_att_qkv</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Apply RoPE
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_rope</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">.</span><span class="n">update_cache</span><span class="p">:</span>
            <span class="n">cache_size</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore passed value
</span>        <span class="k">with</span> <span class="n">jax</span><span class="p">.</span><span class="nf">ensure_compile_time_eval</span><span class="p">():</span>
            <span class="n">rope_cos</span><span class="p">,</span> <span class="n">rope_sin</span> <span class="o">=</span> <span class="nf">_precompute_rope_sincos</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">())</span>
        <span class="n">_apply_rope_one_head</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span>
            <span class="n">_apply_rope</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">rope_cos</span><span class="p">,</span> <span class="n">rope_sin</span><span class="p">,</span> <span class="n">positions</span> <span class="o">+</span> <span class="n">cache_size</span>
        <span class="p">)</span>
        <span class="n">_apply_rope_all_heads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">_apply_rope_one_head</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="nf">_apply_rope_all_heads</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="nf">_apply_rope_all_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Attention computation
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache_size</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">s</span><span class="p">))[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># broadcast over heads
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_fa</span><span class="p">:</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">snh,tnh-&gt;nst</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="c1"># Scale and causal mask
</span>        <span class="n">logits</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># type: ignore[reportArgumentType]
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">)</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">nst,tnh-&gt;snh</span><span class="sh">"</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">snh,hnd-&gt;sd</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">attn_out</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_o</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">kv_cache</span>
</code></pre></div></div>

<p>The changes to the <code class="language-plaintext highlighter-rouge">_attn</code> function are all within the code block marked with <code class="language-plaintext highlighter-rouge"># Apply
RoPE</code> (everything else is unchanged); the <code class="language-plaintext highlighter-rouge">_apply_rope</code> function takes care of the
necessary permutation/concatenation operation. We use JAX’s <code class="language-plaintext highlighter-rouge">ensure_compile_time_eval</code>
context manager to ensure the RoPE multiplying sequences are precomputed. The
multiplying sequences themselves are chosen to have geometrically-varying time values,
with a rate parameter <code class="language-plaintext highlighter-rouge">rope_theta</code> set to <code class="language-plaintext highlighter-rouge">1e4</code> by default (in general, set based on the
target context length, here 1024).</p>

<p>With RoPE, we find it much easier to learn a strong model for our toy data.
After rerunning training with these modifications, we observe the following loss
progression:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
loss: 0.6326758861541748	step: 50
loss: 0.4087381660938263	step: 100
loss: 0.14004486799240112	step: 150
loss: 0.08325601369142532	step: 200
loss: 0.06802305579185486	step: 250
loss: 0.05945558100938797	step: 300
loss: 0.05321500450372696	step: 350
loss: 0.052829645574092865	step: 400
loss: 0.048602353781461716	step: 450
loss: 0.04696694016456604	step: 500
loss: 0.04576549679040909	step: 550
loss: 0.04564153775572777	step: 600
loss: 0.04341501370072365	step: 650
loss: 0.043622132390737534	step: 700
loss: 0.04205687716603279	step: 750
loss: 0.04401983320713043	step: 800
loss: 0.04256521165370941	step: 850
loss: 0.041673485189676285	step: 900
loss: 0.0421551875770092	step: 950
</code></pre></div></div>

<p>Much better! For a completely overconfident model, we might expect to have loss around
$\log(2) / 256 \approx 0.002$, corresponding to randomly guessing for the first token in
each sequence of the batch (we train with sequence length <code class="language-plaintext highlighter-rouge">256</code>). This is within about
an order of magnitude.</p>

<h1 id="building-the-model-sampling-with-a-cache">Building the Model: Sampling with a Cache</h1>

<p>The implementation of sampling we gave above is extremely slow: although we <code class="language-plaintext highlighter-rouge">jit</code> the
<code class="language-plaintext highlighter-rouge">sample_one_token</code> function, we are re-forwarding the entire updated sequence through
the model after every generation, which is very wasteful. A quick test on our
previously-implemented sampling function (on 1x TPU v4 host) takes about 50 seconds to
sample 63 tokens, including the time to <code class="language-plaintext highlighter-rouge">jit</code> compile the sampling function.</p>

<p>This makes sense, given our naive implementation. Indeed, if we recall the attention
definition, at the $i$-th token each head computes</p>

\[\begin{equation*}
    \mathrm{softmax}( \vq_i \vK^T ) \vV,
\end{equation*}\]

<p>and the output projection just accumulates these independent heads and ‘lifts’ the head
dimension $H$ to the output dimension $D$.
In other words, if we were to save the previous key and value tokens $(\vk_j,
\vv_j)_{j=1}^{i-1}$ in a cache, we could reuse these to compute each head’s attention
output with no wasted operations: we’d just need to compute $\vk_i$ and $\vv_i$ for the
current position, then load the previous positions’ keys and values from the cache to
compute the full attention output.</p>

<p>This data structure is called a <em>KV cache</em>, and it’s essential for fast inference, since
autoregressive operation implies we can only compute one token at a time (in contrast to
training, where we forward an entire batch of sequences at once).
A back-of-the-envelope analysis suggests that the naive sampling strategy (re-forward
the entire sequence on every new token generation) takes $O(S^2D^2 + DS^3)$ FLOPs to
generate an $S$-length sequence starting from an empty prompt, whereas the KV caching
strategy takes only $O(SD^2 + DS^2)$ FLOPs. This is a significant savings!</p>

<p>The catch is that the KV caching strategy requires significantly more memory accesses
and usage compared to the naive approach. In JAX, we also need to be careful to
implement the KV cache ‘correctly’ – since the cache grows after every generation,
naive implementations can entail inputs of different sizes to <code class="language-plaintext highlighter-rouge">_attn</code> (triggering a
re-compilation every time we call it, when it’s <code class="language-plaintext highlighter-rouge">jit</code>ted!), or dynamically-sized arrays
(not allowed within a <code class="language-plaintext highlighter-rouge">jit</code> context!).<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">5</a></sup></p>

<p>Our simple implementation of the KV cache is below. Since we left the API for the cache
in the code we’ve written already, we just need to implement its functionality in the
<code class="language-plaintext highlighter-rouge">_attn</code> function, and add a function to initialize an empty cache. To play nice with
<code class="language-plaintext highlighter-rouge">jit</code>, we will implement a static-sized cache with a fixed size <code class="language-plaintext highlighter-rouge">config.max_seq_len</code>
(the same parameter we used for RoPE), ensuring we can <code class="language-plaintext highlighter-rouge">jit</code> the <code class="language-plaintext highlighter-rouge">_attn</code> function only
once, and maintain an auxilary <code class="language-plaintext highlighter-rouge">cache_size</code> variable to keep track of how much we’ve
written to the cache. To avoid having dynamically-sized arrays in the <code class="language-plaintext highlighter-rouge">_attn</code> function,
we are a bit wasteful and zero-initialize the cache, then just multiply the current
query $\vq_i$ against the entire cache on every attention operation.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">6</a></sup> To avoid having
these extra keys influence the attention computation (since softmaxing them produces a
nonzero output!), we augment the attention mask to mask them out.</p>

<p>The implementation is below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">Attn</span><span class="p">,</span>
    <span class="n">x_seq</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">kv_cache</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>  <span class="c1"># 2 x config.max_seq_len x n x h (see below)
</span>    <span class="n">cache_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># s: sequence length
</span>    <span class="c1"># d: embedding dim (config.d_model)
</span>    <span class="c1"># n: attention heads (config.num_heads)
</span>    <span class="c1"># h: head dim (config.d_head)
</span>    <span class="c1"># x_seq: s x d
</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">sd,d3nh-&gt;3snh</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">x_seq</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_qkv</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_att_qkv</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">qkv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># we save k shape later, after possibly prepending cache
</span>
    <span class="c1"># Apply RoPE
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_rope</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">.</span><span class="n">update_cache</span><span class="p">:</span>
            <span class="n">cache_size</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore passed value
</span>        <span class="k">with</span> <span class="n">jax</span><span class="p">.</span><span class="nf">ensure_compile_time_eval</span><span class="p">():</span>
            <span class="n">rope_cos</span><span class="p">,</span> <span class="n">rope_sin</span> <span class="o">=</span> <span class="nf">_precompute_rope_sincos</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">())</span>
        <span class="n">_apply_rope_one_head</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span>
            <span class="n">_apply_rope</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">rope_cos</span><span class="p">,</span> <span class="n">rope_sin</span><span class="p">,</span> <span class="n">positions</span> <span class="o">+</span> <span class="n">cache_size</span>
        <span class="p">)</span>
        <span class="n">_apply_rope_all_heads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">vmap</span><span class="p">(</span><span class="n">_apply_rope_one_head</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="nf">_apply_rope_all_heads</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="nf">_apply_rope_all_heads</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Read/update/concatenate the cache
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">update_cache</span><span class="p">:</span>
        <span class="n">k_cache</span><span class="p">,</span> <span class="n">v_cache</span> <span class="o">=</span> <span class="n">kv_cache</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kv_cache</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_update_slice</span><span class="p">(</span><span class="n">k_cache</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="n">cache_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">dynamic_update_slice</span><span class="p">(</span><span class="n">v_cache</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">cache_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">kv_cache_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">k</span><span class="p">[</span><span class="bp">None</span><span class="p">],</span> <span class="n">v</span><span class="p">[</span><span class="bp">None</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">kv_cache_out</span> <span class="o">=</span> <span class="n">kv_cache</span>
        <span class="n">cache_size</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore passed value
</span>
    <span class="c1"># Attention computation
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">cache_size</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">s</span><span class="p">))[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="c1"># with static kv cache, must mask unused memory!
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">t</span><span class="p">)[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&lt;</span> <span class="n">cache_size</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># broadcast over heads
</span>    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">use_fa</span><span class="p">:</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">dot_product_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">snh,tnh-&gt;nst</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="c1"># Scale and causal mask
</span>        <span class="n">logits</span> <span class="o">*=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="o">**</span><span class="mf">0.5</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># type: ignore[reportArgumentType]
</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">)</span>
        <span class="n">attn_out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">nst,tnh-&gt;snh</span><span class="sh">"</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">snh,hnd-&gt;sd</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">attn_out</span><span class="p">,</span>
        <span class="n">params</span><span class="p">.</span><span class="n">w_o</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="o">*</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_res_stream</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">kv_cache_out</span>


<span class="k">def</span> <span class="nf">init_kv_cache</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span>
        <span class="p">(</span>
            <span class="n">config</span><span class="p">.</span><span class="n">global_batch_size</span><span class="p">,</span>
            <span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="n">config</span><span class="p">.</span><span class="n">max_seq_len</span><span class="p">,</span>
            <span class="n">config</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">config</span><span class="p">.</span><span class="n">d_head</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">,</span>
        <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">sharding_data</span> <span class="o">+</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="p">.</span><span class="n">sharding_att_qkv</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>Running this block and then rerunning our training script shows that our changes haven’t
broken anything. We can test how much it speeds up sampling, as well. We modify the
sampler above slightly – we enable the cache in the config, and forward one token per
sampling step to the model, rather than the entire output, as the cache will store all
the context.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">config_sampling_args</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">sharding_data</span><span class="sh">"</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(),</span> <span class="sh">"</span><span class="s">update_cache</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
<span class="n">config_sampling</span> <span class="o">=</span> <span class="nc">Config</span><span class="p">(</span><span class="o">**</span><span class="n">config_sampling_args</span><span class="p">)</span>

<span class="n">next_token</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">((</span><span class="mi">1</span><span class="p">,))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">next_token</span>
<span class="n">cache</span> <span class="o">=</span> <span class="nf">init_kv_cache</span><span class="p">(</span><span class="n">config_sampling</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cache_size</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tokens_to_generate</span> <span class="o">=</span> <span class="mi">63</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">tokens_to_generate</span><span class="p">):</span>
    <span class="n">key_sampling</span><span class="p">,</span> <span class="n">sk</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">key_sampling</span><span class="p">)</span>
    <span class="n">next_token</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_size</span> <span class="o">=</span> <span class="nf">sample_one_token</span><span class="p">(</span>
        <span class="n">config_sampling</span><span class="p">,</span>
        <span class="n">sk</span><span class="p">,</span>
        <span class="n">train_state</span><span class="p">.</span><span class="n">params</span><span class="p">,</span>
        <span class="n">next_token</span><span class="p">,</span>
        <span class="n">cache</span><span class="p">,</span>
        <span class="n">cache_size</span><span class="p">,</span>
        <span class="n">temperature</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">((</span><span class="n">output</span><span class="p">,</span> <span class="n">next_token</span><span class="p">))</span>
<span class="n">output</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Output:
Array([1, 2, 3, 4, 5, 6, 7, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4,
       5, 6, 7, 8, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8,
       9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 8],      dtype=int32)
</code></pre></div></div>

<p>Correct outputs!
The runtime is about 2 seconds including the <code class="language-plaintext highlighter-rouge">jit</code> compilation time on the first run,
and <code class="language-plaintext highlighter-rouge">0.5</code> seconds on subsequent runs (since the static cache and
one-token-input-per-sampling-step means the <code class="language-plaintext highlighter-rouge">jit</code>ted sampler sees the same input shapes
on every call). Quite a speedup!</p>

<h1 id="conclusions">Conclusions</h1>

<p>In this blog post, we’ve built up:</p>
<ul>
  <li>A simple <code class="language-plaintext highlighter-rouge">jit</code>table transformer model with RoPE;</li>
  <li>A basic training loop with SGD and toy data (digit alphabet);</li>
  <li>A minimal <code class="language-plaintext highlighter-rouge">jit</code>table sampler with a static KV cache.</li>
</ul>

<p>All in bare-metal JAX.</p>

<p>Since the length is already nontrivial, we’ll save further extensions – refactoring
into a python package, improving the config scaffolding for running experiments and
adding distributed training support, improving the optimizer, and training on actual
language data – to the next post.
I hope this exposition is helpful to learners of JAX and transformers – writing it has
been very helpful to me!</p>

<h1 id="accumulated-configs">Accumulated Configs</h1>

<p>For reference, here’s the implementation of the <code class="language-plaintext highlighter-rouge">Config</code> class we’ve used throughout the
discussion above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Architecture: config
</span><span class="nd">@jax.tree_util.register_static</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">kw_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">frozen</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
    <span class="c1"># Experiment orchestration params
</span>    <span class="n">mesh_axis_names</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="p">...]</span> <span class="o">=</span> <span class="p">(</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,)</span>
    <span class="n">mesh_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="p">...]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1337</span>

    <span class="c1"># Data and training params
</span>    <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">global_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-2</span>

    <span class="c1"># Model architecture params
</span>    <span class="n">num_vocab</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">d_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">mlp_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">param_std</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="n">rope_theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10000.0</span>
    <span class="n">max_seq_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>

    <span class="c1"># Model dtypes
</span>    <span class="n">param_dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">bfloat16</span>  <span class="c1"># weights, activations
</span>    <span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>  <span class="c1"># layernorm, attn logits, rope
</span>    <span class="n">optimizer_dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="n">float32</span>  <span class="c1"># optimizer state
</span>
    <span class="c1"># Model call-time params
</span>    <span class="n">eps_ln</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span>
    <span class="n">use_bias_ln</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">use_fa</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">use_bias_mlp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">use_rope</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">update_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>  <span class="c1"># default training
</span>
    <span class="c1"># Model sharding params
</span>    <span class="n">sharding_data</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">(</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">sharding_wqkv</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_wo</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_wup</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_wdown</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_mlp_hidden</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_res_stream</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>
    <span class="n">sharding_att_qkv</span><span class="p">:</span> <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">PartitionSpec</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nc">P</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Set up and register mesh
</span>        <span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">make_mesh</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">mesh_shape</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">mesh_axis_names</span><span class="p">,</span>
            <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mesh_shape</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">AxisType</span><span class="p">.</span><span class="n">Explicit</span><span class="p">,),</span>
        <span class="p">)</span>
        <span class="n">jax</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="nf">set_mesh</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>

        <span class="c1"># Checks
</span>        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">d_head</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">(</span>
            <span class="sh">"</span><span class="s">Head dimension needs to be divisible by 2 for RoPE</span><span class="sh">"</span>
        <span class="p">)</span>
</code></pre></div></div>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to the <a href="https://sites.research.google/trc/about/">TRC program</a> for
compute support.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Namely, because we won’t have a clean way to combine our model
parameter definitions with the actual application of the model. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The causal mask actually <em>is</em> enough to enable sequence-to-sequence mappings that
depend on position to be learned in multi-layer transformers. The recent trend of
“NoPE” (no positional encoding) in open-source large models is evidence of this. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>It’s actually a block-diagonal rotation matrix, with $2 \times 2$ blocks. This is
what leads to the efficient approach to computing its induced quadratic form below! <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>In code, for a list <code class="language-plaintext highlighter-rouge">v</code>, this is just <code class="language-plaintext highlighter-rouge">v[H//2:] + v[:H//2]</code>. Moreover, the choice
of the permutation $\vPi$ is not essential (because the coordinates of the embedded
keys and queries follow a learnable linear transformation), and can be changed for
either mathematical clarity or code clarity (we choose based on the latter; the
original RoPE paper emphasized the former). <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Actually, our naive sampling implementation above, which re-forwards the entire
sequence after concatenating, runs so slow because the <em>input</em> to the transformer
keeps growing in size – which triggers a recompilation after every generation!
Avoiding these pitfalls is a general challenge in JAX. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Although this <em>feels</em> very wasteful, it actually only loses (asymptotically) a
factor of $2$ in FLOPs versus the optimal implementation. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>

  
    <h2>References</h2>
    
      <ol class="bibliography"><li><div class="bibcompact" id="jax-training-cookbook">
  <span class="author">JAX Team</span>
  <span class="year">(2025).</span>
  <span class="title"><em>The Training Cookbook</em>.</span>
  <span class="publication">
    
      Retrieved from <a href="https://docs.jax.dev/en/latest/the-training-cookbook.html">https://docs.jax.dev/en/latest/the-training-cookbook.html</a>. (Accessed 2025-08-29)
    
  </span>
</div>
</li>
<li><div class="bibcompact" id="scaling-book">
  <span class="author">Austin, J., Douglas, S., Frostig, R., Levskaya, A., Chen, C., Vikram, S., Lebron, F., Choy, P., Ramasesh, V., Webson, A., & Pope, R.</span>
  <span class="year">(2025).</span>
  <span class="title"><em>How to Scale Your Model</em>.</span>
  <span class="publication">
    
      
        Retrieved from <a href="https://jax-ml.github.io/scaling-book/">https://jax-ml.github.io/scaling-book/</a>
      
    
  </span>
</div>
</li></ol>
    
  

  

</article>


      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      


<div id="icon-container">
  <div>
    <a href=mailto:sam@ttic.edu style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" />
    </a>
  </div>
  <div>
    <a href=https://scholar.google.com/citations?user=5WT38A0AAAAJ style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" />
    </a>
  </div>

  <div>
    <a href=https://twitter.com/_sdbuchanan
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" />
    </a>
  </div>

  <div>
    <a href=https://www.linkedin.com/in/sam-buchanan-4507a6b3
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/linkedin.svg" alt="LinkedIn icon" />
    </a>
  </div>

  <div>
    <a href=https://github.com/sdbuch
       style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" />
    </a>
  </div>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Littlefoot for footnotes and citations -->
<script src="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/littlefoot@4/dist/littlefoot.css">

<script>
document.addEventListener('DOMContentLoaded', function() {
  // Initialize Littlefoot for standard footnotes
  littlefoot.littlefoot({
    buttonTemplate: '<button aria-expanded="false" class="littlefoot__button" id="<% id %>" title="See Footnote"><% number %></button>',
    activateDelay: 100,
    activateCallback: function(popover, button) {
      // Render MathJax in the footnote popup
      if (window.MathJax && window.MathJax.typesetPromise) {
        window.MathJax.typesetPromise([popover]).catch(function (err) {
          console.log('MathJax typeset failed: ' + err.message);
        });
      }
    },
    allowMultiple: true,
    dismissDelay: 500,
    hoverDelay: 250,
    numberResetSelector: 'article',
    scope: 'body'
  });

  // Custom handler for jekyll-scholar citations
  const citeLinks = document.querySelectorAll('a[href*="#"]');
  citeLinks.forEach(function(link) {
    // Check if this is a citation link (points to bibliography)
    const href = link.getAttribute('href');
    if (href && href.startsWith('#') && href.match(/^#[A-Za-z]/)) {
      const targetId = href.substring(1);
      const targetElement = document.getElementById(targetId);
      
      // If target exists and looks like a bibliography entry
      if (targetElement && (targetElement.closest('.bibliography') || targetElement.tagName === 'LI')) {
        link.addEventListener('mouseenter', function(e) {
          showCitationPopup(e, targetElement);
        });
        
        link.addEventListener('mouseleave', function() {
          hideCitationPopup();
        });
        
        // Prevent default click behavior for hover-only interaction
        link.addEventListener('click', function(e) {
          e.preventDefault();
        });
      }
    }
  });
});

let citationPopup = null;
let hideTimeout = null;
let showTimeout = null;

function showCitationPopup(event, targetElement) {
  // Clear any pending show/hide operations
  if (hideTimeout) {
    clearTimeout(hideTimeout);
    hideTimeout = null;
  }
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Immediately hide any existing popup
  if (citationPopup && citationPopup.parentNode) {
    citationPopup.parentNode.removeChild(citationPopup);
    citationPopup = null;
  }
  
  // Show new popup after a short delay to prevent flicker
  showTimeout = setTimeout(function() {
    // Create popup
    citationPopup = document.createElement('div');
    citationPopup.className = 'littlefoot__popover citation-popup';
    citationPopup.innerHTML = '<div class="littlefoot__content citation-content">' + targetElement.innerHTML + '</div>';
    
    // Position popup
    const rect = event.target.getBoundingClientRect();
    citationPopup.style.position = 'absolute';
    citationPopup.style.top = (rect.bottom + window.scrollY + 5) + 'px';
    citationPopup.style.left = rect.left + 'px';
    citationPopup.style.zIndex = '1000';
    citationPopup.style.maxWidth = '400px';
    
    document.body.appendChild(citationPopup);
    
    // Render MathJax in the popup
    if (window.MathJax && window.MathJax.typesetPromise) {
      window.MathJax.typesetPromise([citationPopup]).catch(function (err) {
        console.log('MathJax typeset failed: ' + err.message);
      });
    }
    
    // Add hover handlers to keep popup open
    citationPopup.addEventListener('mouseenter', function() {
      if (hideTimeout) {
        clearTimeout(hideTimeout);
        hideTimeout = null;
      }
    });
    
    citationPopup.addEventListener('mouseleave', function() {
      hideCitationPopup();
    });
    
    showTimeout = null;
  }, 50);
}

function hideCitationPopup() {
  // Clear any pending show operation
  if (showTimeout) {
    clearTimeout(showTimeout);
    showTimeout = null;
  }
  
  // Clear any existing hide timeout
  if (hideTimeout) {
    clearTimeout(hideTimeout);
  }
  
  hideTimeout = setTimeout(function() {
    if (citationPopup && citationPopup.parentNode) {
      citationPopup.parentNode.removeChild(citationPopup);
      citationPopup = null;
    }
    hideTimeout = null;
  }, 100);
}
</script>


</html>
