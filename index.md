---
layout: home
---

{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu). I completed
my Ph.D. in Electrical Engineering at [Columbia
University](https://ee.columbia.edu) in 2022, working with [John
Wright](http://www.columbia.edu/~jw2966/), and my B.S. in Electrical
Engineering at the [University of Kansas](https://eecs.ku.edu).

I study the mathematics of representation learning from the
perspective of signals and data. I'm interested in questions that span theory
and practice &mdash; What structural properties of modern data play a role in the
success or failure of deep learning? How can we design better deep
architectures by exploiting these structures? I'm especially interested in
applications to visual data.

## Upcoming Events

- **ICLR 2024**: Presenting two posters in Vienna: _Learned Proximal Networks_
  (Friday, May 10, a.m. session; [project
  page](https://zhenghanfang.github.io/learned-proximal-networks/), [ICLR
  page](https://iclr.cc/virtual/2024/poster/17978)) and _CRATE-MAE_ (Thursday,
  May 9, a.m. session; [project
  page](https://ma-lab-berkeley.github.io/CRATE/), [ICLR
  page](https://iclr.cc/virtual/2024/poster/18688)). Come say hi!

- **Tutorial:** Giving a tutorial lecture on designing deep network
  architectures to pursue low-dimensional structures in data and our recent
  white-box transformers work at [CVPR
  2024](https://cvpr2024-tutorial-low-dim-models.github.io) in Seattle (Jun
  2024).

- **Travel:** Will be at BIRS Oaxaca for the
  ["Mathematics of Deep Learning" workshop](https://www.birs.ca/events/2024/5-day-workshops/24w5297) (Jun 2024).

## Recent Highlights

- **1st Conference on Parsimony and Learning:** I co-organized the inaugural [Conference on
  Parsimony and Learning (CPAL)](https://cpal.cc), which took place at the
  University of Hong Kong from January 3--6, 2024. Thanks to all authors,
  speakers, organizers, and especially to the local team at HKU, whose hard
  work made the conference a success! Stay tuned for CPAL 2025. _(Jan 2024)_

- **White-Box Deep Networks Tutorial:** We delivered a tutorial on building white-box deep neural networks
  at [ICASSP 2024](https://cmsworkshops.com/ICASSP2024/tutorials.php#tut25) in
  Seoul. [Slides are
  available](https://drive.google.com/drive/folders/1j7wtXteUA0dNT8Bl5Q1hVGpBU9_QAzKu),
  and video recordings will be published soon! _(Apr 2024)_

## Recent Updates

- **Publication:** [Learned proximal networks](https://zhenghanfang.github.io/learned-proximal-networks/), a methodology for
  parameterizing, learning, and evaluating expressive priors for data-driven inverse
  problem solvers with convergence guarantees, will appear in ICLR 2024.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=kNPcOaqC5r). _(May 2024)_

- **Publication:** [CRATE-MAE](https://ma-lab-berkeley.github.io/CRATE/) will
  appear in ICLR 2024. At the heart of this work is a connection between
  denoising and compression, which we use to derive a corresponding decoder
  architecture for the "white-box" transformer CRATE encoder.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=PvyOYleymy). _(May 2024)_
  <!-- This work is described in Section 3 of the [complete CRATE -->
  <!-- paper](https://arxiv.org/abs/2311.13110). _(Jan 2024)_ -->

- **Talk:** Gave my annual Research at TTIC talk about [TILTED](https://brentyi.github.io/tilted)! [Here is the
  video
  recording](https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=db5b4c2a-96aa-4722-bb5f-b067015c0314). _(Mar 2024)_

- **Talk:** Gave the [Redwood
  Seminar](https://redwood.berkeley.edu/seminars/sam-buchanan-feb-2024/). _(Feb 2024)_

- **Publication:** We presented [CRATE](https://ma-lab-berkeley.github.io/CRATE/) at [NeurIPS
  2023](https://neurips.cc/virtual/2023/poster/71567), and as an oral in the
  [XAI in Action](https://neurips.cc/virtual/2023/75163) workshop. _(Dec 2023)_

- **Preprint Release:** The full version of [the CRATE
  story](http://arxiv.org/abs/2311.13110) is now on arXiv.
  [CRATE](https://ma-lab-berkeley.github.io/CRATE/) is a "white-box" (yet
  scalable) transformer architecture where each layer is derived from the
  principles of compression and sparsification of the input data distribution.
  This white-box derivation leads CRATE's representations to have surprising
  [emergent segmentation properties](https://arxiv.org/abs/2308.16271) in
  vision applications without any complex self-supervised pretraining. _(Nov 2023)_

- **Publication:** We presented [TILTED](https://brentyi.github.io/tilted/) at
  ICCV 2023. TILTED improves visual quality, compactness, and interpretability
  for hybrid neural field 3D representations by incorporating geometry into the
  latent features. Find the full version [on arXiv](https://arxiv.org/abs/2308.15461). _(Oct 2023)_

## [Past Updates]({{ site.baseurl }}/past_updates)
