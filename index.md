---
layout: home
---

{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu). I completed
my Ph.D. in Electrical Engineering at [Columbia
University](https://ee.columbia.edu) in 2022, working with [John
Wright](http://www.columbia.edu/~jw2966/), and my B.S. in Electrical
Engineering at the [University of Kansas](https://eecs.ku.edu).

I study the mathematics of representation learning from the
perspective of signals and data. I'm interested in questions that span theory
and practice &mdash; What structural properties of modern data play a role in the
success or failure of deep learning? How can we design better deep
architectures by exploiting these structures? I'm especially interested in
applications to visual data.

## Upcoming Events

- **2nd Conference on Parsimony and Learning:** I am co-organizing the second [Conference on
  Parsimony and Learning (CPAL)](https://cpal.cc), to take place at Stanford
  University in March 2025!

## Recent Highlights

- **White-Box Deep Networks Tutorial:** We delivered a tutorial on building white-box deep neural networks
  at [ICASSP 2024](https://cmsworkshops.com/ICASSP2024/tutorials.php#tut25) in
  Seoul (Apr 2024), and at [CVPR
  2024](https://cvpr2024-tutorial-low-dim-models.github.io) in Seattle (Jun
  2024). The most recent tutorial slides can be found
  [here](https://www.dropbox.com/home/CVPR-tutorial) (Lectures 2-1 -- 2-3).

## Recent Updates

- **Talk:** Spoke at the [IDEAL Privacy and Interpretability in Generative
  AI](https://www.ideal-institute.org/2024/09/04/workshop-on-harmonious-human-ai-ecosystems-2/)
  workshop. _(Nov 2024)_

- **Talk:** Spoke at [Asilomar
  2024](https://cmsworkshops.com/Asilomar2024/view_session.php?SessionID=1086).
  _(Oct 2024)_

- **MDS '24 Special Session:** Co-organizing a SIAM MDS '24 special session
  on [Mathematical Principles in Foundation
  Models](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=80543).
  _(Oct 2024)_

- **Publication:** The full version of [the CRATE
  story](http://arxiv.org/abs/2311.13110) has been accepted for publication in
  JMLR.
  [CRATE](https://ma-lab-berkeley.github.io/CRATE/) is a "white-box" (yet
  scalable) *transformer architecture* where each layer is derived from the
  principles of compression and sparsification of the input data distribution.
  This white-box derivation leads CRATE's representations to have surprising
  [emergent segmentation properties](https://arxiv.org/abs/2308.16271) in
  vision applications without any complex self-supervised pretraining. _(Aug 2024)_

- **Talk:** Spoke at the BIRS Oaxaca ["Mathematics of Deep Learning"
  workshop](https://www.birs.ca/events/2024/5-day-workshops/24w5297) about
  white-box networks (Jun 2024).

- **Publication:** [Learned proximal networks](https://zhenghanfang.github.io/learned-proximal-networks/), a methodology for
  parameterizing, learning, and evaluating expressive priors for data-driven inverse
  problem solvers with convergence guarantees, appeared in ICLR 2024.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=kNPcOaqC5r). _(May 2024)_

- **Publication:** [CRATE-MAE](https://ma-lab-berkeley.github.io/CRATE/) appeared in ICLR 2024. At the heart of this work is a connection between
  denoising and compression, which we use to derive a corresponding decoder
  architecture for the "white-box" transformer CRATE encoder.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=PvyOYleymy). _(May 2024)_
  <!-- This work is described in Section 3 of the [complete CRATE -->
  <!-- paper](https://arxiv.org/abs/2311.13110). _(Jan 2024)_ -->

- **Talk:** Gave my annual Research at TTIC talk about [TILTED](https://brentyi.github.io/tilted)! [Here is the
  video
  recording](https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=db5b4c2a-96aa-4722-bb5f-b067015c0314). _(Mar 2024)_

- **Talk:** Gave the [Redwood
  Seminar](https://redwood.berkeley.edu/seminars/sam-buchanan-feb-2024/). _(Feb 2024)_

- **Publication:** We presented [CRATE](https://ma-lab-berkeley.github.io/CRATE/) at [NeurIPS
  2023](https://neurips.cc/virtual/2023/poster/71567), and as an oral in the
  [XAI in Action](https://neurips.cc/virtual/2023/75163) workshop. _(Dec 2023)_

- **Publication:** We presented [TILTED](https://brentyi.github.io/tilted/) at
  ICCV 2023. TILTED improves visual quality, compactness, and interpretability
  for hybrid neural field 3D representations by incorporating geometry into the
  latent features. Find the full version [on arXiv](https://arxiv.org/abs/2308.15461). _(Oct 2023)_

## [Past Updates]({{ site.baseurl }}/past_updates)
