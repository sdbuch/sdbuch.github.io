---
layout: home
---


{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu).  I completed
my Ph.D. in Electrical Engineering at [Columbia
University](https://ee.columbia.edu) in 2022, working with [John
Wright](http://www.columbia.edu/~jw2966/), and my B.S. in Electrical
Engineering at the [University of Kansas](https://eecs.ku.edu).


My research studies the mathematics of representation learning from the
perspective of signals and data. I'm interested in questions that span theory
and practice &mdash; What structural properties of modern data play a role in the
success or failure of deep learning? How can we design better deep
architectures by exploiting these structures? I'm especially interested in
applications to visual data. 

## Recent Updates

- (November 2023) Presenting [CRATE](https://ma-lab-berkeley.github.io/CRATE/),
  a "white-box" transformer neural network architecture with strong performance
  at scale. "White-box" means we derive each layer of CRATE from first
  principles, from the perspective of compressing the data distribution with
  respect to a simple, local model.
  We'll present this work at [NeurIPS
  2023](https://neurips.cc/virtual/2023/poster/71567), also in the
  [XAI in Action](https://neurips.cc/virtual/2023/75163) workshop.

- (November 2023) [New preprint posted](https://arxiv.org/abs/2310.14344) on
  methodology for data-driven inverse problem solvers with convergence
  guarantees. 
  We propose learned proximal operators, an *exact* parameterization of proximal
  operators with deep networks, and proximal matching training, an unsupervised
  training scheme to learn expressive proximal mappings. 
  We'll present this work at the [NeurIPS 2023 Learning-Based Solutions for Inverse Problems](https://neurips.cc/virtual/2023/79286) workshop.

- (October 2023) We presented [TILTED](https://brentyi.github.io/tilted/) at
  ICCV 2023. TILTED improves visual quality, compactness, and interpretability
  for hybrid neural field 3D representations by incorporating geometry into the
  latent features. We prove that the nonconvex optimization procedure at the
  heart of our method converges in a simple model problem.

- (July 2023) I am co-organizing the inaugural [Conference on Parsimony and
  Learning (CPAL)](https://cpal.cc), to be held at Hong Kong University on
  January 3rd, 2024.

- (June 2023) We taught a short course at ICASSP 2023 in Rhodes, Greece, titled
  ["Learning Nonlinear and Deep Low-Dimensional Representations from High-Dimensional Data: From Theory to Practice"](https://highdimdata-lowdimmodels-tutorial.github.io/).


## [Past Updates]({{ site.baseurl }}/past_updates)
