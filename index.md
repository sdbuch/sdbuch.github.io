---
layout: home
---


{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu){:target="_blank"}. {::comment}__{:/comment}
Previously, I completed my Ph.D. in Electrical Engineering at [Columbia University](https://ee.columbia.edu){:target="_blank"} in 2022, {::comment}__{:/comment}
working with [John
Wright](http://www.columbia.edu/~jw2966/){:target="_blank"}, {::comment}__{:/comment}
and my B.S. in Electrical Engineering at the [University of
Kansas](https://eecs.ku.edu){:target="_blank"} in 2014. {::comment}__{:/comment}

My research interests include the analysis
of algorithms for high-dimensional data with low-dimensional structure.
I am especially interested in understanding the role of low-dimensional
structure in the successes of modern data-driven signal processing frameworks
involving deep learning, as a path towards the development of more
resource-efficient and reliable models for science and engineering
applications.

## Recent Updates

- (August 2023) To appear at ICCV 2023: A new hybrid neural field architecture
  for implicit 3D representation,
  [TILTED](https://brentyi.github.io/tilted/){:target="_blank"}{::comment}__{:/comment}.
  Featuring a [throwback mixture of theory and
  practice](http://arxiv.org/abs/2308.15461){:target="_blank"}{::comment}__{:/comment},
  wherein we prove that our method
  succeeds under idealized conditions and dig into the mathematics of
  pose-appearance disentanglement in visual data.

- (August 2023) [New preprint posted](https://arxiv.org/abs/2306.01129){:target="_blank"}{::comment}__{:/comment}
  on understanding the transformer architecture from first-principles signal
  models, and a consequent white-box redesign of the architecture that
  maintains strong performance at the ImageNet-21K scale.
  This white-box architecture shows promising representation learning ability:
  for example, [whole-parts input segmentation emerges in its self-attention
  maps](http://arxiv.org/abs/2308.16271) without any specialized
  self-supervised training.

- (July 2023) I am a co-organizer of the inaugural [Conference on Parsimony and Learning
  (CPAL)](https://cpal.cc){:target="_blank"}{::comment}__{:/comment}, a new
  conference aiming to unite researchers working at the interface of AI/ML and
  low-dimensional structures in data. Please consider submitting your best work
  to the ["Recent Spotlight"](https://cpal.cc/tracks/#recent-spotlight-track-non-archival){:target="_blank"}{::comment}__{:/comment} (workshop) track, 
  deadline [October 10th](https://cpal.cc/deadlines/#conference-submission-recent-spotlight-track){:target="_blank"}{::comment}__{:/comment}!

- (June 2023) We taught a short course at ICASSP 2023 in Rhodes, Greece, titled
  ["Learning Nonlinear and Deep Low-Dimensional Representations from High-Dimensional Data: From Theory to Practice"](https://highdimdata-lowdimmodels-tutorial.github.io/){:target="_blank"}{::comment}__{:/comment}.
  Slides are available at the course website.

- (January 2023) I co-organized the third [Workshop on Seeking Low-Dimensionality in Deep Neural Networks (SLowDNN)](https://slowdnn-workshop.github.io/){:target="_blank"}{::comment}__{:/comment}. Thanks to all speakers and
  students for a very successful workshop, and to our hosts at MBZUAI.
  [Here is a link to my tutorial](https://www.youtube.com/watch?v=EO39D_Jfq_E&t=3s&pp=ygUMc2FtIGJ1Y2hhbmFu){:target="_blank"}{::comment}__{:/comment}.


## [Past Updates]({{ site.baseurl }}/past_updates)
