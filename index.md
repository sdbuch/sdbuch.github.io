---
layout: home
---

{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu). I completed
my Ph.D. in Electrical Engineering at [Columbia
University](https://ee.columbia.edu) in 2022, working with [John
Wright](http://www.columbia.edu/~jw2966/), and my B.S. in Electrical
Engineering at the [University of Kansas](https://eecs.ku.edu).

I study the mathematics of representation learning from the
perspective of signals and data. I'm interested in questions that span theory
and practice &mdash; What structural properties of modern data play a role in the
success or failure of deep learning? How can we design better deep
architectures by exploiting these structures? I'm especially interested in
applications to visual data.

## Upcoming Events

- **Tutorials:** I will give tutorial lectures on designing deep
  network architectures to pursue low-dimensional structures in data and our
  recent white-box transformers work at [ICASSP
  2024](https://cmsworkshops.com/ICASSP2024/tutorials.php#tut25) in Seoul (Apr 2024) and at [CVPR 2024]() in Seattle (Jun 2024, website TBA).

## Recent Highlights

- **1st Conference on Parsimony and Learning:** I co-organized the inaugural [Conference on
  Parsimony and Learning (CPAL)](https://cpal.cc), which took place at the
  University of Hong Kong from January 3--6, 2024. Thanks to all authors,
  speakers, organizers, and especially to the local team at HKU, whose hard
  work made the conference a success! Stay tuned for CPAL 2025. _(Jan 2024)_

## Recent Updates

- **Talk:** Gave the [Redwood
  Seminar](https://redwood.berkeley.edu/seminars/sam-buchanan-feb-2024/). _(Feb 2024)_

- **Publication:** Learned proximal networks, a methodology for
  parameterizing, learning, and evaluating expressive priors for data-driven inverse
  problem solvers with convergence guarantees, will appear in ICLR 2024.
  Find our extended preprint [on arXiv](https://arxiv.org/abs/2310.14344), with
  camera-ready additions to appear soon. _(Jan 2024)_

- **Publication:** [CRATE-MAE](https://ma-lab-berkeley.github.io/CRATE/) will
  appear in ICLR 2024. At the heart of this work is a connection between
  denoising and compression, which we use to derive a corresponding decoder
  architecture for the "white-box" transformer CRATE encoder. This work is
  described in Section 3 of the [complete CRATE
  paper](https://arxiv.org/abs/2311.13110). _(Jan 2024)_

- **Publication:** We presented [CRATE](https://ma-lab-berkeley.github.io/CRATE/) at [NeurIPS
  2023](https://neurips.cc/virtual/2023/poster/71567), and as an oral in the
  [XAI in Action](https://neurips.cc/virtual/2023/75163) workshop. _(Dec 2023)_

- **Preprint Release:** The full version of [the CRATE
  story](http://arxiv.org/abs/2311.13110) is now on arXiv.
  [CRATE](https://ma-lab-berkeley.github.io/CRATE/) is a "white-box" (yet
  scalable) transformer architecture where each layer is derived from the
  principles of compression and sparsification of the input data distribution.
  This white-box derivation leads CRATE's representations to have surprising
  [emergent segmentation properties](https://arxiv.org/abs/2308.16271) in
  vision applications without any complex self-supervised pretraining. _(Nov 2023)_

- **Publication:** We presented [TILTED](https://brentyi.github.io/tilted/) at
  ICCV 2023. TILTED improves visual quality, compactness, and interpretability
  for hybrid neural field 3D representations by incorporating geometry into the
  latent features. Find the full version [on arXiv](https://arxiv.org/abs/2308.15461). _(Oct 2023)_

## [Past Updates]({{ site.baseurl }}/past_updates)
