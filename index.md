---
layout: home
---


{% include contact.html %}

I am a Research Assistant Professor at [TTIC](https://ttic.edu){:target="_blank"}. {::comment}__{:/comment}
Previously, I completed my Ph.D. in Electrical Engineering at [Columbia University](https://ee.columbia.edu){:target="_blank"} in 2022, {::comment}__{:/comment}
working with [John
Wright](http://www.columbia.edu/~jw2966/){:target="_blank"}, {::comment}__{:/comment}
and my B.S. in Electrical Engineering at the [University of
Kansas](https://eecs.ku.edu){:target="_blank"} in 2014. {::comment}__{:/comment}


My research studies the mathematics of representation learning from the
perspective of signals and data. I'm interested in questions that span theory
and practice -- What structural properties of modern data play a role in the
success or failure of deep learning? How can we design better deep
architectures by exploiting these structures? I'm especially interested in
applications to visual data. 

## Recent Updates

- (August 2023) To appear at ICCV 2023: A new hybrid neural field architecture
  for implicit 3D representation,
  [TILTED](https://brentyi.github.io/tilted/){:target="_blank"}{::comment}__{:/comment}.
  Featuring a [throwback mixture of theory and
  practice](http://arxiv.org/abs/2308.15461){:target="_blank"}{::comment}__{:/comment},
  wherein we prove that our method
  succeeds under idealized conditions and dig into the mathematics of
  pose-appearance disentanglement in visual data.

- (August 2023) [New preprint posted](https://arxiv.org/abs/2306.01129){:target="_blank"}{::comment}__{:/comment}
  on understanding the transformer architecture from first-principles signal
  models, and a consequent white-box redesign of the architecture that
  maintains strong performance at the ImageNet-21K scale.
  This white-box architecture shows promising representation learning ability:
  for example, [whole-parts input segmentation emerges in its self-attention
  maps](http://arxiv.org/abs/2308.16271) without any specialized
  self-supervised training.

- (July 2023) I am a co-organizer of the inaugural [Conference on Parsimony and Learning
  (CPAL)](https://cpal.cc){:target="_blank"}{::comment}__{:/comment}, a new
  conference aiming to unite researchers working at the interface of AI/ML and
  low-dimensional structures in data. Please consider submitting your best work
  to the ["Recent Spotlight"](https://cpal.cc/tracks/#recent-spotlight-track-non-archival){:target="_blank"}{::comment}__{:/comment} (workshop) track, 
  deadline [October 10th](https://cpal.cc/deadlines/#conference-submission-recent-spotlight-track){:target="_blank"}{::comment}__{:/comment}!

- (June 2023) We taught a short course at ICASSP 2023 in Rhodes, Greece, titled
  ["Learning Nonlinear and Deep Low-Dimensional Representations from High-Dimensional Data: From Theory to Practice"](https://highdimdata-lowdimmodels-tutorial.github.io/){:target="_blank"}{::comment}__{:/comment}.
  Slides are available at the course website.

- (January 2023) I co-organized the third [Workshop on Seeking Low-Dimensionality in Deep Neural Networks (SLowDNN)](https://slowdnn-workshop.github.io/){:target="_blank"}{::comment}__{:/comment}. Thanks to all speakers and
  students for a very successful workshop, and to our hosts at MBZUAI.
  [Here is a link to my tutorial](https://www.youtube.com/watch?v=EO39D_Jfq_E&t=3s&pp=ygUMc2FtIGJ1Y2hhbmFu){:target="_blank"}{::comment}__{:/comment}.


## [Past Updates]({{ site.baseurl }}/past_updates)
