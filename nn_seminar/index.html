<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Neural Networks Seminar</title>
  <meta name="description" content="List of papers discussed in the neural nets seminar (2017)">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://sdbuchanan.com/nn_seminar/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Sam D. Buchanan" href="http://sdbuchanan.com/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>


  
  <meta property="og:title" content="Neural Networks Seminar">
  <meta property="og:site_name" content="Sam D. Buchanan">
  <meta property="og:url" content="http://sdbuchanan.com/nn_seminar/">
  <meta property="og:description" content="List of papers discussed in the neural nets seminar (2017)">
  
  
    <meta property="og:image" content="/assets/sam-2023.jpeg">
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Neural Networks Seminar">
  <meta name="twitter:description" content="List of papers discussed in the neural nets seminar (2017)">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Sam D. Buchanan</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Home</a>
      
        
        <a class="page-link" href="/publications/">Publications</a>
      
        
        <a class="page-link" href="/misc/">Misc.</a>
      
        
        <a class="page-link" href="/assets/cv.pdf">CV</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Neural Networks Seminar</h1>
  </header>

  <div class="post-content">
    <p>During Spring and Summer 2017, the neural networks seminar met
Tuesdays at 2:00 p.m. in Mudd 417. Each week we would discuss
one or two contemporary works concerning optimization, geometry,
and symmetry in the training of neural networks and related
learning problems.</p>

<p>This page collects the papers that have been discussed in the
seminar so far, in reverse chronological order.</p>

<h2 id="past-discussion-topics">Past Discussion Topics</h2>
<ul>
  <li>August 1, 2017
    <ul>
      <li>Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G.
Dimakis, <em>Compressed Sensing using Generative Models</em>. 
ICML 2017. <a href="https://arxiv.org/abs/1703.03208">link</a></li>
    </ul>
  </li>
  <li>July 25, 2017
    <ul>
      <li>Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi
Zhang, <em>Generalization and Equilibrium in Generative
Adversarial Nets (GANs).</em> ICML 2017.
<a href="https://arxiv.org/abs/1703.00573">link</a></li>
      <li>Sanjeev Arora and Yi Zhang, <em>Do GANs Actually Learn the
Distribution? An Empirical Study</em>. Preprint, 2017.
<a href="https://arxiv.org/abs/1706.08224">link</a></li>
    </ul>
  </li>
  <li>July 19, 2017
    <ul>
      <li>Mark Borgerding, Philip Schniter, and Sundeep Rangan,
<em>AMP-Inspired Deep Networks for Sparse Linear Inverse
Problems</em>. IEEE Trans. Sig. Proc. 65(16), 2017.
<a href="http://ieeexplore.ieee.org/document/7934066/">link</a></li>
    </ul>
  </li>
  <li>July 11, 2017
    <ul>
      <li>Martin Arjovsky, Soumith Chintala, and Léon Bottou,
<em>Wasserstein GAN</em>. Preprint, 2017.
<a href="https://arxiv.org/abs/1701.07875">link</a></li>
    </ul>
  </li>
  <li>June 27, 2017
    <ul>
      <li>Sachin Ravi and Hugo Larochelle, <em>Optimization as a Model
for Few-Shot Learning</em>. ICLR 2017.
<a href="https://openreview.net/forum?id=rJY0-Kcll&amp;noteId=rJY0-Kcll">link</a></li>
      <li>Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew
Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and
Nando de Freitas, <em>Learning to Learn by Gradient Descent by
Gradient Descent</em>. NIPS 2016.
<a href="https://arxiv.org/abs/1606.04474">link</a></li>
    </ul>
  </li>
  <li>June 20, 2017
    <ul>
      <li>Gintare Karolina Dziugaite and Daniel Roy, <em>Computing
Nonvacuous Generalization Bounds for Deep (Stochastic)
Neural Networks with Many More Parameters than Training Data</em>.
Preprint, 2017. <a href="https://arxiv.org/abs/1703.11008">link</a></li>
    </ul>
  </li>
  <li>June 13, 2017
    <ul>
      <li>Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov,
and Nathan Srebro, <em>Geometry of Optimization and Implicit
Regularization in Deep Learning</em>. Preprint, 2017.
<a href="https://arxiv.org/abs/1705.03071">link</a></li>
      <li>Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann
LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes,
Levent Sagun, and Riccardo Zecchina, <em>Entropy-SGD: Biasing
Gradient Descent Into Wide Valleys</em>. ICLR 2017.
<a href="https://arxiv.org/abs/1611.01838">link</a></li>
    </ul>
  </li>
  <li>June 6, 2017
    <ul>
      <li>Ashia Wilson, Rebecca Roelofs, Mitchell Stern, Nathan
Srebro, and Benjamin Recht, <em>The Marginal Value of
Adaptive Gradient Methods in Machine Learning</em>. 
Preprint, 2017. <a href="https://arxiv.org/abs/1705.08292">link</a></li>
      <li>John Duchi, Elad Hazan, and Yoram Singer, <em>Adaptive
Subgradient Methods for Online Learning and Stochastic
Optimization</em>. JMLR (12), 2011.
<a href="http://jmlr.org/papers/v12/duchi11a.html">link</a></li>
      <li>Diederik Kingma and Jimmy Ba, <em>Adam: A Method for
Stochastic Optimization</em>. ICLR 2015, revised 2017.
<a href="https://arxiv.org/abs/1412.6980">link</a></li>
    </ul>
  </li>
  <li>May 30, 2017
    <ul>
      <li>Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah,
<em>Failures of Gradient-Based Deep Learning</em>. Preprint, 2017.
<a href="https://arxiv.org/abs/1703.07950">link</a></li>
    </ul>
  </li>
  <li>May 2, 2017
    <ul>
      <li>Moritz Hardt, Benjamin Recht, and Yoram Singer, <em>Train
Faster, Generalize Better: Stability of Stochastic Gradient
Descent</em>. ICML 2016. <a href="https://arxiv.org/abs/1509.01240">link</a></li>
    </ul>
  </li>
  <li>April 25, 2017
    <ul>
      <li>Ian Goodfellow, Oriol Vinyals, and Andrew Saxe,
<em>Qualitatively Characterizing Neural Network Optimization
Problems</em>. ICLR 2015. <a href="https://arxiv.org/abs/1412.6544">link</a></li>
      <li>C. Daniel Freeman and Joan Bruna, <em>Topology and Geometry
of Half-Rectified Network Optimization</em>. ICLR 2017.
<a href="https://arxiv.org/abs/1611.01540">link</a></li>
    </ul>
  </li>
  <li>April 18, 2017
    <ul>
      <li>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
and Oriol Vinyals, <em>Understanding Deep Learning Requires
Rethinking Generalization</em>. ICLR 2017.
<a href="https://arxiv.org/abs/1611.03530">link</a></li>
    </ul>
  </li>
  <li>April 14, 2017
    <ul>
      <li>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal,
Mikhail Smelyanskiy, and Ping Tak Peter Tang, <em>On Large-Batch
Training for Deep Learning: Generalization Gap and Sharp
Minima</em>. ICLR 2017. <a href="https://arxiv.org/abs/1609.04836">link</a></li>
      <li>Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua
Bengio, <em>Sharp Minima Can Generalize For Deep Nets</em>. 
ICML 2017.  <a href="https://arxiv.org/abs/1703.04933">link</a></li>
    </ul>
  </li>
  <li>April 7, 2017
    <ul>
      <li>Haohan Wang and Bhiksha Raj, <em>On the Origin of Deep
Learning</em>. Preprint, 2017. <a href="https://arxiv.org/abs/1702.07800">link</a></li>
      <li>Jürgen Schmidhuber, <em>Deep Learning in Neural Networks: An
Overview</em>. Neural Networks (61), 2015.
<a href="https://arxiv.org/abs/1404.7828">link</a></li>
    </ul>
  </li>
</ul>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      


<div id="icon-container">
  <div>
    <a href=mailto:sam@ttic.edu target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" />
    </a>
  </div>
  <div>
    <a href=https://scholar.google.com/citations?user=5WT38A0AAAAJ target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" />
    </a>
  </div>

  <div>
    <a href=https://twitter.com/_sdbuchanan
       target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" />
    </a>
  </div>

  <div>
    <a href=https://github.com/sdbuch
       target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" />
    </a>
  </div>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
