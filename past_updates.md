---
layout: page
title: Past Updates
description: Archived updates from the homepage.
permalink: /past_updates/
---

- **2nd Conference on Parsimony and Learning:** I co-organized the second
  [Conference on Parsimony and Learning (CPAL)](https://2025.cpal.cc). Thanks to all
  attendees for making it a success! _(Mar 2025)_

- **Talk:** Spoke at the [IDEAL Privacy and Interpretability in Generative
  AI](https://www.ideal-institute.org/2024/09/04/workshop-on-harmonious-human-ai-ecosystems-2/)
  workshop. _(Nov 2024)_

- **Talk:** Spoke at [Asilomar
  2024](https://cmsworkshops.com/Asilomar2024/view_session.php?SessionID=1086).
  _(Oct 2024)_

- **MDS '24 Special Session:** Co-organizing a SIAM MDS '24 special session
  on [Mathematical Principles in Foundation
  Models](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=80543).
  _(Oct 2024)_

- **Publication:** The full version of [the CRATE
  story](http://arxiv.org/abs/2311.13110) has been accepted for publication in
  JMLR.
  [CRATE](https://ma-lab-berkeley.github.io/CRATE/) is a "white-box" (yet
  scalable) *transformer architecture* where each layer is derived from the
  principles of compression and sparsification of the input data distribution.
  This white-box derivation leads CRATE's representations to have surprising
  [emergent segmentation properties](https://arxiv.org/abs/2308.16271) in
  vision applications without any complex self-supervised pretraining. _(Aug 2024)_

- **Talk:** Spoke at the BIRS Oaxaca ["Mathematics of Deep Learning"
  workshop](https://www.birs.ca/events/2024/5-day-workshops/24w5297) about
  white-box networks (Jun 2024).

- **White-Box Deep Networks Tutorials:** We delivered a tutorial on building white-box deep neural networks
  at [ICASSP 2024](https://cmsworkshops.com/ICASSP2024/tutorials.php#tut25) in
  Seoul (Apr 2024), and at [CVPR
  2024](https://cvpr2024-tutorial-low-dim-models.github.io) in Seattle (Jun
  2024). The most recent tutorial slides can be found
  [here](https://www.dropbox.com/home/CVPR-tutorial) (Lectures 2-1 -- 2-3).

- **Publication:** [Learned proximal networks](https://zhenghanfang.github.io/learned-proximal-networks/), a methodology for
  parameterizing, learning, and evaluating expressive priors for data-driven inverse
  problem solvers with convergence guarantees, appeared in ICLR 2024.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=kNPcOaqC5r). _(May 2024)_

- **Publication:** [CRATE-MAE](https://ma-lab-berkeley.github.io/CRATE/) appeared in ICLR 2024. At the heart of this work is a connection between
  denoising and compression, which we use to derive a corresponding decoder
  architecture for the "white-box" transformer CRATE encoder.
  The camera-ready version of the manuscript can be found
  [here](https://openreview.net/forum?id=PvyOYleymy). _(May 2024)_
  <!-- This work is described in Section 3 of the [complete CRATE -->
  <!-- paper](https://arxiv.org/abs/2311.13110). _(Jan 2024)_ -->

- **Talk:** Gave my annual Research at TTIC talk about [TILTED](https://brentyi.github.io/tilted)! [Here is the
  video
  recording](https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=db5b4c2a-96aa-4722-bb5f-b067015c0314). _(Mar 2024)_

- **Talk:** Gave the [Redwood
  Seminar](https://redwood.berkeley.edu/seminars/sam-buchanan-feb-2024/). _(Feb 2024)_

- **Publication:** We presented [CRATE](https://ma-lab-berkeley.github.io/CRATE/) at [NeurIPS
  2023](https://neurips.cc/virtual/2023/poster/71567), and as an oral in the
  [XAI in Action](https://neurips.cc/virtual/2023/75163) workshop. _(Dec 2023)_

- **Publication:** We presented [TILTED](https://brentyi.github.io/tilted/) at
  ICCV 2023. TILTED improves visual quality, compactness, and interpretability
  for hybrid neural field 3D representations by incorporating geometry into the
  latent features. Find the full version [on arXiv](https://arxiv.org/abs/2308.15461). _(Oct 2023)_

- **ICLR 2024**: Presented two posters in Vienna: _Learned Proximal Networks_
  (Friday, May 10, a.m. session; [project
  page](https://zhenghanfang.github.io/learned-proximal-networks/), [ICLR
  page](https://iclr.cc/virtual/2024/poster/17978)) and _CRATE-MAE_ (Thursday,
  May 9, a.m. session; [project
  page](https://ma-lab-berkeley.github.io/CRATE/), [ICLR
  page](https://iclr.cc/virtual/2024/poster/18688)). _(May 2024)_

- **1st Conference on Parsimony and Learning:** I co-organized the inaugural [Conference on
  Parsimony and Learning (CPAL)](https://2024.cpal.cc), which took place at the
  University of Hong Kong from January 3--6, 2024. Thanks to all authors,
  speakers, organizers, and especially to the local team at HKU, whose hard
  work made the conference a success! Stay tuned for CPAL 2025. _(Jan 2024)_

- (December 2023) [New preprint posted](https://arxiv.org/abs/2310.14344) on
  methodology for data-driven inverse problem solvers with convergence
  guarantees. We presented this work at the [NeurIPS 2023 Learning-Based
  Solutions for Inverse Problems](https://neurips.cc/virtual/2023/79286)
  workshop.

- (June 2023) We taught a short course at ICASSP 2023 in Rhodes, Greece, titled
  ["Learning Nonlinear and Deep Low-Dimensional Representations from High-Dimensional Data: From Theory to Practice"](https://highdimdata-lowdimmodels-tutorial.github.io/).


- (January 2023) I co-organized the third [Workshop on Seeking
  Low-Dimensionality in Deep Neural Networks
  (SLowDNN)](https://slowdnn-workshop.github.io/). [Here is a link to my
  tutorial](https://www.youtube.com/watch?v=EO39D_Jfq_E&t=3s&pp=ygUMc2FtIGJ1Y2hhbmFu).

- (September 2022) I defended my Ph.D. thesis (back in June!), and started as a
  Research Assistant Professor at TTIC. 

- (May 2022) I received the [Eli Jury
  Award](https://www.ee.columbia.edu/student-awards-and-fellowships) from the
  Columbia EE Department for "outstanding achievement in the area of signal
  processing". 

- (May 2022) We taught a short course at ICASSP 2022 in May, titled
  ["Low-Dimensional Models for High-Dimensional Data: From Linear to Nonlinear,
  Convex to Nonconvex, and Shallow to
  Deep"](https://highdimdata-lowdimmodels-tutorial.github.io/2022){:target="_blank"}{::comment}__{:/comment}.
  Slides are available!

- (April 2022) I attended the [Princeton ML Theory Summer
  School](https://mlschool.princeton.edu/) this summer from June 13--17. 

- (March 2022) [New preprint released]({{ site.baseurl }}/assets/pdf/refine.pdf){:target="_blank"}{::comment}__{:/comment}
  on invariance-by-design neural architectures for computing with visual data,
  with theoretical guarantees.  Feedback is very much appreciated! 

- (December 2021) We presented our paper [Deep Networks Provably Classify Data
  on
  Curves](https://papers.nips.cc/paper/2021/hash/f26df67e8110ee2b44923db775e3e47f-Abstract.html){:target="_blank"}{::comment}__{:/comment}
  at NeurIPS. 

- (August 2021) I gave a [talk about our work on the multiple manifold
  problem](https://www.youtube.com/watch?v=PEaYY2TLvYY){:target="_blank"}{::comment}__{:/comment}
  at the IMA Workshop on Mathematical Foundation and Applications of Deep
  Learning at Purdue. Thanks to the organizers for the opportunity to speak!

- (July 2021) I will be attending the [Princeton Deep Learning Theory Summer
  School](https://deep-learning-summer-school.princeton.edu){:target="_blank"}{::comment}__{:/comment} this year.

- (May 2021) We will present our paper "Deep Networks and the Multiple Manifold
  Problem" at ICLR 2021 on Thursday, May 6th! Conference link
  [here](https://iclr.cc/virtual/2021/poster/2530){:target="_blank"}{::comment}__{:/comment}, paper
  link [here](https://openreview.net/forum?id=O-6Pm_d_Q-){:target="_blank"}{::comment}__{:/comment}.
