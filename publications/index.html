<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Publications</title>
  <meta name="description" content="Academic publication list for Sam D. Buchanan">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://sdbuchanan.com/publications/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Sam D. Buchanan" href="http://sdbuchanan.com/feed.xml">

  
<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- MathJax -->
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>



<link
  href="https://fonts.googleapis.com"
  rel="preconnect"
  />
<link
  href="https://fonts.gstatic.com"
  rel="preconnect"
  crossorigin="anonymous"
  />
<script
  src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"
  type="text/javascript"
  ></script>
<script type="text/javascript">
  WebFont.load({
    google: {
      families: [
        "Lato:300,300italic,400,400italic,700,700italic",
        "Open Sans:300,300italic,400,400italic,700,700italic",
      ],
    },
  });
</script>


  
  <meta property="og:title" content="Publications">
  <meta property="og:site_name" content="Sam D. Buchanan">
  <meta property="og:url" content="http://sdbuchanan.com/publications/">
  <meta property="og:description" content="Academic publication list for Sam D. Buchanan">
  
  
    <meta property="og:image" content="/assets/sam-2023.jpeg">
  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Publications">
  <meta name="twitter:description" content="Academic publication list for Sam D. Buchanan">
  
  

  <link rel="dns-prefetch" href="https://fonts.gstatic.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Bitter:ital,wght@0,400;0,700;1,400&amp;display=swap" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Sam D. Buchanan</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/">Home</a>
      
        
        <a class="page-link" href="/publications/">Publications</a>
      
        
        <a class="page-link" href="/misc/">Misc.</a>
      
        
        <a class="page-link" href="/assets/cv.pdf">CV</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
  </header>

  <div class="post-content">
    <div class="publications">


  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Yu2023-ig" class="col-sm-8">
    
      <div class="title">White-Box Transformers via Sparse Rate Reduction</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yaodongyu.github.io/" target="_blank">Yaodong Yu</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://druvpai.github.io/" target="_blank">Druv Pai</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://tianzhechu.com/" target="_blank">Tianzhe Chu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://robinwu218.github.io/" target="_blank">Ziyang Wu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://tsb0601.github.io/petertongsb/" target="_blank">Shengbang Tong</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.cis.jhu.edu/~haeffele/" target="_blank">Benjamin D Haeffele</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://people.eecs.berkeley.edu/~yima/" target="_blank">Yi Ma</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        
          <em>Preprint</em>,
        
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2306.01129" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/Ma-Lab-Berkeley/CRATE" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we contend that the objective of
                   representation learning is to compress and transform the
                   distribution of the data, say sets of tokens, towards a
                   mixture of low-dimensional Gaussian distributions supported
                   on incoherent subspaces. The quality of the final
                   representation can be measured by a unified objective
                   function called sparse rate reduction. From this
                   perspective, popular deep networks such as transformers can
                   be naturally viewed as realizing iterative schemes to
                   optimize this objective incrementally. Particularly, we show
                   that the standard transformer block can be derived from
                   alternating optimization on complementary parts of this
                   objective: the multi-head self-attention operator can be
                   viewed as a gradient descent step to compress the token sets
                   by minimizing their lossy coding rate, and the subsequent
                   multi-layer perceptron can be viewed as attempting to
                   sparsify the representation of the tokens. This leads to a
                   family of white-box transformer-like deep network
                   architectures which are mathematically fully interpretable.
                   Despite their simplicity, experiments show that these
                   networks indeed learn to optimize the designed objective:
                   they compress and sparsify representations of large-scale
                   real-world vision datasets such as ImageNet, and achieve
                   performance very close to thoroughly engineered transformers
                   such as ViT.  </p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="tilted2023" class="col-sm-8">
    
      <div class="title">Canonical Factors for Hybrid Neural Fields</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Brent Yi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Weijia Zeng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://people.eecs.berkeley.edu/~yima/" target="_blank">Yi Ma</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Computer Vision (ICCV)</em>,
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2308.15461" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
      <a href="https://brentyi.github.io/tilted/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> Factored feature volumes offer a simple way to build more
                compact, efficient, and intepretable neural fields, but also
                introduce biases that are not necessarily beneficial for
                real-world data. In this work, we (1) characterize the
                undesirable biases that these architectures have for
                axis-aligned signals -- they can lead to radiance field
                reconstruction differences of as high as 2 PSNR -- and (2)
                explore how learning a set of canonicalizing transformations
                can improve representations by removing these biases. We prove
                in a two-dimensional model problem that simultaneously learning
                these transformations together with scene appearance succeeds
                with drastically improved efficiency. We validate the resulting
                architectures, which we call TILTED, using image, signed
                distance, and radiance field reconstruction tasks, where we
                observe improvements across quality, robustness, compactness,
                and runtime. Results demonstrate that TILTED can enable
                capabilities comparable to baselines that are 2x larger, while
                highlighting weaknesses of neural field evaluation procedures.
                </p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Buchanan2022-qp" class="col-sm-8">
    
      <div class="title">Resource-Efficient Invariant Networks: Exponential Gains
                   by Unrolled Optimization</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jingkai Yan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ellie Haber,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.columbia.edu/~jw2966/" target="_blank">John Wright</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        
          <em>Preprint</em>,
        
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
      <a href="http://arxiv.org/abs/2203.05006" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
      <a href="https://github.com/sdbuch/refine" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p> Achieving invariance to nuisance transformations is a fundamental challenge in the
              construction of robust and reliable vision systems.  Existing approaches to
              invariance scale exponentially with the dimension of the family of transformations,
              making them unable to cope with natural variabilities in visual data such as changes
              in pose and perspective.  We identify a common limitation of these approaches---they
              rely on sampling to traverse the high-dimensional space of transformations---and
              propose a new computational primitive for building invariant networks based instead
              on optimization, which in many scenarios provides a provably more efficient method
              for high-dimensional exploration than sampling.  We provide empirical and
              theoretical corroboration of the efficiency gains and soundness of our proposed
              method, and demonstrate its utility in constructing an efficient invariant network
              for a simple hierarchical object detection task when combined with unrolled
              optimization.  Code for our networks and experiments is available at
              https://github.com/sdbuch/refine.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Buchanan2022-ye" class="col-sm-8">
    
      <div class="title">Deep Networks Through the Lens of Low-Dimensional Structure:
            Towards Mathematical and Computational Principles for Nonlinear
            Data</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Sam Buchanan</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>PhD Thesis</em>,
      
      
        2022
      
      </div>
    

    <div class="links">
    
    
    
      <a href="https://academiccommons.columbia.edu/doi/10.7916/p2n1-kp52" class="btn btn-sm z-depth-0" role="button" target="_blank">Download</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Wang2021-rc" class="col-sm-8">
    
      <div class="title">Deep Networks Provably Classify Data on Curves</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Tingran Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dar Gilboa,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.columbia.edu/~jw2966/" target="_blank">John Wright</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>,
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://papers.nips.cc/paper/2021/hash/f26df67e8110ee2b44923db775e3e47f-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Download</a>
    
    
    
    
    
    
    
    
    
      
      <a href="https://www.youtube.com/watch?v=PEaYY2TLvYY" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data with low-dimensional nonlinear structure are ubiquitous in engineering and scientific problems. We study a model problem with such structure: a binary classification task that uses a deep fully-connected neural network to classify data drawn from two disjoint smooth curves on the unit sphere. Aside from mild regularity conditions, we place no restrictions on the configuration of the curves. We prove that when (i) the network depth is large relative to certain geometric properties that set the difficulty of the problem and (ii) the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To our knowledge, this is the first generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. Our analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime, where the network depth plays the role of a fitting resource in solving the classification problem. In particular, via fine-grained control of the decay properties of the NTK, we demonstrate that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Buchanan2021-sj" class="col-sm-8">
    
      <div class="title">Deep Networks and the Multiple Manifold Problem</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dar Gilboa,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.columbia.edu/~jw2966/" target="_blank">John Wright</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations</em>,
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="https://openreview.net/forum?id=O-6Pm_d_Q-" class="btn btn-sm z-depth-0" role="button" target="_blank">Download</a>
    
    
    
    
    
    
    
    
    
      
      <a href="https://www.youtube.com/watch?v=qloPDlYqnIk" class="btn btn-sm z-depth-0" role="button" target="_blank">Talk</a>
      
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth \( L \) is large relative to certain geometric and statistical properties of the data, the network width \(n\) grows as a sufficiently large polynomial in \(L\), and the number of i.i.d. samples from the manifolds is polynomial in \(L\), randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the "neural tangent kernel" of Jacot et al. and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected ReLU networks, requiring width \(n \geq L\,\mathrm{poly}(d_0)\) to achieve uniform concentration of the initial kernel over a \(d_0\)-dimensional submanifold of the unit sphere \(\mathbb{S}^{n_0-1}\), and a nonasymptotic framework for establishing generalization of networks trained in the "NTK regime" with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Gilboa2019-px" class="col-sm-8">
    
      <div class="title">Efficient Dictionary Learning with Gradient Descent</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Dar Gilboa,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Sam Buchanan</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.columbia.edu/~jw2966/" target="_blank">John Wright</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 36th International Conference on Machine
               Learning</em>,
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="http://proceedings.mlr.press/v97/gilboa19a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Download</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem â€“ complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well.</p>
    </div>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Buchanan2018-ip" class="col-sm-8">
    
      <div class="title">Efficient Model-Free Learning to Overcome Hardware
               Nonidealities in Analog-to-Information Converters</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>S Buchanan</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  T Haque,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.ee.columbia.edu/~kinget/" target="_blank">P Kinget</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.columbia.edu/~jw2966/" target="_blank">J Wright</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2018 IEEE International Conference on Acoustics, Speech and
               Signal Processing (ICASSP)</em>,
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
    
    
    
      <a href="http://dx.doi.org/10.1109/ICASSP.2018.8461811" class="btn btn-sm z-depth-0" role="button" target="_blank">Download</a>
    
    
    
    
    
    
    
      
      <a href="/assets/pdf/sslista_deck.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper considers compressed sensing (CS) in the context of RF spectrum sensing and presents an efficient approach for learning hardware nonidealities in an analog-to-information converter (A2IC). The proposed methodology is based on the learned iterative shrinkage-thresholding algorithm (LISTA), which enables co-optimization of the hardware and the reconstruction algorithm and leads to a model-free recovery approach that is optimally tuned for the unique computational constraints and hardware nonidealities present in the RF frontend. To achieve this, we devise a training protocol that employs a dataset and neural network of minimal sizes. We demonstrate the effectiveness of our methodology on simulated data from a model of a well-established CS A2IC in the presence of linear impairments and noise. The recovery process extrapolates from training on 1-sparse signals to recovering the support of signals whose sparsity runs up to the theoretical optimum for \(\ell^1\) -based algorithms across a range of typical operating SNRs.</p>
    </div>
    
  </div>
</div>
</li></ol>


</div>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      


<div id="icon-container">
  <div>
    <a href=mailto:sam@ttic.edu target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/gmail.svg" alt="Mail icon" />
    </a>
  </div>
  <div>
    <a href=https://scholar.google.com/citations?user=5WT38A0AAAAJ target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/scholar.svg" alt="Google Scholar icon" />
    </a>
  </div>

  <div>
    <a href=https://twitter.com/_sdbuchanan
       target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/twitter.svg" alt="Twitter icon" />
    </a>
  </div>

  <div>
    <a href=https://github.com/sdbuch
       target="_blank" style="text-decoration: none">
      <img id="icon-container-item" src="/assets/icons/github.svg" alt="GitHub icon" />
    </a>
  </div>
</div>

    </p>

  </div>

</footer>


  </body>

  <!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
